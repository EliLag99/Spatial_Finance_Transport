{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AADT Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as album\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import plotly.express as px\n",
    "import torchmetrics\n",
    "from torchmetrics import MeanAbsolutePercentageError\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR_PATH = os.path.abspath('..')\n",
    "\n",
    "IMG_SIZE = 1024\n",
    "VEHICLE_DETECTION_COUNT_PATH = os.path.join(ROOT_DIR_PATH, 'data/vehicle_counts_detection.csv')\n",
    "AADT_PROCESSED_PATH = os.path.join(ROOT_DIR_PATH, 'data/ground_truth_data/aadt_processed.csv')\n",
    "\n",
    "NN_MODEL_PATH = os.path.join(ROOT_DIR_PATH, \"models/nn_aadt_model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Count Data\n",
    "- Traffic monitoring stations for long-term traffic count data\n",
    "    - Extract at same time as Satellite Image!\n",
    "- How to use permanent and temporary traffic count stations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle detection number\n",
    "From vehicle detection model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road characteristics\n",
    "From road characterstics pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Includes:\n",
    "- Road width\n",
    "- Live speed data\n",
    "- Directionality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.labels = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['aadt'].values.astype('float32'))\n",
    "        self.vehicle_count = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['total_volume'].values.astype('float32')).unsqueeze(1) # Training\n",
    "        #self.speed_data = pd.read_csv(SPEED_DATA_PATH) \n",
    "        #self.road_width = pd.read_csv(ROAD_WIDTH_PATH)\n",
    "        self.hour = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['hour'].values.astype('float32')).unsqueeze(1)\n",
    "        self.avg_mph = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['avg_mph'].values.astype('float32')).unsqueeze(1)\n",
    "        self.day = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['day'].values.astype('float32')).unsqueeze(1)\n",
    "        self.month = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['month'].values.astype('float32')).unsqueeze(1)\n",
    "\n",
    "        self.x = torch.concat((self.vehicle_count, self.avg_mph, self.day, self.month, self.hour), dim=-1)\n",
    "        self.y = self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "custom_data = CustomDataset()\n",
    "train_split = 0.8\n",
    "train_data, val_data = random_split(custom_data, [train_split, 1-train_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5, 5),\n",
    "            nn.Linear(5,20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20,1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=5, bias=True)\n",
       "    (1): Linear(in_features=5, out_features=20, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=20, out_features=1, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = NeuralNetwork()\n",
    "nn_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopper(patience=3, min_delta=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=5, bias=True)\n",
       "    (1): Linear(in_features=5, out_features=20, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=20, out_features=1, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "nn_model.apply(init_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "batch_size = 1\n",
    "epochs = 1\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "MAPE = MeanAbsolutePercentageError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False, sampler=None,\n",
    "                    batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "                    pin_memory=False, drop_last=False, timeout=0,\n",
    "                    worker_init_fn=None, prefetch_factor=2,\n",
    "                    persistent_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False, sampler=None,\n",
    "                    batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "                    pin_memory=False, drop_last=False, timeout=0,\n",
    "                    worker_init_fn=None, prefetch_factor=2,\n",
    "                    persistent_workers=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(ep_id, action, loader, model, optimizer, criterion):\n",
    "    losses = [] # Keep list of accuracies to track progress\n",
    "    is_training = action == \"train\" # True when action == \"train\", else False \n",
    "\n",
    "    # Looping over all batches\n",
    "    for batch_idx, batch in enumerate(loader): \n",
    "        x, y = batch\n",
    "\n",
    "        # Resetting the optimizer gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Setting model to train or test\n",
    "        with torch.set_grad_enabled(is_training):\n",
    "            \n",
    "            # Feed batch to model\n",
    "            logits = nn_model(x).squeeze(1)\n",
    "            print(\"logits: {}\".format(logits))\n",
    "\n",
    "            # Calculate the loss based on predictions and real labels\n",
    "            loss = criterion(logits, y)\n",
    "            mape_loss = MAPE(logits, y)\n",
    "            print(\"MAPE loss: {}\".format(mape_loss))\n",
    "\n",
    "            # If training, perform backprop and update weights\n",
    "            if is_training:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Append current batch accuracy\n",
    "            losses.append(mape_loss.detach().numpy())\n",
    "\n",
    "            # Print some stats every 50th batch \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"{action.capitalize()}ing, Epoch: {ep_id+1}, Batch {batch_idx}: Loss = {loss.item()}\")\n",
    "\n",
    "        if not is_training:\n",
    "            if early_stopper.early_stop(mape_loss.detach().numpy()):             \n",
    "                break\n",
    "                    \n",
    "    # Return accuracies to main loop                 \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(epochs, train_dl, val_dl, model, optimizer, criterion):\n",
    "\n",
    "    # Keep lists of accuracies to track performance on train and test sets\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Looping over epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Looping over train set and training\n",
    "        train_loss = run_epoch(epoch, \"train\", train_dl, model, optimizer, criterion)\n",
    "\n",
    "        # Looping over test set\n",
    "        val_loss = run_epoch(epoch, \"val\", val_dl, model, optimizer, criterion) \n",
    "\n",
    "        # Collecting stats\n",
    "        train_losses += train_loss\n",
    "        val_losses += val_loss         \n",
    "            \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "Training, Epoch: 1, Batch 0: Loss = 332801760.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0\n",
      "logits: tensor([0.0122], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9999980926513672\n",
      "logits: tensor([73.8922], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9959495067596436\n",
      "logits: tensor([73.0033], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.995998203754425\n",
      "logits: tensor([213.9885], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9882700443267822\n",
      "logits: tensor([198.1483], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9687219262123108\n",
      "logits: tensor([776.4324], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9571363925933838\n",
      "logits: tensor([1841.2173], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8990718722343445\n",
      "logits: tensor([743.4202], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9589588642120361\n",
      "logits: tensor([3300.1790], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8178107142448425\n",
      "logits: tensor([5751.6763], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6824737191200256\n",
      "logits: tensor([2692.4226], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5749967098236084\n",
      "logits: tensor([12438.8828], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.31330057978630066\n",
      "logits: tensor([5685.5972], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6883383989334106\n",
      "logits: tensor([9592.0332], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5141180753707886\n",
      "logits: tensor([12854.2070], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29037222266197205\n",
      "logits: tensor([21388.5137], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1724323332309723\n",
      "logits: tensor([19909.1895], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09134173393249512\n",
      "logits: tensor([44428.9180], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.4354146718978882\n",
      "logits: tensor([10534.9951], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4184063673019409\n",
      "logits: tensor([44523.8594], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.4579787254333496\n",
      "logits: tensor([31685.9980], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7492532730102539\n",
      "logits: tensor([8876.5566], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.40117889642715454\n",
      "logits: tensor([7058.3135], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6103396415710449\n",
      "logits: tensor([6164.1079], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6597050428390503\n",
      "logits: tensor([8344.6855], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3172221779823303\n",
      "logits: tensor([6280.2285], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.008655711077153683\n",
      "logits: tensor([14618.3105], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1986829787492752\n",
      "logits: tensor([5397.8691], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7041105031967163\n",
      "logits: tensor([3707.6807], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7953142523765564\n",
      "logits: tensor([3615.3064], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.42931801080703735\n",
      "logits: tensor([1292.9446], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7959065437316895\n",
      "Training, Epoch: 1, Batch 50: Loss = 25422956.0\n",
      "logits: tensor([2246.3889], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6454038619995117\n",
      "logits: tensor([9778.6533], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4639735221862793\n",
      "logits: tensor([5812.1792], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6813997030258179\n",
      "logits: tensor([3856.2458], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.39128533005714417\n",
      "logits: tensor([2316.3235], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8721253275871277\n",
      "logits: tensor([3950.8386], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3763536810874939\n",
      "logits: tensor([7270.3350], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6014694571495056\n",
      "logits: tensor([9590.3232], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47429704666137695\n",
      "logits: tensor([8383.5889], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5404453873634338\n",
      "logits: tensor([5880.1260], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6753824949264526\n",
      "logits: tensor([5787.1328], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6827726364135742\n",
      "logits: tensor([7735.8418], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5729360580444336\n",
      "logits: tensor([4261.1895], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7664186954498291\n",
      "logits: tensor([6717.9297], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6291307806968689\n",
      "logits: tensor([5084.8057], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7192888259887695\n",
      "logits: tensor([11587.3125], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8290761113166809\n",
      "logits: tensor([8035.2998], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5564042329788208\n",
      "logits: tensor([6297.9116], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6523182392120361\n",
      "logits: tensor([4959.1133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7281613349914551\n",
      "logits: tensor([8738.9863], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5175566077232361\n",
      "logits: tensor([8085.5874], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.276323139667511\n",
      "logits: tensor([9555.1992], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4762223958969116\n",
      "logits: tensor([22202.2109], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2256925106048584\n",
      "logits: tensor([7040.3105], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11132447421550751\n",
      "logits: tensor([9926.3867], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4558753967285156\n",
      "logits: tensor([15406.4482], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.431932806968689\n",
      "logits: tensor([16544.4590], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.611569881439209\n",
      "logits: tensor([9901.2646], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5629307627677917\n",
      "logits: tensor([16096.3203], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.540830373764038\n",
      "logits: tensor([27947.2383], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5319552421569824\n",
      "logits: tensor([31530.4980], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.740668773651123\n",
      "logits: tensor([17033.1152], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.6887050867080688\n",
      "logits: tensor([6414.1152], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.012478525750339031\n",
      "logits: tensor([12566.0215], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.98356693983078\n",
      "logits: tensor([10576.4482], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.41611790657043457\n",
      "logits: tensor([5749.0200], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6826203465461731\n",
      "logits: tensor([4094.7117], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3536430895328522\n",
      "logits: tensor([13354.9346], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.26272913813591003\n",
      "logits: tensor([9390.5420], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48524823784828186\n",
      "logits: tensor([5935.4038], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0630868524312973\n",
      "logits: tensor([10834.9590], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.40607109665870667\n",
      "logits: tensor([9052.5322], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5037765502929688\n",
      "logits: tensor([7310.6396], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.599260151386261\n",
      "logits: tensor([6872.4453], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08482667058706284\n",
      "logits: tensor([13653.1855], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25158724188804626\n",
      "logits: tensor([16325.8125], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10508459061384201\n",
      "logits: tensor([7434.7256], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17358353734016418\n",
      "logits: tensor([6756.4199], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06651187688112259\n",
      "logits: tensor([20055.1855], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10716409236192703\n",
      "logits: tensor([7861.8037], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5690475106239319\n",
      "Training, Epoch: 1, Batch 100: Loss = 107766232.0\n",
      "logits: tensor([14914.5352], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18244513869285583\n",
      "logits: tensor([17078.5469], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06382271647453308\n",
      "logits: tensor([11241.1074], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3794248104095459\n",
      "logits: tensor([9244.6035], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4592756927013397\n",
      "logits: tensor([11640.2656], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8374348878860474\n",
      "logits: tensor([10631.7207], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4130665361881256\n",
      "logits: tensor([10705.9609], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.41314226388931274\n",
      "logits: tensor([7474.2461], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1798219084739685\n",
      "logits: tensor([22162.0938], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21483688056468964\n",
      "logits: tensor([10677.8311], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6855130195617676\n",
      "logits: tensor([7795.5742], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5726779699325562\n",
      "logits: tensor([11004.9453], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3924623429775238\n",
      "logits: tensor([14060.6670], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22376856207847595\n",
      "logits: tensor([10331.0811], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6307779550552368\n",
      "logits: tensor([10839.4180], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4058266878128052\n",
      "logits: tensor([11727.3643], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3571530282497406\n",
      "logits: tensor([16003.2988], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1165238618850708\n",
      "logits: tensor([20501.4766], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13180197775363922\n",
      "logits: tensor([26379.3145], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.45629316568374634\n",
      "logits: tensor([31748.6582], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7403337955474854\n",
      "logits: tensor([14340.2480], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21392525732517242\n",
      "logits: tensor([15889.4814], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.5081803798675537\n",
      "logits: tensor([20951.1504], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1566266119480133\n",
      "logits: tensor([19286.9004], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05723034590482712\n",
      "logits: tensor([16096.8135], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1113613024353981\n",
      "logits: tensor([21067.8887], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16307125985622406\n",
      "logits: tensor([7868.5791], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24206800758838654\n",
      "logits: tensor([9955.4229], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5714796781539917\n",
      "logits: tensor([7353.5068], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16076302528381348\n",
      "logits: tensor([9728.1641], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.466741144657135\n",
      "logits: tensor([10211.8154], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4362477660179138\n",
      "logits: tensor([8776.1240], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.515506386756897\n",
      "logits: tensor([11134.6201], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.38964489102363586\n",
      "logits: tensor([8174.7427], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5487061738967896\n",
      "logits: tensor([5142.9722], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18817347288131714\n",
      "logits: tensor([13971.6436], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23413066565990448\n",
      "logits: tensor([12929.7568], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29124271869659424\n",
      "logits: tensor([11970.8164], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3391405940055847\n",
      "logits: tensor([7936.7695], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5649381875991821\n",
      "logits: tensor([13915.1045], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23180446028709412\n",
      "logits: tensor([9654.4346], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47078269720077515\n",
      "logits: tensor([17796.2812], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.017540697008371353\n",
      "logits: tensor([10767.1201], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4055916965007782\n",
      "logits: tensor([20943.6660], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1480475813150406\n",
      "logits: tensor([25733.5898], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.41061195731163025\n",
      "logits: tensor([13208.2344], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.27082785964012146\n",
      "logits: tensor([19417.4609], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07195794582366943\n",
      "logits: tensor([12896.3594], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0357112884521484\n",
      "logits: tensor([23311.5762], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2869359850883484\n",
      "logits: tensor([23886.0957], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.31865283846855164\n",
      "Training, Epoch: 1, Batch 150: Loss = 33316930.0\n",
      "logits: tensor([17367.0332], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.048009056597948074\n",
      "logits: tensor([11441.3838], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3728293478488922\n",
      "logits: tensor([14280.2686], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21721307933330536\n",
      "logits: tensor([17726.2305], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.028319304808974266\n",
      "logits: tensor([14905.8857], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17710751295089722\n",
      "logits: tensor([20849.4922], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14288534224033356\n",
      "logits: tensor([18494.8438], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.021024057641625404\n",
      "logits: tensor([15957.7773], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11903691291809082\n",
      "logits: tensor([13176.0703], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0798641443252563\n",
      "logits: tensor([20020.2461], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09742940962314606\n",
      "logits: tensor([17153.6250], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.053019098937511444\n",
      "logits: tensor([16139.4688], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10900647938251495\n",
      "logits: tensor([16512.1289], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09487147629261017\n",
      "logits: tensor([11324.8828], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7876511812210083\n",
      "logits: tensor([16164.5713], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10762067884206772\n",
      "logits: tensor([10501.4219], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6576665043830872\n",
      "logits: tensor([10780.5850], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.701732873916626\n",
      "logits: tensor([17520.7852], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03274970129132271\n",
      "logits: tensor([11532.1797], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8203733563423157\n",
      "logits: tensor([11931.4990], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8834065198898315\n",
      "logits: tensor([12746.1436], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30130764842033386\n",
      "logits: tensor([9634.7598], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5208625197410583\n",
      "logits: tensor([12103.8115], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.33651772141456604\n",
      "logits: tensor([12467.5547], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.31657877564430237\n",
      "logits: tensor([14541.7783], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19720838963985443\n",
      "logits: tensor([9839.2158], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5531362295150757\n",
      "logits: tensor([10203.5020], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4406850337982178\n",
      "logits: tensor([14160.3145], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21826742589473724\n",
      "logits: tensor([11756.7080], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3509606420993805\n",
      "logits: tensor([13210.1885], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0852497816085815\n",
      "logits: tensor([13980.3584], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23365294933319092\n",
      "logits: tensor([11199.8369], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7679125070571899\n",
      "logits: tensor([11168.1523], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7629110217094421\n",
      "logits: tensor([16386.7871], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1017422080039978\n",
      "logits: tensor([13965.7373], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2344544231891632\n",
      "logits: tensor([19760.4492], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09089291840791702\n",
      "logits: tensor([14048.0186], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.2175025939941406\n",
      "logits: tensor([17423.6699], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03811103105545044\n",
      "logits: tensor([18093.7461], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.008173576556146145\n",
      "logits: tensor([17638.3906], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0331343337893486\n",
      "logits: tensor([19130.6133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.048663314431905746\n",
      "logits: tensor([18121.9570], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.00043852010276168585\n",
      "logits: tensor([13554.6660], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25170278549194336\n",
      "logits: tensor([15026.3105], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17631806433200836\n",
      "logits: tensor([18860.2559], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.041196953505277634\n",
      "logits: tensor([17302.1055], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04482210427522659\n",
      "logits: tensor([18156.1562], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.004752502776682377\n",
      "logits: tensor([8604.7607], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.35827547311782837\n",
      "logits: tensor([16343.8799], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09772178530693054\n",
      "logits: tensor([13454.6475], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.123837947845459\n",
      "Training, Epoch: 1, Batch 200: Loss = 50688484.0\n",
      "logits: tensor([19958.1699], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09402664005756378\n",
      "logits: tensor([14726.5752], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19274835288524628\n",
      "logits: tensor([14986.7344], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17264419794082642\n",
      "logits: tensor([16417.6562], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09364889562129974\n",
      "logits: tensor([17588.5098], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03586860001087189\n",
      "logits: tensor([18565.4043], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0249194148927927\n",
      "logits: tensor([11041.5371], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7429245710372925\n",
      "logits: tensor([14159.5879], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21830753982067108\n",
      "logits: tensor([18825.0449], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.031913284212350845\n",
      "logits: tensor([16552.2793], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09267058968544006\n",
      "logits: tensor([20439.1699], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12039312720298767\n",
      "logits: tensor([14114.1826], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.2279466390609741\n",
      "logits: tensor([13523.2510], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.134667158126831\n",
      "logits: tensor([14626.5146], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19253043830394745\n",
      "logits: tensor([17629.3359], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02675705961883068\n",
      "logits: tensor([15150.6787], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1695006936788559\n",
      "logits: tensor([14271.2295], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21770857274532318\n",
      "logits: tensor([16683.9219], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08545447140932083\n",
      "logits: tensor([12743.6797], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0116106271743774\n",
      "logits: tensor([14216.4541], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21516819298267365\n",
      "logits: tensor([16840.7461], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07685799896717072\n",
      "logits: tensor([11413.6123], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8016572594642639\n",
      "logits: tensor([10923.0596], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.724222719669342\n",
      "logits: tensor([16525.5098], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08769474923610687\n",
      "logits: tensor([14764.3594], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19067716598510742\n",
      "logits: tensor([14914.2246], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1824621558189392\n",
      "logits: tensor([15751.0068], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13659311830997467\n",
      "logits: tensor([14108.3809], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22663527727127075\n",
      "logits: tensor([13762.3535], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1724098920822144\n",
      "logits: tensor([12766.3193], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0151842832565308\n",
      "logits: tensor([12013.5693], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8963614702224731\n",
      "logits: tensor([16828.6211], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07096122205257416\n",
      "logits: tensor([14998.7539], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17198064923286438\n",
      "logits: tensor([14181.2871], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21710962057113647\n",
      "logits: tensor([14549.2031], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19679848849773407\n",
      "logits: tensor([11131.1719], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7570735812187195\n",
      "logits: tensor([15379.1885], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15697471797466278\n",
      "logits: tensor([14600.4668], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19396843016147614\n",
      "logits: tensor([16653.8398], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08061017841100693\n",
      "logits: tensor([12285.1172], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9392257332801819\n",
      "logits: tensor([13515.8311], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1334959268569946\n",
      "logits: tensor([18067.2559], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0025813060346990824\n",
      "logits: tensor([12860.0283], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2950649559497833\n",
      "logits: tensor([17796.2402], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.017542961984872818\n",
      "logits: tensor([18148.6250], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0019107486587017775\n",
      "logits: tensor([14735.3330], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18652302026748657\n",
      "logits: tensor([14663.7256], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19619351625442505\n",
      "logits: tensor([18558.8438], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.017321206629276276\n",
      "logits: tensor([19082.6367], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.053473684936761856\n",
      "logits: tensor([13166.0605], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0782840251922607\n",
      "Training, Epoch: 1, Batch 250: Loss = 46662528.0\n",
      "logits: tensor([17607.9766], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.034801509231328964\n",
      "logits: tensor([18094.1016], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.00815409142524004\n",
      "logits: tensor([18299.6914], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0031155175529420376\n",
      "logits: tensor([17992.6660], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0066991038620471954\n",
      "logits: tensor([11391.1055], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7981045246124268\n",
      "logits: tensor([12829.7012], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0251891613006592\n",
      "logits: tensor([17836.8535], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.015300869010388851\n",
      "logits: tensor([11572.9492], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8268088698387146\n",
      "logits: tensor([10861.8350], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7145583033561707\n",
      "logits: tensor([14406.0547], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20470112562179565\n",
      "logits: tensor([14425.9990], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20922473073005676\n",
      "logits: tensor([13803.3242], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23797538876533508\n",
      "logits: tensor([14179.9814], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22271041572093964\n",
      "logits: tensor([11221.1182], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7712717652320862\n",
      "logits: tensor([10679.3877], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6857587099075317\n",
      "logits: tensor([14598.8496], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19405771791934967\n",
      "logits: tensor([15176.5186], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1621669977903366\n",
      "logits: tensor([11888.5791], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8766315579414368\n",
      "logits: tensor([11149.5498], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7599745988845825\n",
      "logits: tensor([16332.5742], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10471393913030624\n",
      "logits: tensor([15444.7520], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1473589390516281\n",
      "logits: tensor([15879.5801], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12335386127233505\n",
      "logits: tensor([18025.9414], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.011890357360243797\n",
      "logits: tensor([17054.9043], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.065118707716465\n",
      "logits: tensor([14507.2461], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19911476969718933\n",
      "logits: tensor([11660.8281], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.840680718421936\n",
      "logits: tensor([12970.5322], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0474196672439575\n",
      "logits: tensor([15270.5713], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16292867064476013\n",
      "logits: tensor([11421.9668], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8029760718345642\n",
      "logits: tensor([15335.6436], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15936167538166046\n",
      "logits: tensor([15042.6074], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1754247397184372\n",
      "logits: tensor([16840.3633], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0703129842877388\n",
      "logits: tensor([22407.4531], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23702308535575867\n",
      "logits: tensor([12667.4453], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9995768666267395\n",
      "logits: tensor([17657.0215], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03211306408047676\n",
      "logits: tensor([13541.4287], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25771331787109375\n",
      "logits: tensor([17260.2539], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04713255539536476\n",
      "logits: tensor([17331.5488], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0431966558098793\n",
      "logits: tensor([15015.1035], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17693237960338593\n",
      "logits: tensor([12854.5664], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29035240411758423\n",
      "logits: tensor([13973.1328], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23404903709888458\n",
      "logits: tensor([10464.2607], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6518005728721619\n",
      "logits: tensor([15149.2383], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1695796549320221\n",
      "logits: tensor([18710.5293], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.032931167632341385\n",
      "logits: tensor([13174.4639], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0796105861663818\n",
      "logits: tensor([19443.6133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06582071632146835\n",
      "logits: tensor([12811.9004], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0223792791366577\n",
      "logits: tensor([17354.7207], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.048683978617191315\n",
      "logits: tensor([11350.4111], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7916808724403381\n",
      "logits: tensor([16033.2812], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12111997604370117\n",
      "Training, Epoch: 1, Batch 300: Loss = 4882218.0\n",
      "logits: tensor([10399.4346], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6415676474571228\n",
      "logits: tensor([16835.7754], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07056626677513123\n",
      "logits: tensor([10360.0947], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6353577971458435\n",
      "logits: tensor([13808.1445], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23770928382873535\n",
      "logits: tensor([13490.2637], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.255258172750473\n",
      "logits: tensor([16272.4609], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10166453570127487\n",
      "logits: tensor([17012.1211], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06083094701170921\n",
      "logits: tensor([17110.3066], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.062081772834062576\n",
      "logits: tensor([16379.2910], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10215311497449875\n",
      "logits: tensor([14658.5967], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19647465646266937\n",
      "logits: tensor([10967.6875], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7312673330307007\n",
      "logits: tensor([16495.3496], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08935976773500443\n",
      "logits: tensor([17234.4316], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04855809733271599\n",
      "logits: tensor([19693.1309], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07949826866388321\n",
      "logits: tensor([18734.1016], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.026928136125206947\n",
      "logits: tensor([20093.3984], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10143932700157166\n",
      "logits: tensor([11587.3730], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8290857076644897\n",
      "logits: tensor([15847.9287], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1251012086868286\n",
      "logits: tensor([14916.5576], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1823342740535736\n",
      "logits: tensor([11100.9600], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7523046135902405\n",
      "logits: tensor([13620.0547], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25340336561203003\n",
      "logits: tensor([17540.7754], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03164612129330635\n",
      "logits: tensor([15353.4463], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15239953994750977\n",
      "logits: tensor([17409.4824], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03889426589012146\n",
      "logits: tensor([11673.9434], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8427509665489197\n",
      "logits: tensor([12233.0391], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9310051202774048\n",
      "logits: tensor([11840.2617], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8690045475959778\n",
      "logits: tensor([17093.6133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.056332096457481384\n",
      "logits: tensor([14706.2773], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1938609927892685\n",
      "logits: tensor([9715.2559], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5335689187049866\n",
      "logits: tensor([13422.9121], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.118828535079956\n",
      "logits: tensor([10373.9336], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6375423073768616\n",
      "logits: tensor([9193.0938], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.451144814491272\n",
      "logits: tensor([12972.3447], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2889082133769989\n",
      "logits: tensor([12853.6562], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2954142391681671\n",
      "logits: tensor([12602.7139], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3042561411857605\n",
      "logits: tensor([8820.8447], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.39238467812538147\n",
      "logits: tensor([10590.0400], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6716550588607788\n",
      "logits: tensor([14460.3477], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20734187960624695\n",
      "logits: tensor([14746.0332], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18593230843544006\n",
      "logits: tensor([10855.2529], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7135193347930908\n",
      "logits: tensor([10018.4219], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5814241766929626\n",
      "logits: tensor([9376.2725], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4800598621368408\n",
      "logits: tensor([17055.7012], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06507502496242523\n",
      "logits: tensor([17011.0312], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06752365082502365\n",
      "logits: tensor([11179.3887], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7646846771240234\n",
      "logits: tensor([8590.2637], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3559871017932892\n",
      "logits: tensor([9867.7578], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5576416254043579\n",
      "logits: tensor([9985.3291], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5762004256248474\n",
      "logits: tensor([10513.0137], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6594963073730469\n",
      "Training, Epoch: 1, Batch 350: Loss = 17455272.0\n",
      "logits: tensor([16478.1348], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09031012654304504\n",
      "logits: tensor([16492.0508], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08954188227653503\n",
      "logits: tensor([12113.4092], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.33126863837242126\n",
      "logits: tensor([10821.9785], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7082669138908386\n",
      "logits: tensor([11710.8301], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3580593764781952\n",
      "logits: tensor([16004.5879], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12269283086061478\n",
      "logits: tensor([10299.5439], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6257997751235962\n",
      "logits: tensor([14293.4961], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21091501414775848\n",
      "logits: tensor([10108.1768], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5955921411514282\n",
      "logits: tensor([16337.9902], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0980469286441803\n",
      "logits: tensor([17067.8320], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06441006064414978\n",
      "logits: tensor([13790.9971], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2386559247970581\n",
      "logits: tensor([16359.8262], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09684146195650101\n",
      "logits: tensor([14931.2510], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17570720613002777\n",
      "logits: tensor([20303.9883], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12089946866035461\n",
      "logits: tensor([10988.8457], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7346071600914001\n",
      "logits: tensor([12411.5020], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9591757655143738\n",
      "logits: tensor([21160.4180], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16817942261695862\n",
      "logits: tensor([13031.9189], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0571095943450928\n",
      "logits: tensor([9835.9502], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5526207685470581\n",
      "logits: tensor([9046.6572], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4280295670032501\n",
      "logits: tensor([12069.0859], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3384212255477905\n",
      "logits: tensor([18067.1055], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0025896085426211357\n",
      "logits: tensor([16955.3555], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07057557255029678\n",
      "logits: tensor([9428.5410], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4883105456829071\n",
      "logits: tensor([17286.0801], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04570680111646652\n",
      "logits: tensor([7843.1147], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23804841935634613\n",
      "logits: tensor([17045.5332], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.058986399322748184\n",
      "logits: tensor([17372.5703], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04770553484559059\n",
      "logits: tensor([17259.5176], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.053902629762887955\n",
      "logits: tensor([13748.2432], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2410161942243576\n",
      "logits: tensor([18678.8477], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.023899339139461517\n",
      "logits: tensor([16513.4922], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09479673951864243\n",
      "logits: tensor([11233.3730], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3798517882823944\n",
      "logits: tensor([18930.0820], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03767099604010582\n",
      "logits: tensor([18456.3652], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01170374732464552\n",
      "logits: tensor([18136.1074], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.005851498804986477\n",
      "logits: tensor([15497.1416], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14446671307086945\n",
      "logits: tensor([19738.0938], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.081962950527668\n",
      "logits: tensor([14143.0283], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.2325000762939453\n",
      "logits: tensor([18295.5801], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0028901512268930674\n",
      "logits: tensor([18835.5547], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03983330354094505\n",
      "logits: tensor([9599.2412], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.515255868434906\n",
      "logits: tensor([16798.4434], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07262720912694931\n",
      "logits: tensor([10957.2920], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7296263575553894\n",
      "logits: tensor([13102.4521], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2817762494087219\n",
      "logits: tensor([10371.3066], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6371276378631592\n",
      "logits: tensor([12813.7705], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2976006269454956\n",
      "logits: tensor([12081.0723], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.337764173746109\n",
      "logits: tensor([10511.4941], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6592564582824707\n",
      "Training, Epoch: 1, Batch 400: Loss = 17442578.0\n",
      "logits: tensor([10519.2080], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6604741215705872\n",
      "logits: tensor([18530.6660], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01577661745250225\n",
      "logits: tensor([19035.6641], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04345858097076416\n",
      "logits: tensor([14949.4658], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18053038418293\n",
      "logits: tensor([13709.3779], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24316178262233734\n",
      "logits: tensor([11621.8418], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8345266580581665\n",
      "logits: tensor([19615.7520], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08290477842092514\n",
      "logits: tensor([20617.2812], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13819508254528046\n",
      "logits: tensor([20407.3457], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11864865571260452\n",
      "logits: tensor([17607.4453], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.027965549379587173\n",
      "logits: tensor([12682.6494], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0019768476486206\n",
      "logits: tensor([10558.1504], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.666621208190918\n",
      "logits: tensor([14386.3008], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2057916522026062\n",
      "logits: tensor([7141.5479], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12730494141578674\n",
      "logits: tensor([13317.0029], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2700154483318329\n",
      "logits: tensor([7220.1201], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13970771431922913\n",
      "logits: tensor([12124.4492], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3306591510772705\n",
      "logits: tensor([13648.0146], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25187069177627563\n",
      "logits: tensor([10139.6201], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6005555391311646\n",
      "logits: tensor([9969.8496], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.573756992816925\n",
      "logits: tensor([15805.5732], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13360200822353363\n",
      "logits: tensor([9696.8076], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5306568741798401\n",
      "logits: tensor([13141.6416], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.274504154920578\n",
      "logits: tensor([14461.1152], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20729979872703552\n",
      "logits: tensor([13510.7285], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2541283965110779\n",
      "logits: tensor([16489.1113], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08970415592193604\n",
      "logits: tensor([16751.2891], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07523040473461151\n",
      "logits: tensor([18031.3652], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.011593044735491276\n",
      "logits: tensor([13035.2568], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0576364994049072\n",
      "logits: tensor([22609.1211], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2393411248922348\n",
      "logits: tensor([22908.5703], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2557557225227356\n",
      "logits: tensor([19160.5000], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0577721931040287\n",
      "logits: tensor([21887.0254], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2082924246788025\n",
      "logits: tensor([18424.7852], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.009972654283046722\n",
      "logits: tensor([11039.6279], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7426232099533081\n",
      "logits: tensor([16157.8760], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11429019272327423\n",
      "logits: tensor([15391.5859], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15629513561725616\n",
      "logits: tensor([9386.6318], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48169511556625366\n",
      "logits: tensor([15255.4111], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15781165659427643\n",
      "logits: tensor([13255.4209], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2682228684425354\n",
      "logits: tensor([13763.8662], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2455201894044876\n",
      "logits: tensor([14125.5566], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22569377720355988\n",
      "logits: tensor([14736.6846], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19219419360160828\n",
      "logits: tensor([14001.8125], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22701767086982727\n",
      "logits: tensor([17062.2910], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05806127190589905\n",
      "logits: tensor([18494.1973], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02098836936056614\n",
      "logits: tensor([22127.1016], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21291875839233398\n",
      "logits: tensor([17333.4844], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04308980330824852\n",
      "logits: tensor([16222.2295], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10443760454654694\n",
      "logits: tensor([22926.7715], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25675344467163086\n",
      "Training, Epoch: 1, Batch 450: Loss = 21939070.0\n",
      "logits: tensor([24220.8848], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3371351659297943\n",
      "logits: tensor([10586.8018], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6711438894271851\n",
      "logits: tensor([11278.4873], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.780327558517456\n",
      "logits: tensor([10279.6836], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6226648092269897\n",
      "logits: tensor([10343.8848], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6327990293502808\n",
      "logits: tensor([15056.6064], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17465735971927643\n",
      "logits: tensor([10963.5479], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3990223705768585\n",
      "logits: tensor([12808.9609], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29786425828933716\n",
      "logits: tensor([13674.3633], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24509479105472565\n",
      "logits: tensor([10736.1348], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.40730226039886475\n",
      "logits: tensor([8161.7866], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28835129737854004\n",
      "logits: tensor([12322.8828], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.31970444321632385\n",
      "logits: tensor([15107.4053], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1659824550151825\n",
      "logits: tensor([12921.3193], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2917052209377289\n",
      "logits: tensor([15622.8262], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.137528195977211\n",
      "logits: tensor([18951.0410], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03881988301873207\n",
      "logits: tensor([11404.1377], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8001617193222046\n",
      "logits: tensor([12439.1592], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9635415077209473\n",
      "logits: tensor([15449.2754], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.153132826089859\n",
      "logits: tensor([19073.8086], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05298632010817528\n",
      "logits: tensor([14739.0361], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.3265807628631592\n",
      "logits: tensor([24032.3047], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.31735432147979736\n",
      "logits: tensor([11641.3838], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8376113772392273\n",
      "logits: tensor([20689.1875], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14216472208499908\n",
      "logits: tensor([17700.8789], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.022807467728853226\n",
      "logits: tensor([18539.4688], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.016259146854281425\n",
      "logits: tensor([17465.3750], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04261835291981697\n",
      "logits: tensor([11450.6318], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8075008392333984\n",
      "logits: tensor([11668.6514], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8419156074523926\n",
      "logits: tensor([15320.7578], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15420414507389069\n",
      "logits: tensor([9718.0615], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5340118408203125\n",
      "logits: tensor([13157.2061], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.27877485752105713\n",
      "logits: tensor([8126.8887], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28284260630607605\n",
      "logits: tensor([12465.3164], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3167014718055725\n",
      "logits: tensor([8782.2822], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3862975239753723\n",
      "logits: tensor([11253.1396], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3831481337547302\n",
      "logits: tensor([14408.3828], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21019037067890167\n",
      "logits: tensor([13877.7148], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2338685840368271\n",
      "logits: tensor([9875.2510], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5588244199752808\n",
      "logits: tensor([18035.9180], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.011343481950461864\n",
      "logits: tensor([16861.0293], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06917209923267365\n",
      "logits: tensor([7916.5635], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24964243173599243\n",
      "logits: tensor([19306.7637], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05831917002797127\n",
      "logits: tensor([14989.3096], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17834630608558655\n",
      "logits: tensor([20321.5352], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11394486576318741\n",
      "logits: tensor([19806.6133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08571891486644745\n",
      "logits: tensor([14995.0195], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17218680679798126\n",
      "logits: tensor([18668.2930], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.023320773616433144\n",
      "logits: tensor([12741.3037], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3015729486942291\n",
      "logits: tensor([21658.5371], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18723393976688385\n",
      "Training, Epoch: 1, Batch 500: Loss = 11666881.0\n",
      "logits: tensor([15408.4766], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14936155080795288\n",
      "logits: tensor([10122.6973], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5978842377662659\n",
      "logits: tensor([14565.1143], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20159898698329926\n",
      "logits: tensor([21441.7324], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17534957826137543\n",
      "logits: tensor([13813.9082], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24277707934379578\n",
      "logits: tensor([18956.1191], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0464891716837883\n",
      "logits: tensor([12585.2344], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9865997433662415\n",
      "logits: tensor([23788.3281], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3132554888725281\n",
      "logits: tensor([11627.7871], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.835465133190155\n",
      "logits: tensor([12615.1611], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.991323709487915\n",
      "logits: tensor([12258.5273], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3280368149280548\n",
      "logits: tensor([15530.6885], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14261473715305328\n",
      "logits: tensor([10236.4121], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6158342957496643\n",
      "logits: tensor([11244.7129], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3792257606983185\n",
      "logits: tensor([13836.5947], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24153350293636322\n",
      "logits: tensor([14699.9326], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18847733736038208\n",
      "logits: tensor([10744.8203], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4110121428966522\n",
      "logits: tensor([9942.2695], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5694034099578857\n",
      "logits: tensor([14866.5322], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18507646024227142\n",
      "logits: tensor([9961.1318], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5723808407783508\n",
      "logits: tensor([16528.0938], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09399634599685669\n",
      "logits: tensor([14578.1963], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20088188350200653\n",
      "logits: tensor([14923.5430], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17613273859024048\n",
      "logits: tensor([11896.2148], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8778368830680847\n",
      "logits: tensor([17913.6523], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.011061122640967369\n",
      "logits: tensor([18262.7656], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.008211981505155563\n",
      "logits: tensor([10260.7061], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6196691393852234\n",
      "logits: tensor([20341.0078], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12294316291809082\n",
      "logits: tensor([20584.2266], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13637027144432068\n",
      "logits: tensor([10617.6836], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6760186553001404\n",
      "logits: tensor([16409.1016], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10051901638507843\n",
      "logits: tensor([19854.8203], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08836143463850021\n",
      "logits: tensor([14073.8594], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22852760553359985\n",
      "logits: tensor([19982.2812], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10313934832811356\n",
      "logits: tensor([9169.9863], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.44749724864959717\n",
      "logits: tensor([14174.3525], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22301897406578064\n",
      "logits: tensor([16241.0889], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10972879827022552\n",
      "logits: tensor([10722.3408], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6925389170646667\n",
      "logits: tensor([17019.8750], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06040288507938385\n",
      "logits: tensor([11190.6621], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7664642333984375\n",
      "logits: tensor([9302.7578], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4684554636478424\n",
      "logits: tensor([16044.1074], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12052652984857559\n",
      "logits: tensor([12527.3174], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3084184527397156\n",
      "logits: tensor([11234.9463], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7734545469284058\n",
      "logits: tensor([14492.8740], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19990818202495575\n",
      "logits: tensor([15899.7236], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12224182486534119\n",
      "logits: tensor([13324.8398], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2643905282020569\n",
      "logits: tensor([17451.3867], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.036580901592969894\n",
      "logits: tensor([18058.7305], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.010092992335557938\n",
      "logits: tensor([11047.8066], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7439142465591431\n",
      "Training, Epoch: 1, Batch 550: Loss = 22209952.0\n",
      "logits: tensor([19964.3535], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10214963555335999\n",
      "logits: tensor([15568.2246], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14054252207279205\n",
      "logits: tensor([17513.1055], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04000196233391762\n",
      "logits: tensor([18710.2832], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.032917581498622894\n",
      "logits: tensor([11740.4229], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8532448410987854\n",
      "logits: tensor([18279.7969], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.002024979330599308\n",
      "logits: tensor([14286.3564], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21130916476249695\n",
      "logits: tensor([17604.5664], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.034988440573215485\n",
      "logits: tensor([17718.4199], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.028747448697686195\n",
      "logits: tensor([19304.1934], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05817827582359314\n",
      "logits: tensor([12601.4355], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9891570806503296\n",
      "logits: tensor([12759.4609], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0141016244888306\n",
      "logits: tensor([16769.0840], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08078622817993164\n",
      "logits: tensor([10045.5967], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.585713803768158\n",
      "logits: tensor([16493.5605], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09588931500911713\n",
      "logits: tensor([10103.2412], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5948130488395691\n",
      "logits: tensor([17750.6289], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02006097510457039\n",
      "logits: tensor([14462.4658], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2072257697582245\n",
      "logits: tensor([16433.3438], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09919016063213348\n",
      "logits: tensor([14890.2686], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18377533555030823\n",
      "logits: tensor([10484.8486], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6550503969192505\n",
      "logits: tensor([10965.9189], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7309881448745728\n",
      "logits: tensor([11759.6191], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3553849458694458\n",
      "logits: tensor([10211.9678], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6119757294654846\n",
      "logits: tensor([13907.6318], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23763953149318695\n",
      "logits: tensor([15826.3945], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13246066868305206\n",
      "logits: tensor([17567.0469], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.030195781961083412\n",
      "logits: tensor([10973.0869], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.732119619846344\n",
      "logits: tensor([18094.9902], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0010502055520191789\n",
      "logits: tensor([17310.1816], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.051125429570674896\n",
      "logits: tensor([13137.3438], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2798636257648468\n",
      "logits: tensor([11582.3086], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8282862305641174\n",
      "logits: tensor([14816.4785], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18204331398010254\n",
      "logits: tensor([17098.4043], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05606760457158089\n",
      "logits: tensor([20505.0391], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1240038126707077\n",
      "logits: tensor([12107.0850], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9111230373382568\n",
      "logits: tensor([23327.1445], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2787002921104431\n",
      "logits: tensor([20717.9180], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13567297160625458\n",
      "logits: tensor([10955.8037], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7293914556503296\n",
      "logits: tensor([9424.0234], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48759743571281433\n",
      "logits: tensor([9540.0479], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5059120655059814\n",
      "logits: tensor([15230.3818], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15919342637062073\n",
      "logits: tensor([13329.9609], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2641078233718872\n",
      "logits: tensor([12804.8330], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29309797286987305\n",
      "logits: tensor([11771.5742], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.35013991594314575\n",
      "logits: tensor([15118.1240], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1712852120399475\n",
      "logits: tensor([9241.6270], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4588058590888977\n",
      "logits: tensor([14442.0312], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20271500945091248\n",
      "logits: tensor([14071.1094], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22319208085536957\n",
      "logits: tensor([15856.3174], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13082042336463928\n",
      "Training, Epoch: 1, Batch 600: Loss = 5695564.0\n",
      "logits: tensor([17424.4375], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.038068655878305435\n",
      "logits: tensor([14786.2783], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18371054530143738\n",
      "logits: tensor([11741.0352], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8533415198326111\n",
      "logits: tensor([20123.9785], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11096186935901642\n",
      "logits: tensor([10285.2949], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.623550534248352\n",
      "logits: tensor([3006.9834], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8339968323707581\n",
      "logits: tensor([14284.8359], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.2548846006393433\n",
      "logits: tensor([23348.9375], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.288998544216156\n",
      "logits: tensor([19620.9199], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08319008350372314\n",
      "logits: tensor([20664.9375], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13276879489421844\n",
      "logits: tensor([16274.4131], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10155676305294037\n",
      "logits: tensor([24423.1445], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.34830111265182495\n",
      "logits: tensor([16720.7949], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07691386342048645\n",
      "logits: tensor([17725.0078], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.028386326506733894\n",
      "logits: tensor([17189.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05776812136173248\n",
      "logits: tensor([12907.2646], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2924756407737732\n",
      "logits: tensor([15206.2314], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16645552217960358\n",
      "logits: tensor([9753.2520], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.539566695690155\n",
      "logits: tensor([12195.7168], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3314798176288605\n",
      "logits: tensor([15016.9492], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1709761619567871\n",
      "logits: tensor([16339.4385], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1043376699090004\n",
      "logits: tensor([8623.9316], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.36130163073539734\n",
      "logits: tensor([13519.0146], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25367096066474915\n",
      "logits: tensor([10917.5596], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7233545184135437\n",
      "logits: tensor([15074.0576], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17370076477527618\n",
      "logits: tensor([13991.6240], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22758013010025024\n",
      "logits: tensor([8038.8853], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2689511179924011\n",
      "logits: tensor([14122.7256], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2258489578962326\n",
      "logits: tensor([14696.1641], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1944153606891632\n",
      "logits: tensor([11079.2598], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7488791942596436\n",
      "logits: tensor([23645.6055], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3053763806819916\n",
      "logits: tensor([22659.3125], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.250927209854126\n",
      "logits: tensor([9496.9717], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4757113456726074\n",
      "logits: tensor([12849.8105], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0283634662628174\n",
      "logits: tensor([23067.8086], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.27347859740257263\n",
      "logits: tensor([9542.3594], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5062769651412964\n",
      "logits: tensor([10070.6924], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5896751880645752\n",
      "logits: tensor([8987.9844], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4187679588794708\n",
      "logits: tensor([9113.8662], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4386385977268219\n",
      "logits: tensor([7419.6553], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17120465636253357\n",
      "logits: tensor([9704.0801], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5318048000335693\n",
      "logits: tensor([12018.1475], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.33652764558792114\n",
      "logits: tensor([7915.9702], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2495487779378891\n",
      "logits: tensor([9835.9629], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4569970369338989\n",
      "logits: tensor([12016.1768], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.34132149815559387\n",
      "logits: tensor([10158.8896], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.43916958570480347\n",
      "logits: tensor([11033.0879], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.39521047472953796\n",
      "logits: tensor([9214.8877], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4545850157737732\n",
      "logits: tensor([8758.8691], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3826017379760742\n",
      "logits: tensor([9140.0322], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.44276896119117737\n",
      "Training, Epoch: 1, Batch 650: Loss = 7867852.5\n",
      "logits: tensor([13073.3213], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.283373087644577\n",
      "logits: tensor([13218.3477], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2754233181476593\n",
      "logits: tensor([11043.9941], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.743312418460846\n",
      "logits: tensor([14500.0996], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19950929284095764\n",
      "logits: tensor([10907.4541], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7217593789100647\n",
      "logits: tensor([15582.3633], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14583748579025269\n",
      "logits: tensor([18800.4492], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03789527714252472\n",
      "logits: tensor([17639.5996], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.026190444827079773\n",
      "logits: tensor([18441.0723], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01805555634200573\n",
      "logits: tensor([11972.6738], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8899060487747192\n",
      "logits: tensor([17946.9668], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.009221969172358513\n",
      "logits: tensor([11468.2236], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8102777600288391\n",
      "logits: tensor([18166.6152], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0029039152432233095\n",
      "logits: tensor([17915.8848], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.010937879793345928\n",
      "logits: tensor([12346.0059], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9488371014595032\n",
      "logits: tensor([16826.7227], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07762670516967773\n",
      "logits: tensor([11377.5156], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7959593534469604\n",
      "logits: tensor([17110.4180], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05540437996387482\n",
      "logits: tensor([12493.6250], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9721390604972839\n",
      "logits: tensor([15263.4629], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15736715495586395\n",
      "logits: tensor([12540.7266], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9795740842819214\n",
      "logits: tensor([11379.1953], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7962244749069214\n",
      "logits: tensor([8457.5391], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3350363075733185\n",
      "logits: tensor([13904.9355], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23236584663391113\n",
      "logits: tensor([8270.3750], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3054921627044678\n",
      "logits: tensor([8512.8213], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.34376269578933716\n",
      "logits: tensor([13382.7568], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.261193186044693\n",
      "logits: tensor([10296.9863], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6253960728645325\n",
      "logits: tensor([7758.7490], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22473116219043732\n",
      "logits: tensor([10680.6230], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.41036683320999146\n",
      "logits: tensor([11345.7471], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.37807175517082214\n",
      "logits: tensor([7661.4165], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2093670666217804\n",
      "logits: tensor([12862.0879], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29495203495025635\n",
      "logits: tensor([13964.0244], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22910378873348236\n",
      "logits: tensor([9779.8945], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.543772280216217\n",
      "logits: tensor([12363.5029], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3174619972705841\n",
      "logits: tensor([15931.3906], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12670521438121796\n",
      "logits: tensor([18068.1133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.002533971332013607\n",
      "logits: tensor([17075.7109], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0639781728386879\n",
      "logits: tensor([16163.0439], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1077049970626831\n",
      "logits: tensor([13135.3877], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0734423398971558\n",
      "logits: tensor([22749.7461], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24704962968826294\n",
      "logits: tensor([12053.2031], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9026177525520325\n",
      "logits: tensor([12125.6035], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9140462279319763\n",
      "logits: tensor([18814.6953], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03134595975279808\n",
      "logits: tensor([10872.8809], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7163019180297852\n",
      "logits: tensor([10085.3916], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5919954776763916\n",
      "logits: tensor([15487.9062], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14497655630111694\n",
      "logits: tensor([14539.9141], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19731129705905914\n",
      "logits: tensor([14495.6299], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19975604116916656\n",
      "Training, Epoch: 1, Batch 700: Loss = 13092701.0\n",
      "logits: tensor([14079.6934], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2282078117132187\n",
      "logits: tensor([14857.8340], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.3453333377838135\n",
      "logits: tensor([14205.1006], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21579496562480927\n",
      "logits: tensor([12280.2227], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3220595419406891\n",
      "logits: tensor([15719.1807], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13220885396003723\n",
      "logits: tensor([8364.7852], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.32039493322372437\n",
      "logits: tensor([10545.8057], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6646725535392761\n",
      "logits: tensor([8866.8096], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3996403217315674\n",
      "logits: tensor([9391.0352], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4823901653289795\n",
      "logits: tensor([8878.6855], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4015149474143982\n",
      "logits: tensor([13654.7178], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25150325894355774\n",
      "logits: tensor([15689.5879], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13995987176895142\n",
      "logits: tensor([17678.9688], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.024017035961151123\n",
      "logits: tensor([17504.9336], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0336247980594635\n",
      "logits: tensor([12869.5498], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2945430278778076\n",
      "logits: tensor([15189.2354], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16738717257976532\n",
      "logits: tensor([15365.3203], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1517440229654312\n",
      "logits: tensor([10527.1191], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6617228984832764\n",
      "logits: tensor([17753.2324], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01991724595427513\n",
      "logits: tensor([12956.2246], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0451611280441284\n",
      "logits: tensor([11763.1465], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8568317890167236\n",
      "logits: tensor([22076.1562], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.218733549118042\n",
      "logits: tensor([21331.6699], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17763353884220123\n",
      "logits: tensor([9916.2637], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5652983784675598\n",
      "logits: tensor([11148.4336], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7597984075546265\n",
      "logits: tensor([13771.6738], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24509219825267792\n",
      "logits: tensor([15013.4619], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17116867005825043\n",
      "logits: tensor([7771.3325], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2267174869775772\n",
      "logits: tensor([8865.3027], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3994024693965912\n",
      "logits: tensor([6417.0815], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.012946762144565582\n",
      "logits: tensor([12826.5381], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29690074920654297\n",
      "logits: tensor([12897.5537], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2930079400539398\n",
      "logits: tensor([12437.3076], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3182367980480194\n",
      "logits: tensor([13839.6260], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24136734008789062\n",
      "logits: tensor([14545.2432], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20268824696540833\n",
      "logits: tensor([14465.2793], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20143157243728638\n",
      "logits: tensor([11770.3135], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8579631447792053\n",
      "logits: tensor([18199.3672], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0047120158560574055\n",
      "logits: tensor([20737.3066], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13673578202724457\n",
      "logits: tensor([10709.3252], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6904844045639038\n",
      "logits: tensor([11091.8350], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7508642077445984\n",
      "logits: tensor([14829.5430], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18132208287715912\n",
      "logits: tensor([19000.1523], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04892006143927574\n",
      "logits: tensor([17464.7734], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03584187477827072\n",
      "logits: tensor([19092.5918], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05402326211333275\n",
      "logits: tensor([18231.3047], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.006475153379142284\n",
      "logits: tensor([17165.7344], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05235058814287186\n",
      "logits: tensor([18864.4668], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03407423570752144\n",
      "logits: tensor([21335.7539], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17785899341106415\n",
      "logits: tensor([11874.2109], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8743635416030884\n",
      "Training, Epoch: 1, Batch 750: Loss = 30682160.0\n",
      "logits: tensor([19759.4277], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08313239365816116\n",
      "logits: tensor([17613.7500], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.027617493644356728\n",
      "logits: tensor([12137.8887], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9159854650497437\n",
      "logits: tensor([15707.7568], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13896392285823822\n",
      "logits: tensor([9878.1318], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.559279203414917\n",
      "logits: tensor([15993.6572], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11705613881349564\n",
      "logits: tensor([9285.8994], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.46579432487487793\n",
      "logits: tensor([13766.6914], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2399977296590805\n",
      "logits: tensor([14187.2568], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21678005158901215\n",
      "logits: tensor([12949.3955], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2901661992073059\n",
      "logits: tensor([13534.3643], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25810056924819946\n",
      "logits: tensor([13639.5781], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2523331642150879\n",
      "logits: tensor([10502.6924], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6578670740127563\n",
      "logits: tensor([9367.3994], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4786592423915863\n",
      "logits: tensor([7615.4233], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20210696756839752\n",
      "logits: tensor([10074.0332], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5902025103569031\n",
      "logits: tensor([15607.7793], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13835886120796204\n",
      "logits: tensor([14459.1338], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20740841329097748\n",
      "logits: tensor([9451.9668], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4920083284378052\n",
      "logits: tensor([13418.6338], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1181532144546509\n",
      "logits: tensor([22920.4863], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2564089298248291\n",
      "logits: tensor([22651.4141], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2504911720752716\n",
      "logits: tensor([17502.8379], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03374049440026283\n",
      "logits: tensor([9724.0879], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.534963071346283\n",
      "logits: tensor([15374.8984], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1572098731994629\n",
      "logits: tensor([9944.2334], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5697134137153625\n",
      "logits: tensor([8182.1499], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2915656864643097\n",
      "logits: tensor([6365.2793], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.004769693594425917\n",
      "logits: tensor([12488.2236], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.31544578075408936\n",
      "logits: tensor([12974.1787], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2837491035461426\n",
      "logits: tensor([12808.7910], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2978735566139221\n",
      "logits: tensor([14927.5361], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18173247575759888\n",
      "logits: tensor([10301.2588], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.43532639741897583\n",
      "logits: tensor([16227.5645], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10414308309555054\n",
      "logits: tensor([18294.1172], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.009942772798240185\n",
      "logits: tensor([12367.7129], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.952263593673706\n",
      "logits: tensor([10874.3848], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7165393233299255\n",
      "logits: tensor([10638.6670], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6793308854103088\n",
      "logits: tensor([8577.2744], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.35393673181533813\n",
      "logits: tensor([22424.9707], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22924675047397614\n",
      "logits: tensor([18010.5781], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.012732510454952717\n",
      "logits: tensor([17828.7500], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01574823074042797\n",
      "logits: tensor([18930.0781], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04505155235528946\n",
      "logits: tensor([16599.9883], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08358310163021088\n",
      "logits: tensor([13829.7402], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2419092357158661\n",
      "logits: tensor([10758.5283], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6982511878013611\n",
      "logits: tensor([17250.4199], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.054401326924562454\n",
      "logits: tensor([17739.2539], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.020688941702246666\n",
      "logits: tensor([14051.1592], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22429344058036804\n",
      "logits: tensor([15459.6816], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15256239473819733\n",
      "Training, Epoch: 1, Batch 800: Loss = 7746056.5\n",
      "logits: tensor([13899.1094], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23810669779777527\n",
      "logits: tensor([17917.2383], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.017849024385213852\n",
      "logits: tensor([13539.7793], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25252461433410645\n",
      "logits: tensor([18024.5449], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.011966906487941742\n",
      "logits: tensor([16045.4551], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12045265734195709\n",
      "logits: tensor([11620.8740], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8343738913536072\n",
      "logits: tensor([22992.8555], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2603759169578552\n",
      "logits: tensor([17621.3594], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03406791761517525\n",
      "logits: tensor([17025.8926], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06007067859172821\n",
      "logits: tensor([18449.3945], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01851499453186989\n",
      "logits: tensor([20982.7168], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15018817782402039\n",
      "logits: tensor([12459.6904], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9667823910713196\n",
      "logits: tensor([9692.1553], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5299224853515625\n",
      "logits: tensor([11818.9980], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.865648090839386\n",
      "logits: tensor([15655.5322], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14182665944099426\n",
      "logits: tensor([15807.3535], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12734119594097137\n",
      "logits: tensor([15787.5820], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13458822667598724\n",
      "logits: tensor([16098.0518], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11756951361894608\n",
      "logits: tensor([10399.0674], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6415097117424011\n",
      "logits: tensor([14726.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19277988374233246\n",
      "logits: tensor([15794.7646], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1341944932937622\n",
      "logits: tensor([12499.3809], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3099607229232788\n",
      "logits: tensor([17200.3594], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05043908581137657\n",
      "logits: tensor([17034.2227], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06625239551067352\n",
      "logits: tensor([13664.1689], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1569112539291382\n",
      "logits: tensor([19841.3633], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09535984694957733\n",
      "logits: tensor([10305.0049], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6266617774963379\n",
      "logits: tensor([19238.7148], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06209011375904083\n",
      "logits: tensor([18340.3301], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.012493995949625969\n",
      "logits: tensor([9818.8887], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5499275326728821\n",
      "logits: tensor([15678.0801], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13447785377502441\n",
      "logits: tensor([12582.1094], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9861064553260803\n",
      "logits: tensor([13036.3486], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28539976477622986\n",
      "logits: tensor([11744.9619], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.35160908102989197\n",
      "logits: tensor([10640.8994], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.679683268070221\n",
      "logits: tensor([10998.1689], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7360788583755493\n",
      "logits: tensor([10151.3486], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.602406919002533\n",
      "logits: tensor([17448.0137], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.043570030480623245\n",
      "logits: tensor([16529.8906], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09389784932136536\n",
      "logits: tensor([9278.5068], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4646274149417877\n",
      "logits: tensor([16062.4541], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11952083557844162\n",
      "logits: tensor([8669.9922], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3685723841190338\n",
      "logits: tensor([10631.0303], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6781254410743713\n",
      "logits: tensor([16193.8555], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1060040146112442\n",
      "logits: tensor([15157.8076], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16319994628429413\n",
      "logits: tensor([8806.6846], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.39014947414398193\n",
      "logits: tensor([6780.2939], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07028043270111084\n",
      "logits: tensor([17203.3809], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05027228221297264\n",
      "logits: tensor([12662.8701], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30093514919281006\n",
      "logits: tensor([10599.9629], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6732214093208313\n",
      "Training, Epoch: 1, Batch 850: Loss = 18189372.0\n",
      "logits: tensor([15469.9424], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15199995040893555\n",
      "logits: tensor([16319.0010], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10545796900987625\n",
      "logits: tensor([15151.7363], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16353511810302734\n",
      "logits: tensor([13284.9590], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.26659220457077026\n",
      "logits: tensor([10802.2246], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7051487565040588\n",
      "logits: tensor([23353.9531], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2892754375934601\n",
      "logits: tensor([8346.7109], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3175418972969055\n",
      "logits: tensor([15374.9854], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15720510482788086\n",
      "logits: tensor([13408.2031], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1165066957473755\n",
      "logits: tensor([12532.3770], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.31302547454833984\n",
      "logits: tensor([21771.4805], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20191365480422974\n",
      "logits: tensor([18146.7441], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.00526843685656786\n",
      "logits: tensor([17203.9336], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05024176836013794\n",
      "logits: tensor([16025.7227], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1215343102812767\n",
      "logits: tensor([12482.8496], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3108733594417572\n",
      "logits: tensor([10415.7715], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6441465020179749\n",
      "logits: tensor([17398.3926], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04629006236791611\n",
      "logits: tensor([15934.7148], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12031010538339615\n",
      "logits: tensor([9901.6152], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5629860758781433\n",
      "logits: tensor([10371.8682], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6372162699699402\n",
      "logits: tensor([10506.0459], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6583964228630066\n",
      "logits: tensor([8199.4492], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29429641366004944\n",
      "logits: tensor([16554.3047], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0925595685839653\n",
      "logits: tensor([8827.6846], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3934643864631653\n",
      "logits: tensor([18270.6719], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0015247835544869304\n",
      "logits: tensor([15516.2754], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14341041445732117\n",
      "logits: tensor([17812.1836], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01666279323399067\n",
      "logits: tensor([17217.1523], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04951201379299164\n",
      "logits: tensor([17194.6543], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.050754040479660034\n",
      "logits: tensor([18173.4434], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0038048927672207355\n",
      "logits: tensor([18086.1953], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0015357369557023048\n",
      "logits: tensor([17498.9512], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03395506367087364\n",
      "logits: tensor([14487.2812], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20586548745632172\n",
      "logits: tensor([10117.4434], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5970548987388611\n",
      "logits: tensor([9881.6885], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5598406195640564\n",
      "logits: tensor([12309.7666], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9431166648864746\n",
      "logits: tensor([14670.4043], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1958274096250534\n",
      "logits: tensor([17012.9492], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06741851568222046\n",
      "logits: tensor([15509.8291], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14376629889011383\n",
      "logits: tensor([17600.5332], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.028347140178084373\n",
      "logits: tensor([20467.2363], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12193161994218826\n",
      "logits: tensor([19728.2891], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08911748975515366\n",
      "logits: tensor([7378.9102], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16477297246456146\n",
      "logits: tensor([17848.7637], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.021602528169751167\n",
      "logits: tensor([18218.1035], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.001356802531518042\n",
      "logits: tensor([13174.8916], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0796780586242676\n",
      "logits: tensor([10700.5195], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6890944242477417\n",
      "logits: tensor([13382.7705], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2611924111843109\n",
      "logits: tensor([15087.1660], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17298221588134766\n",
      "logits: tensor([16830.8203], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07740209251642227\n",
      "Training, Epoch: 1, Batch 900: Loss = 1993843.25\n",
      "logits: tensor([13810.4883], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24296455085277557\n",
      "logits: tensor([14110.4854], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22101828455924988\n",
      "logits: tensor([10396.8154], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6411542296409607\n",
      "logits: tensor([17232.4160], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.048669371753931046\n",
      "logits: tensor([11602.5195], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3639965355396271\n",
      "logits: tensor([12559.4209], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9825250506401062\n",
      "logits: tensor([10097.0830], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5938409566879272\n",
      "logits: tensor([16823.8750], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07778280228376389\n",
      "logits: tensor([9606.3359], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5163757801055908\n",
      "logits: tensor([6771.5884], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06890624016523361\n",
      "logits: tensor([17586.5840], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.029117217287421227\n",
      "logits: tensor([11061.1055], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7460135221481323\n",
      "logits: tensor([10899.9707], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.720578134059906\n",
      "logits: tensor([16391.8418], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09507400542497635\n",
      "logits: tensor([17735.1484], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02783045917749405\n",
      "logits: tensor([12846.3203], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2958163619041443\n",
      "logits: tensor([15445.2520], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1473313271999359\n",
      "logits: tensor([10660.9404], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6828467845916748\n",
      "logits: tensor([10605.1006], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6740323901176453\n",
      "logits: tensor([12074.5205], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9059827327728271\n",
      "logits: tensor([12729.2783], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30223214626312256\n",
      "logits: tensor([14328.7109], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21455766260623932\n",
      "logits: tensor([12407.2871], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9585104584693909\n",
      "logits: tensor([17347.8809], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.049058910459280014\n",
      "logits: tensor([10002.5820], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5789238214492798\n",
      "logits: tensor([15865.9873], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13029035925865173\n",
      "logits: tensor([14940.9922], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18099486827850342\n",
      "logits: tensor([14879.2012], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18438200652599335\n",
      "logits: tensor([16904.1074], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07338478416204453\n",
      "logits: tensor([11453.5332], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8079588413238525\n",
      "logits: tensor([15138.0137], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17019493877887726\n",
      "logits: tensor([17981.8633], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.007295478135347366\n",
      "logits: tensor([18265.6777], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.001251024892553687\n",
      "logits: tensor([10937.5762], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7265142202377319\n",
      "logits: tensor([19056.3770], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0520239919424057\n",
      "logits: tensor([11958.1377], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3445029556751251\n",
      "logits: tensor([19445.8301], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06594223529100418\n",
      "logits: tensor([15504.9492], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15008100867271423\n",
      "logits: tensor([20811.7168], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14892905950546265\n",
      "logits: tensor([19224.4004], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.061299871653318405\n",
      "logits: tensor([12197.4756], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9253913760185242\n",
      "logits: tensor([16024.6074], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12159544229507446\n",
      "logits: tensor([18074.7324], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0021685557439923286\n",
      "logits: tensor([14257.8721], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21844077110290527\n",
      "logits: tensor([13474.3770], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1269522905349731\n",
      "logits: tensor([16608.2422], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08960293233394623\n",
      "logits: tensor([8518.3066], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.34462857246398926\n",
      "logits: tensor([10292.8486], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4357874095439911\n",
      "logits: tensor([8516.7695], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.34438592195510864\n",
      "logits: tensor([17419.0254], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.038367435336112976\n",
      "Training, Epoch: 1, Batch 950: Loss = 483008.71875\n",
      "logits: tensor([15906.1348], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12808963656425476\n",
      "logits: tensor([19560.6523], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07986295223236084\n",
      "logits: tensor([19485.7969], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06813304871320724\n",
      "logits: tensor([8680.2578], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.37019282579421997\n",
      "logits: tensor([17090.1445], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05652359500527382\n",
      "logits: tensor([11168.3457], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.38779619336128235\n",
      "logits: tensor([17528.5332], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03915627673268318\n",
      "logits: tensor([17050.2402], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05872654542326927\n",
      "logits: tensor([9720.0654], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5343281626701355\n",
      "logits: tensor([20140.3379], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10401235520839691\n",
      "logits: tensor([14683.4746], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19511094689369202\n",
      "logits: tensor([15849.2188], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12502998113632202\n",
      "logits: tensor([11429.4922], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.804163932800293\n",
      "logits: tensor([19036.0020], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05089917033910751\n",
      "logits: tensor([9796.3486], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5463695526123047\n",
      "logits: tensor([20095.7324], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10940252244472504\n",
      "logits: tensor([20699.6094], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14274007081985474\n",
      "logits: tensor([15339.6484], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1591421365737915\n",
      "logits: tensor([11554.4365], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8238866329193115\n",
      "logits: tensor([10233.3623], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6153528690338135\n",
      "logits: tensor([16544.7578], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09308288991451263\n",
      "logits: tensor([8334.9414], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.315684050321579\n",
      "logits: tensor([15199.3896], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16683055460453033\n",
      "logits: tensor([15735.7666], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13129320740699768\n",
      "logits: tensor([18006.6309], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.005928162485361099\n",
      "logits: tensor([10636.3730], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.41280969977378845\n",
      "logits: tensor([9330.9951], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4729127585887909\n",
      "logits: tensor([15450.1641], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14706015586853027\n",
      "logits: tensor([16791.5605], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07955415546894073\n",
      "logits: tensor([17752.0938], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.019980106502771378\n",
      "logits: tensor([19908.8438], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09132277965545654\n",
      "logits: tensor([9027.3164], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4249765872955322\n",
      "logits: tensor([19764.5078], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08341086655855179\n",
      "logits: tensor([14394.2861], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21096310019493103\n",
      "logits: tensor([18279.9219], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.009159107692539692\n",
      "logits: tensor([16220.7881], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10451717674732208\n",
      "logits: tensor([20676.1348], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1414441466331482\n",
      "logits: tensor([14487.9961], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2058262974023819\n",
      "logits: tensor([16042.5791], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11435536295175552\n",
      "logits: tensor([13912.5098], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1961122751235962\n",
      "logits: tensor([18556.5156], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.017193589359521866\n",
      "logits: tensor([19738.5859], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08198992908000946\n",
      "logits: tensor([21600.9414], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18407677114009857\n",
      "logits: tensor([18970.0312], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.047257199883461\n",
      "logits: tensor([22527.1523], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24363118410110474\n",
      "logits: tensor([16674.1875], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07948686927556992\n",
      "logits: tensor([17743.9316], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.020430702716112137\n",
      "logits: tensor([10759.2471], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6983646750450134\n",
      "logits: tensor([9277.1533], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.46441373229026794\n",
      "logits: tensor([14890.7490], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.177943155169487\n",
      "Training, Epoch: 1, Batch 1000: Loss = 10389435.0\n",
      "logits: tensor([13187.9336], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.27709048986434937\n",
      "logits: tensor([9177.4072], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4486686587333679\n",
      "logits: tensor([9550.6904], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5075920224189758\n",
      "logits: tensor([12297.9648], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.32587501406669617\n",
      "logits: tensor([10377.9570], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6381773948669434\n",
      "logits: tensor([11485.1475], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.36595237255096436\n",
      "logits: tensor([14321.6768], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21494325995445251\n",
      "logits: tensor([13288.1963], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.26641348004341125\n",
      "logits: tensor([10735.4707], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4115246534347534\n",
      "logits: tensor([15214.6104], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1659962236881256\n",
      "logits: tensor([20178.8281], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10612224042415619\n",
      "logits: tensor([19718.8457], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0885961577296257\n",
      "logits: tensor([22608.2559], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23929369449615479\n",
      "logits: tensor([16081.5840], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11847221106290817\n",
      "logits: tensor([13365.7559], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1098062992095947\n",
      "logits: tensor([14475.5566], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20650817453861237\n",
      "logits: tensor([19408.7422], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06390922516584396\n",
      "logits: tensor([13271.9023], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0949913263320923\n",
      "logits: tensor([22596.2539], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2386358082294464\n",
      "logits: tensor([18758.8086], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02828247658908367\n",
      "logits: tensor([12913.4971], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0384165048599243\n",
      "logits: tensor([16595.2754], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08384327590465546\n",
      "logits: tensor([14781.1328], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18975771963596344\n",
      "logits: tensor([16231.7598], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10391147434711456\n",
      "logits: tensor([14646.1074], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19144880771636963\n",
      "logits: tensor([12975.8857], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28871411085128784\n",
      "logits: tensor([14890.4521], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17795954644680023\n",
      "logits: tensor([15163.7422], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1628723293542862\n",
      "logits: tensor([19847.3535], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09569054841995239\n",
      "logits: tensor([18268.1113], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.008507096208631992\n",
      "logits: tensor([14120.3242], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22598059475421906\n",
      "logits: tensor([8338.6133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.31626367568969727\n",
      "logits: tensor([17114.6367], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.055171482264995575\n",
      "logits: tensor([18656.7773], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.029963744804263115\n",
      "logits: tensor([14158.4316], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.234931468963623\n",
      "logits: tensor([15815.4717], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.4964978694915771\n",
      "logits: tensor([15060.4463], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17444688081741333\n",
      "logits: tensor([18520.9629], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01524473074823618\n",
      "logits: tensor([16139.1113], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1090262159705162\n",
      "logits: tensor([18813.5312], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03861748054623604\n",
      "logits: tensor([17174.7578], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05185244232416153\n",
      "logits: tensor([17674.2012], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.031171342357993126\n",
      "logits: tensor([16903.0762], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07344131171703339\n",
      "logits: tensor([16723.1152], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07678576558828354\n",
      "logits: tensor([17339.9688], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.049492619931697845\n",
      "logits: tensor([19245.8516], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.054980214685201645\n",
      "logits: tensor([12635.1797], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9944836497306824\n",
      "logits: tensor([18324.9590], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.8926246166229248\n",
      "logits: tensor([10303.9170], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.62649005651474\n",
      "logits: tensor([14701.5771], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18838654458522797\n",
      "Training, Epoch: 1, Batch 1050: Loss = 11644723.0\n",
      "logits: tensor([13740.6777], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24143384397029877\n",
      "logits: tensor([15053.1172], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1689794659614563\n",
      "logits: tensor([15371.2861], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1514146775007248\n",
      "logits: tensor([15213.1553], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16607598960399628\n",
      "logits: tensor([8153.6284], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28706350922584534\n",
      "logits: tensor([12485.0967], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3107492923736572\n",
      "logits: tensor([8039.6021], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2690642774105072\n",
      "logits: tensor([9664.9463], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5256274938583374\n",
      "logits: tensor([17352.7891], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.042024072259664536\n",
      "logits: tensor([8791.9951], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3878307342529297\n",
      "logits: tensor([11636.9902], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.35756975412368774\n",
      "logits: tensor([12039.2695], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3400556445121765\n",
      "logits: tensor([8270.3418], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30548691749572754\n",
      "logits: tensor([18631.7129], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.021315600723028183\n",
      "logits: tensor([19176.7871], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.058671340346336365\n",
      "logits: tensor([8717.4326], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.37606093287467957\n",
      "logits: tensor([11924.4248], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8822898864746094\n",
      "logits: tensor([12487.5508], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9711802005767822\n",
      "logits: tensor([8866.7412], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.39962953329086304\n",
      "logits: tensor([13257.8662], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2732570767402649\n",
      "logits: tensor([14440.7891], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20278358459472656\n",
      "logits: tensor([20805.0254], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14855965971946716\n",
      "logits: tensor([19441.4414], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06570166349411011\n",
      "logits: tensor([18240.4629], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.00013115150795783848\n",
      "logits: tensor([9289.1084], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4663008749485016\n",
      "logits: tensor([8704.8398], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3740731179714203\n",
      "logits: tensor([9666.8252], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5259240865707397\n",
      "logits: tensor([9464.5117], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.493988573551178\n",
      "logits: tensor([9596.9482], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4739338755607605\n",
      "logits: tensor([8992.9385], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.41954997181892395\n",
      "logits: tensor([18258.5020], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0008576773689128458\n",
      "logits: tensor([16681.8906], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08556582033634186\n",
      "logits: tensor([9398.3848], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48481833934783936\n",
      "logits: tensor([10058.2402], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5877096056938171\n",
      "logits: tensor([18856.8184], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.041007183492183685\n",
      "logits: tensor([7382.5269], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16534388065338135\n",
      "logits: tensor([8966.5811], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.41538941860198975\n",
      "logits: tensor([11528.8037], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.36803731322288513\n",
      "logits: tensor([8392.8301], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3248218894004822\n",
      "logits: tensor([12663.7549], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3008863031864166\n",
      "logits: tensor([7971.2666], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25827741622924805\n",
      "logits: tensor([18518.7070], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.015121073462069035\n",
      "logits: tensor([18689.6367], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.024490751326084137\n",
      "logits: tensor([14401.1445], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20497219264507294\n",
      "logits: tensor([14620.2471], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1928764432668686\n",
      "logits: tensor([16838.8340], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07039741426706314\n",
      "logits: tensor([15643.3955], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14249195158481598\n",
      "logits: tensor([21276.4316], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1745840609073639\n",
      "logits: tensor([21270.9277], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16598674654960632\n",
      "logits: tensor([21249.2676], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17308443784713745\n",
      "Training, Epoch: 1, Batch 1100: Loss = 9829817.0\n",
      "logits: tensor([16950.3887], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06423893570899963\n",
      "logits: tensor([13142.6436], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0745877027511597\n",
      "logits: tensor([15596.1650], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1390000432729721\n",
      "logits: tensor([15143.1367], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1699141263961792\n",
      "logits: tensor([11396.0547], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7988857626914978\n",
      "logits: tensor([19294.6660], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.057656027376651764\n",
      "logits: tensor([11437.9131], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.805493175983429\n",
      "logits: tensor([7911.7754], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2488866150379181\n",
      "logits: tensor([12794.4922], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29366883635520935\n",
      "logits: tensor([15375.7480], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15716330707073212\n",
      "logits: tensor([10201.7949], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6103699207305908\n",
      "logits: tensor([8091.5503], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.27726438641548157\n",
      "logits: tensor([14258.7402], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21839317679405212\n",
      "logits: tensor([8383.0410], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.32327666878700256\n",
      "logits: tensor([15344.1367], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15889610350131989\n",
      "logits: tensor([15227.9814], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16526328027248383\n",
      "logits: tensor([8590.2832], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3559901714324951\n",
      "logits: tensor([16756.8340], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0749242976307869\n",
      "logits: tensor([17252.9258], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04753711074590683\n",
      "logits: tensor([15341.1943], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1590573936700821\n",
      "logits: tensor([16317.6533], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10553184151649475\n",
      "logits: tensor([19237.9883], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05454917997121811\n",
      "logits: tensor([21299.7188], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16756495833396912\n",
      "logits: tensor([19153.7461], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.057399339973926544\n",
      "logits: tensor([19592.4883], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07398144155740738\n",
      "logits: tensor([19511.4062], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07714428007602692\n",
      "logits: tensor([13160.0322], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2786199450492859\n",
      "logits: tensor([19024.5781], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04285089299082756\n",
      "logits: tensor([20779.5957], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1471557915210724\n",
      "logits: tensor([12770.9707], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.015918493270874\n",
      "logits: tensor([15978.6670], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12411370873451233\n",
      "logits: tensor([9812.9961], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5489974021911621\n",
      "logits: tensor([16010.8809], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1223478764295578\n",
      "logits: tensor([10844.9590], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7118943929672241\n",
      "logits: tensor([9368.6035], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.478849321603775\n",
      "logits: tensor([12599.1641], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3044520914554596\n",
      "logits: tensor([10369.0996], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.636779248714447\n",
      "logits: tensor([9092.7910], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.43531185388565063\n",
      "logits: tensor([13733.0518], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2472093105316162\n",
      "logits: tensor([10822.2637], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.708311915397644\n",
      "logits: tensor([9168.3281], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4472355246543884\n",
      "logits: tensor([12421.6709], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.31425076723098755\n",
      "logits: tensor([13434.1055], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2583584487438202\n",
      "logits: tensor([13688.8564], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24963192641735077\n",
      "logits: tensor([11942.3760], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.34071066975593567\n",
      "logits: tensor([15136.6904], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1702674776315689\n",
      "logits: tensor([16480.0957], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09662740677595139\n",
      "logits: tensor([18490.5957], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.013580123893916607\n",
      "logits: tensor([20169.2207], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10559559613466263\n",
      "logits: tensor([11371.9072], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7950740456581116\n",
      "Training, Epoch: 1, Batch 1150: Loss = 25369800.0\n",
      "logits: tensor([26347.2441], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4545226991176605\n",
      "logits: tensor([10325.2598], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6298590302467346\n",
      "logits: tensor([17098.7988], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05604582652449608\n",
      "logits: tensor([10989.1807], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7346600294113159\n",
      "logits: tensor([16161.0957], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1078125461935997\n",
      "logits: tensor([8639.0449], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.36368730664253235\n",
      "logits: tensor([15023.6445], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17060653865337372\n",
      "logits: tensor([9361.0088], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4776504635810852\n",
      "logits: tensor([13394.5869], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.265762597322464\n",
      "logits: tensor([9555.5166], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5083538293838501\n",
      "logits: tensor([13525.3115], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2533233165740967\n",
      "logits: tensor([16134.8506], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11555235087871552\n",
      "logits: tensor([15143.8896], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1639682948589325\n",
      "logits: tensor([14986.9307], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1784767061471939\n",
      "logits: tensor([16731.5254], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.082845039665699\n",
      "logits: tensor([8307.7197], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3113870620727539\n",
      "logits: tensor([18993.3652], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04854537546634674\n",
      "logits: tensor([16909.4590], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07309143245220184\n",
      "logits: tensor([11631.7832], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.836095929145813\n",
      "logits: tensor([13166.5020], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.078353762626648\n",
      "logits: tensor([16270.5000], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10811659693717957\n",
      "logits: tensor([19103.3496], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05461715906858444\n",
      "logits: tensor([15060.3389], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1744527667760849\n",
      "logits: tensor([22090.7969], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21954180300235748\n",
      "logits: tensor([21125.4961], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15801477432250977\n",
      "logits: tensor([19704.2832], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0801095962524414\n",
      "logits: tensor([19683.6270], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08665187656879425\n",
      "logits: tensor([11011.3086], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.39640432596206665\n",
      "logits: tensor([17764.5293], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.026219917461276054\n",
      "logits: tensor([9041.7539], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4272555708885193\n",
      "logits: tensor([17197.4863], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.050597693771123886\n",
      "logits: tensor([11743.2314], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8536881804466248\n",
      "logits: tensor([10067.4141], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5891577005386353\n",
      "logits: tensor([5538.3438], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1257634311914444\n",
      "logits: tensor([6474.9795], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.022086048498749733\n",
      "logits: tensor([14218.1455], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22061842679977417\n",
      "logits: tensor([14454.7607], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2076481282711029\n",
      "logits: tensor([13721.1846], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24785982072353363\n",
      "logits: tensor([12903.7061], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2876395881175995\n",
      "logits: tensor([10724.9355], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6929485201835632\n",
      "logits: tensor([14285.1934], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21137337386608124\n",
      "logits: tensor([6858.0098], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08254799991846085\n",
      "logits: tensor([17270.3203], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04657683148980141\n",
      "logits: tensor([8668.5293], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.36834144592285156\n",
      "logits: tensor([18591.4961], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.026359835639595985\n",
      "logits: tensor([15469.9893], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.151997372508049\n",
      "logits: tensor([20717.2148], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1356344372034073\n",
      "logits: tensor([11056.8379], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7453398704528809\n",
      "logits: tensor([19655.8340], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08511754125356674\n",
      "logits: tensor([10127.3848], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5986241698265076\n",
      "Training, Epoch: 1, Batch 1200: Loss = 14381704.0\n",
      "logits: tensor([14450.1289], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20790202915668488\n",
      "logits: tensor([14547.5225], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2025633007287979\n",
      "logits: tensor([16369.5703], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09630352258682251\n",
      "logits: tensor([10047.4668], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4453207850456238\n",
      "logits: tensor([19852.4551], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09597218036651611\n",
      "logits: tensor([13178.6553], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0802721977233887\n",
      "logits: tensor([16989.9453], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06205517798662186\n",
      "logits: tensor([10920.0674], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7237504124641418\n",
      "logits: tensor([11794.7900], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8618267774581909\n",
      "logits: tensor([25961.2148], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.43321162462234497\n",
      "logits: tensor([13150.6396], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2791348099708557\n",
      "logits: tensor([23180.5898], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2706667482852936\n",
      "logits: tensor([14364.0537], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20701982080936432\n",
      "logits: tensor([17321.3223], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05051474645733833\n",
      "logits: tensor([10334.4141], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6313040852546692\n",
      "logits: tensor([9881.3096], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5597807765007019\n",
      "logits: tensor([13536.1035], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2580052316188812\n",
      "logits: tensor([13475.1387], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25609317421913147\n",
      "logits: tensor([12754.9326], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3008258640766144\n",
      "logits: tensor([12120.9395], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3308529257774353\n",
      "logits: tensor([7690.9639], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2140311598777771\n",
      "logits: tensor([6971.4067], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1004478931427002\n",
      "logits: tensor([7795.3359], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23050646483898163\n",
      "logits: tensor([11603.8828], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.36392179131507874\n",
      "logits: tensor([11220.9922], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7712518572807312\n",
      "logits: tensor([7406.1450], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16907204687595367\n",
      "logits: tensor([17234.1289], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04857480898499489\n",
      "logits: tensor([21553.4219], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1898755431175232\n",
      "logits: tensor([15646.5361], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14231978356838226\n",
      "logits: tensor([13647.6113], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24657166004180908\n",
      "logits: tensor([9670.3945], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5264875292778015\n",
      "logits: tensor([21909.8008], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20954975485801697\n",
      "logits: tensor([7517.0698], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18658170104026794\n",
      "logits: tensor([10505.2354], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4241452217102051\n",
      "logits: tensor([16900.1777], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06701087951660156\n",
      "logits: tensor([17870.4023], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02041638270020485\n",
      "logits: tensor([14268.5986], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21228951215744019\n",
      "logits: tensor([18075.7031], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.009162619709968567\n",
      "logits: tensor([10699.9131], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6889986991882324\n",
      "logits: tensor([15418.4941], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1488085240125656\n",
      "logits: tensor([16010.3164], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11613645404577255\n",
      "logits: tensor([19976.0645], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10279614478349686\n",
      "logits: tensor([22308.4453], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23155727982521057\n",
      "logits: tensor([14678.1328], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19540376961231232\n",
      "logits: tensor([13191.6436], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.082322359085083\n",
      "logits: tensor([22655.7793], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24189874529838562\n",
      "logits: tensor([15120.2783], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1652717888355255\n",
      "logits: tensor([12776.5938], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2996384799480438\n",
      "logits: tensor([14452.6504], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20212876796722412\n",
      "logits: tensor([16504.1621], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08887326717376709\n",
      "Training, Epoch: 1, Batch 1250: Loss = 2591622.0\n",
      "logits: tensor([16421.8047], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0934198796749115\n",
      "logits: tensor([17096.0840], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0561956986784935\n",
      "logits: tensor([10970.8262], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7317627668380737\n",
      "logits: tensor([10969.1318], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7314953207969666\n",
      "logits: tensor([13883.7021], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23353806138038635\n",
      "logits: tensor([11974.2080], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8901482224464417\n",
      "logits: tensor([17988.3926], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.00693502277135849\n",
      "logits: tensor([15971.4512], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12450925260782242\n",
      "logits: tensor([19647.8574], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07701656222343445\n",
      "logits: tensor([14732.2354], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19243808090686798\n",
      "logits: tensor([16751.2324], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07523353397846222\n",
      "logits: tensor([10506.0781], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6584015488624573\n",
      "logits: tensor([17793.4961], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01769445464015007\n",
      "logits: tensor([18333.2129], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.012101084925234318\n",
      "logits: tensor([18325.0371], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.011649733409285545\n",
      "logits: tensor([18680.9668], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.024015501141548157\n",
      "logits: tensor([16720.1797], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07694782316684723\n",
      "logits: tensor([18611.3730], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.027457160875201225\n",
      "logits: tensor([19404.8164], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07125989347696304\n",
      "logits: tensor([14122.8486], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22033576667308807\n",
      "logits: tensor([16365.6338], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10290174931287766\n",
      "logits: tensor([10468.9580], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6525420546531677\n",
      "logits: tensor([18945.3809], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04589635506272316\n",
      "logits: tensor([9928.2607], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5671921372413635\n",
      "logits: tensor([10822.2998], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7083176374435425\n",
      "logits: tensor([15843.3643], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12535318732261658\n",
      "logits: tensor([11710.8828], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3580564856529236\n",
      "logits: tensor([17667.8633], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.024630123749375343\n",
      "logits: tensor([10261.0850], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6197289824485779\n",
      "logits: tensor([17205.8125], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.050138041377067566\n",
      "logits: tensor([12148.0029], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9175820350646973\n",
      "logits: tensor([16156.2090], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1143815740942955\n",
      "logits: tensor([16963.0410], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07015428692102432\n",
      "logits: tensor([9425.4629], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4878246486186981\n",
      "logits: tensor([13043.8486], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0589927434921265\n",
      "logits: tensor([18154.7246], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.004830979276448488\n",
      "logits: tensor([17567.8359], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03700185567140579\n",
      "logits: tensor([11518.6309], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3685949742794037\n",
      "logits: tensor([10455.2695], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.650381326675415\n",
      "logits: tensor([15117.2871], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17133109271526337\n",
      "logits: tensor([10439.0400], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.647819459438324\n",
      "logits: tensor([11321.2939], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.787084698677063\n",
      "logits: tensor([14139.1328], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22494958341121674\n",
      "logits: tensor([15316.0898], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1544618308544159\n",
      "logits: tensor([15230.3916], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15919288992881775\n",
      "logits: tensor([15677.2178], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14063794910907745\n",
      "logits: tensor([19293.4902], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.057591576129198074\n",
      "logits: tensor([12599.0684], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9887834191322327\n",
      "logits: tensor([11662.2998], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8409129977226257\n",
      "logits: tensor([18828.6895], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03211306408047676\n",
      "Training, Epoch: 1, Batch 1300: Loss = 343201.46875\n",
      "logits: tensor([14489.5186], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20574283599853516\n",
      "logits: tensor([17650.2637], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02560172602534294\n",
      "logits: tensor([14229.1143], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2144692838191986\n",
      "logits: tensor([14321.1270], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21497339010238647\n",
      "logits: tensor([1152.7723], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.81803297996521\n",
      "logits: tensor([15703.1533], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13921625912189484\n",
      "logits: tensor([19958.3945], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0940389558672905\n",
      "logits: tensor([15023.4854], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.3714815378189087\n",
      "logits: tensor([19989.6777], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09575377404689789\n",
      "logits: tensor([20041.8594], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10642841458320618\n",
      "logits: tensor([15722.8555], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1320059895515442\n",
      "logits: tensor([18136.2578], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.005843255203217268\n",
      "logits: tensor([15379.3584], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15096904337406158\n",
      "logits: tensor([17395.6016], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04644305258989334\n",
      "logits: tensor([15448.9551], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15315039455890656\n",
      "logits: tensor([16893.7617], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06736508011817932\n",
      "logits: tensor([15906.5391], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1280674785375595\n",
      "logits: tensor([16970.9844], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06310193240642548\n",
      "logits: tensor([12128.3457], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9144790768623352\n",
      "logits: tensor([18542.2363], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01641085557639599\n",
      "logits: tensor([19693.2930], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08718549460172653\n",
      "logits: tensor([11769.3018], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8578034043312073\n",
      "logits: tensor([9564.3818], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5097532272338867\n",
      "logits: tensor([13919.2070], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23700502514839172\n",
      "logits: tensor([9819.4443], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5500152707099915\n",
      "logits: tensor([17472.9570], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03539009392261505\n",
      "logits: tensor([15716.8535], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13233733177185059\n",
      "logits: tensor([11312.0059], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7856185436248779\n",
      "logits: tensor([9533.6865], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5049079060554504\n",
      "logits: tensor([13856.2500], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24045607447624207\n",
      "logits: tensor([15075.6885], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16773340106010437\n",
      "logits: tensor([16418.7910], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09998787939548492\n",
      "logits: tensor([13444.1172], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.26304754614830017\n",
      "logits: tensor([12341.1787], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3235062062740326\n",
      "logits: tensor([11683.8516], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8443149924278259\n",
      "logits: tensor([17817.5977], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02331092394888401\n",
      "logits: tensor([20804.0410], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14039389789104462\n",
      "logits: tensor([13056.0137], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0609129667282104\n",
      "logits: tensor([20602.3965], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13737335801124573\n",
      "logits: tensor([17109.5605], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06212267279624939\n",
      "logits: tensor([15822.1943], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1326909065246582\n",
      "logits: tensor([10597.5938], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6728473901748657\n",
      "logits: tensor([18207.5801], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0019336551195010543\n",
      "logits: tensor([11335.5684], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7893379330635071\n",
      "logits: tensor([16357.7891], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10333175957202911\n",
      "logits: tensor([15890.8057], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12892991304397583\n",
      "logits: tensor([10845.2188], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7119354009628296\n",
      "logits: tensor([14975.3223], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17911303043365479\n",
      "logits: tensor([12892.4512], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0350943803787231\n",
      "logits: tensor([16682.5156], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0855315551161766\n",
      "Training, Epoch: 1, Batch 1350: Loss = 2434660.5\n",
      "logits: tensor([9098.5527], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4362213611602783\n",
      "logits: tensor([17070.3027], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0642746239900589\n",
      "logits: tensor([13944.4121], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23018650710582733\n",
      "logits: tensor([6411.8306], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.012117886915802956\n",
      "logits: tensor([15351.8604], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15248709917068481\n",
      "logits: tensor([10509.5596], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6589511036872864\n",
      "logits: tensor([13093.8096], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28224998712539673\n",
      "logits: tensor([15637.2412], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1428292989730835\n",
      "logits: tensor([14076.4580], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22289679944515228\n",
      "logits: tensor([19587.3027], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07369719445705414\n",
      "logits: tensor([20087.2266], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10110101103782654\n",
      "logits: tensor([13595.9199], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.146138072013855\n",
      "logits: tensor([16829.2559], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07092618197202682\n",
      "logits: tensor([17991.9395], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.006739214062690735\n",
      "logits: tensor([11784.0059], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8601244688034058\n",
      "logits: tensor([19582.7324], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07344666868448257\n",
      "logits: tensor([17273.7500], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05312246456742287\n",
      "logits: tensor([20060.9062], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10747990757226944\n",
      "logits: tensor([10823.8711], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7085656523704529\n",
      "logits: tensor([18323.9395], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01158913690596819\n",
      "logits: tensor([8573.9717], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3534153699874878\n",
      "logits: tensor([13521.3799], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25354036688804626\n",
      "logits: tensor([15919.2217], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12737226486206055\n",
      "logits: tensor([6672.0664], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.053196538239717484\n",
      "logits: tensor([8182.5835], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29163411259651184\n",
      "logits: tensor([11292.2070], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3766038119792938\n",
      "logits: tensor([13275.8936], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.26709267497062683\n",
      "logits: tensor([15380.7178], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15089400112628937\n",
      "logits: tensor([15899.8008], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1284368336200714\n",
      "logits: tensor([15840.3652], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12551875412464142\n",
      "logits: tensor([10993.5449], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7353489398956299\n",
      "logits: tensor([15997.5850], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12307669967412949\n",
      "logits: tensor([19130.3770], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0561092272400856\n",
      "logits: tensor([10851.6123], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7129446864128113\n",
      "logits: tensor([13803.8086], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1789536476135254\n",
      "logits: tensor([20503.5898], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1239243671298027\n",
      "logits: tensor([22448.6133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23054273426532745\n",
      "logits: tensor([19405.1406], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07127779722213745\n",
      "logits: tensor([15509.2939], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14379583299160004\n",
      "logits: tensor([17720.5332], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02172243408858776\n",
      "logits: tensor([13468.4014], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.261716365814209\n",
      "logits: tensor([14300.8047], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21608737111091614\n",
      "logits: tensor([9603.3848], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5159099102020264\n",
      "logits: tensor([10684.1641], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6865127086639404\n",
      "logits: tensor([15197.0225], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16696031391620636\n",
      "logits: tensor([16275.0312], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10152263939380646\n",
      "logits: tensor([15326.6592], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1538783460855484\n",
      "logits: tensor([16491.8320], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08955395966768265\n",
      "logits: tensor([15289.7930], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16187500953674316\n",
      "logits: tensor([16111.0029], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11057796329259872\n",
      "Training, Epoch: 1, Batch 1400: Loss = 4012052.0\n",
      "logits: tensor([12229.7031], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9304785132408142\n",
      "logits: tensor([11800.2373], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8626866340637207\n",
      "logits: tensor([9704.7090], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5319041013717651\n",
      "logits: tensor([15828.4346], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13234885036945343\n",
      "logits: tensor([16198.1123], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11208459734916687\n",
      "logits: tensor([13761.8809], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24562901258468628\n",
      "logits: tensor([16025.3467], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1215549185872078\n",
      "logits: tensor([17738.9473], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.027622222900390625\n",
      "logits: tensor([19292.5137], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06506012380123138\n",
      "logits: tensor([18627.9160], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.021107470616698265\n",
      "logits: tensor([11610.0938], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8326721787452698\n",
      "logits: tensor([13365.2539], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1097270250320435\n",
      "logits: tensor([17753.7793], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.026809189468622208\n",
      "logits: tensor([10363.5488], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6359030604362488\n",
      "logits: tensor([15813.1738], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12701988220214844\n",
      "logits: tensor([13287.8867], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0975145101547241\n",
      "logits: tensor([10108.1045], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5955807566642761\n",
      "logits: tensor([15385.5078], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15062955021858215\n",
      "logits: tensor([14054.5068], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22410863637924194\n",
      "logits: tensor([9425.1982], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48778286576271057\n",
      "logits: tensor([11999.1113], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.34225696325302124\n",
      "logits: tensor([13613.5850], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2484501153230667\n",
      "logits: tensor([14671.7334], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19003410637378693\n",
      "logits: tensor([13332.7656], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.26395300030708313\n",
      "logits: tensor([14921.6846], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17623533308506012\n",
      "logits: tensor([11560.2607], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8248059749603271\n",
      "logits: tensor([10514.6631], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6597566604614258\n",
      "logits: tensor([10913.0811], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7226476073265076\n",
      "logits: tensor([13567.8818], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1417121887207031\n",
      "logits: tensor([10611.5459], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6750497817993164\n",
      "logits: tensor([17306.0723], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05135069042444229\n",
      "logits: tensor([19379.0762], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06983888149261475\n",
      "logits: tensor([19962.3105], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10203684866428375\n",
      "logits: tensor([11162.8320], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7620711922645569\n",
      "logits: tensor([12318.1602], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.32476797699928284\n",
      "logits: tensor([31373.1113], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7197478413581848\n",
      "logits: tensor([13962.7051], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23462063074111938\n",
      "logits: tensor([12121.7744], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.33080682158470154\n",
      "logits: tensor([8074.2988], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2745411992073059\n",
      "logits: tensor([16882.3203], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06799671053886414\n",
      "logits: tensor([8733.5361], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.37860289216041565\n",
      "logits: tensor([11606.7900], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.36376243829727173\n",
      "logits: tensor([12163.0576], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3332700729370117\n",
      "logits: tensor([15949.7373], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11948077380657196\n",
      "logits: tensor([15292.1982], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1617431640625\n",
      "logits: tensor([20282.3066], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11970251053571701\n",
      "logits: tensor([17942.0488], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01648901030421257\n",
      "logits: tensor([11704.7324], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8476110696792603\n",
      "logits: tensor([20773.7480], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13873335719108582\n",
      "logits: tensor([25051.6309], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3829972445964813\n",
      "Training, Epoch: 1, Batch 1450: Loss = 48130532.0\n",
      "logits: tensor([24847.3926], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3620341718196869\n",
      "logits: tensor([13738.9375], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1687136888504028\n",
      "logits: tensor([12741.2715], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30157470703125\n",
      "logits: tensor([19345.2168], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.060427017509937286\n",
      "logits: tensor([13217.0127], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2754964828491211\n",
      "logits: tensor([15326.7432], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15387371182441711\n",
      "logits: tensor([10476.8535], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4257010221481323\n",
      "logits: tensor([12633.1455], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30257612466812134\n",
      "logits: tensor([8751.6533], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3814627230167389\n",
      "logits: tensor([9149.2119], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4442179799079895\n",
      "logits: tensor([10967.7100], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7312708497047424\n",
      "logits: tensor([15066.9043], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16821834444999695\n",
      "logits: tensor([9045.5361], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.427852600812912\n",
      "logits: tensor([11490.8291], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8138460516929626\n",
      "logits: tensor([13056.5977], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28428980708122253\n",
      "logits: tensor([14668.7441], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1901991218328476\n",
      "logits: tensor([10836.8604], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7106160521507263\n",
      "logits: tensor([15956.1260], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12534931302070618\n",
      "logits: tensor([16764.4434], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07450421154499054\n",
      "logits: tensor([10276.9609], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6222350001335144\n",
      "logits: tensor([8903.5381], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.40543797612190247\n",
      "logits: tensor([15877.9590], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1234433576464653\n",
      "logits: tensor([15055.4375], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16885137557983398\n",
      "logits: tensor([11786.3623], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.860496461391449\n",
      "logits: tensor([20517.2246], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13267137110233307\n",
      "logits: tensor([18137.3008], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.00578608363866806\n",
      "logits: tensor([9333.6631], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4733338952064514\n",
      "logits: tensor([15141.5732], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16999982297420502\n",
      "logits: tensor([13930.2139], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2309703379869461\n",
      "logits: tensor([13278.1064], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2669704854488373\n",
      "logits: tensor([20224.7832], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11652687937021255\n",
      "logits: tensor([18595.7988], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01934693567454815\n",
      "logits: tensor([10204.5137], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6107990741729736\n",
      "logits: tensor([11963.5371], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8884637951850891\n",
      "logits: tensor([11297.7051], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7833611369132996\n",
      "logits: tensor([14866.4668], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17928367853164673\n",
      "logits: tensor([15558.9561], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1410541981458664\n",
      "logits: tensor([14703.8584], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18826061487197876\n",
      "logits: tensor([11175.1143], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.764009952545166\n",
      "logits: tensor([18487.1270], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.020598046481609344\n",
      "logits: tensor([8272.1689], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30577531456947327\n",
      "logits: tensor([11293.3867], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7826794385910034\n",
      "logits: tensor([14804.1260], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18849731981754303\n",
      "logits: tensor([15646.7676], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1423071026802063\n",
      "logits: tensor([12318.8291], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9445472359657288\n",
      "logits: tensor([10259.6416], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6195011138916016\n",
      "logits: tensor([10335.8438], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6315297484397888\n",
      "logits: tensor([14059.8506], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2292955070734024\n",
      "logits: tensor([7544.1304], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19085325300693512\n",
      "logits: tensor([13518.9912], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25894325971603394\n",
      "Training, Epoch: 1, Batch 1500: Loss = 22314894.0\n",
      "logits: tensor([12202.5029], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3263501226902008\n",
      "logits: tensor([9647.2432], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5228330492973328\n",
      "logits: tensor([9330.4307], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.488543301820755\n",
      "logits: tensor([13261.6387], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2730502784252167\n",
      "logits: tensor([13790.7529], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24404636025428772\n",
      "logits: tensor([11993.7969], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8932403326034546\n",
      "logits: tensor([15086.5869], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17301395535469055\n",
      "logits: tensor([13543.8711], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1379220485687256\n",
      "logits: tensor([14531.0420], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.2937484979629517\n",
      "logits: tensor([10459.6055], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6510657668113708\n",
      "logits: tensor([19271.6133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0563923679292202\n",
      "logits: tensor([11053.0566], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7447429895401001\n",
      "logits: tensor([19797.8301], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08523745834827423\n",
      "logits: tensor([17523.3027], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03261071443557739\n",
      "logits: tensor([16327.4443], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09862912446260452\n",
      "logits: tensor([12944.7344], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28537458181381226\n",
      "logits: tensor([13877.0674], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2393149584531784\n",
      "logits: tensor([6646.7998], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04920816421508789\n",
      "logits: tensor([10396.7295], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6411406397819519\n",
      "logits: tensor([16290.3975], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10702589899301529\n",
      "logits: tensor([13786.9375], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24425551295280457\n",
      "logits: tensor([13788.4482], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2387966364622116\n",
      "logits: tensor([20657.0781], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14039209485054016\n",
      "logits: tensor([10144.9209], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6013922691345215\n",
      "logits: tensor([18832.1816], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.039647091180086136\n",
      "logits: tensor([18546.5664], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.023879453539848328\n",
      "logits: tensor([13579.7979], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25561007857322693\n",
      "logits: tensor([15356.0850], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1522538661956787\n",
      "logits: tensor([22457.0039], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23100267350673676\n",
      "logits: tensor([12488.2930], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9712973833084106\n",
      "logits: tensor([17137.2031], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0606074184179306\n",
      "logits: tensor([18608.9023], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02732076309621334\n",
      "logits: tensor([15240.5020], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1645769476890564\n",
      "logits: tensor([9946.8242], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5701223611831665\n",
      "logits: tensor([19780.2793], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08427539467811584\n",
      "logits: tensor([17263.4609], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0536864697933197\n",
      "logits: tensor([18085.6426], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.00156625104136765\n",
      "logits: tensor([15357.0166], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15819008648395538\n",
      "logits: tensor([10771.3369], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7002730369567871\n",
      "logits: tensor([18580.0527], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.018483798950910568\n",
      "logits: tensor([15857.5078], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12457238137722015\n",
      "logits: tensor([10748.5566], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6966771483421326\n",
      "logits: tensor([9360.9570], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.477642297744751\n",
      "logits: tensor([14768.1816], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1847095936536789\n",
      "logits: tensor([14356.5117], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2074361890554428\n",
      "logits: tensor([9102.4414], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4368351995944977\n",
      "logits: tensor([9345.9404], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47527191042900085\n",
      "logits: tensor([13158.9805], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2786775827407837\n",
      "logits: tensor([17783.0508], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.025204645469784737\n",
      "logits: tensor([16630.3516], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08190687000751495\n",
      "Training, Epoch: 1, Batch 1550: Loss = 2201253.25\n",
      "logits: tensor([16724.5996], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08322468400001526\n",
      "logits: tensor([18027.6797], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.011795070953667164\n",
      "logits: tensor([9420.9814], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4871172606945038\n",
      "logits: tensor([12533.8203], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9784839153289795\n",
      "logits: tensor([12184.7285], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9233792424201965\n",
      "logits: tensor([16016.3604], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11580278724431992\n",
      "logits: tensor([8905.0391], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.40567490458488464\n",
      "logits: tensor([17344.6094], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.049238238483667374\n",
      "logits: tensor([15907.5293], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1218109056353569\n",
      "logits: tensor([15637.2754], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13673050701618195\n",
      "logits: tensor([16146.8535], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11489439755678177\n",
      "logits: tensor([11871.9951], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8740137219429016\n",
      "logits: tensor([7976.5361], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2591091990470886\n",
      "logits: tensor([9152.1680], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.44468459486961365\n",
      "logits: tensor([13705.9219], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2486964613199234\n",
      "logits: tensor([14231.1475], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21990570425987244\n",
      "logits: tensor([8160.9805], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2882240414619446\n",
      "logits: tensor([10150.5684], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6022837162017822\n",
      "logits: tensor([12592.6113], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9877641797065735\n",
      "logits: tensor([17722.3809], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02853032574057579\n",
      "logits: tensor([15375.5371], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15117999911308289\n",
      "logits: tensor([11073.0615], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7479007840156555\n",
      "logits: tensor([9868.1084], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5576969385147095\n",
      "logits: tensor([13365.3457], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1097415685653687\n",
      "logits: tensor([12880.2539], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2939562499523163\n",
      "logits: tensor([8102.5508], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.27900081872940063\n",
      "logits: tensor([14422.8193], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20377561450004578\n",
      "logits: tensor([14308.5566], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21008358895778656\n",
      "logits: tensor([12290.0674], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.32630792260169983\n",
      "logits: tensor([7069.0264], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1158573105931282\n",
      "logits: tensor([16496.9395], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08927199989557266\n",
      "logits: tensor([11426.3398], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.803666353225708\n",
      "logits: tensor([13091.8750], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.27725157141685486\n",
      "logits: tensor([18383.5195], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.014878307469189167\n",
      "logits: tensor([15269.5127], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16298669576644897\n",
      "logits: tensor([14054.3555], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22959673404693604\n",
      "logits: tensor([11885.7051], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8761779069900513\n",
      "logits: tensor([15029.7734], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17026817798614502\n",
      "logits: tensor([11088.1699], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7502856850624084\n",
      "logits: tensor([21460.9883], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17640510201454163\n",
      "logits: tensor([22056.3066], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21763773262500763\n",
      "logits: tensor([17279.2754], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04608245939016342\n",
      "logits: tensor([17177.6992], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05169006064534187\n",
      "logits: tensor([20246.3320], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10982253402471542\n",
      "logits: tensor([13400.0088], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.11521315574646\n",
      "logits: tensor([11928.4219], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8829208016395569\n",
      "logits: tensor([17073.5078], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.057442039251327515\n",
      "logits: tensor([16537.7539], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08701880276203156\n",
      "logits: tensor([16140.2969], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11525380611419678\n",
      "logits: tensor([15100.9443], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16633912920951843\n",
      "Training, Epoch: 1, Batch 1600: Loss = 9078587.0\n",
      "logits: tensor([15262.4609], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16337324678897858\n",
      "logits: tensor([12423.2217], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3190089166164398\n",
      "logits: tensor([8427.9844], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.33037105202674866\n",
      "logits: tensor([18168.4531], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0030053777154535055\n",
      "logits: tensor([13273.2314], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.26723963022232056\n",
      "logits: tensor([12638.0645], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3072321116924286\n",
      "logits: tensor([7028.2686], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10942362993955612\n",
      "logits: tensor([12601.7148], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9892011880874634\n",
      "logits: tensor([12059.4043], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9035965800285339\n",
      "logits: tensor([18863.5859], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04138079285621643\n",
      "logits: tensor([18196.2500], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.004539928399026394\n",
      "logits: tensor([12284.7334], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9391651749610901\n",
      "logits: tensor([19012.2617], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04217575863003731\n",
      "logits: tensor([12352.6084], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9498793482780457\n",
      "logits: tensor([6944.9023], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09626413136720657\n",
      "logits: tensor([16183.7793], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11287027597427368\n",
      "logits: tensor([14070.0107], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22873857617378235\n",
      "logits: tensor([12372.2637], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9529819488525391\n",
      "logits: tensor([13028.8389], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2858114242553711\n",
      "logits: tensor([13087.4473], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2825987637042999\n",
      "logits: tensor([6892.4834], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08798971772193909\n",
      "logits: tensor([16194.5918], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10596336424350739\n",
      "logits: tensor([7913.0776], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24909217655658722\n",
      "logits: tensor([14766.0078], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18482959270477295\n",
      "logits: tensor([12372.5469], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9530266523361206\n",
      "logits: tensor([16521.9082], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0943354144692421\n",
      "logits: tensor([16688.5488], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08520083874464035\n",
      "logits: tensor([15559.8340], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1410057246685028\n",
      "logits: tensor([16901.3125], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06694822758436203\n",
      "logits: tensor([17043.1016], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06576568633317947\n",
      "logits: tensor([22736.8613], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2552083432674408\n",
      "logits: tensor([21495.3320], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17828768491744995\n",
      "logits: tensor([10744.8135], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6960862874984741\n",
      "logits: tensor([16321.4795], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10532210767269135\n",
      "logits: tensor([17403.8867], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04598889499902725\n",
      "logits: tensor([16890.4258], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07413475960493088\n",
      "logits: tensor([17167.4355], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05225667357444763\n",
      "logits: tensor([18147.3691], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.005234176758676767\n",
      "logits: tensor([11142.5381], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7588677406311035\n",
      "logits: tensor([16285.5654], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10729077458381653\n",
      "logits: tensor([8680.0557], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.37016090750694275\n",
      "logits: tensor([9187.8428], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.450315922498703\n",
      "logits: tensor([14698.9062], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19426505267620087\n",
      "logits: tensor([17762.4238], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.026335330680012703\n",
      "logits: tensor([16492.5371], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08951503783464432\n",
      "logits: tensor([3847.0688], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7876192331314087\n",
      "logits: tensor([13189.7959], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0820307731628418\n",
      "logits: tensor([9766.1211], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5415980815887451\n",
      "logits: tensor([14233.0879], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.246716022491455\n",
      "logits: tensor([19273.3984], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0564902238547802\n",
      "Training, Epoch: 1, Batch 1650: Loss = 1062018.75\n",
      "logits: tensor([16188.9619], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10627417266368866\n",
      "logits: tensor([10552.2627], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6656918525695801\n",
      "logits: tensor([10052.3076], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5867730975151062\n",
      "logits: tensor([8205.5908], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2952658534049988\n",
      "logits: tensor([8781.1982], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.38612642884254456\n",
      "logits: tensor([21033.6836], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1611829400062561\n",
      "logits: tensor([12565.3115], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3063209652900696\n",
      "logits: tensor([7516.3403], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1864665448665619\n",
      "logits: tensor([17967.8945], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.008066635578870773\n",
      "logits: tensor([10405.2471], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.42556920647621155\n",
      "logits: tensor([5831.2324], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07953047752380371\n",
      "logits: tensor([14129.2539], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21998216211795807\n",
      "logits: tensor([8259.4219], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30376318097114563\n",
      "logits: tensor([15327.6328], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15980078279972076\n",
      "logits: tensor([11850.6084], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.34577676653862\n",
      "logits: tensor([20497.3555], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13157446682453156\n",
      "logits: tensor([17642.6133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03290286287665367\n",
      "logits: tensor([24033.5391], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3267925977706909\n",
      "logits: tensor([9359.3916], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4773952066898346\n",
      "logits: tensor([7053.1045], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11334402114152908\n",
      "logits: tensor([6313.1538], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.003458399558439851\n",
      "logits: tensor([9483.0488], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.49691468477249146\n",
      "logits: tensor([5064.0337], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20063404738903046\n",
      "logits: tensor([13543.8984], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25757792592048645\n",
      "logits: tensor([16631.9492], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08830340206623077\n",
      "logits: tensor([12867.6582], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2946467101573944\n",
      "logits: tensor([16000.4844], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11667923629283905\n",
      "logits: tensor([12873.5352], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2943245470523834\n",
      "logits: tensor([18392.5508], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.015376885421574116\n",
      "logits: tensor([9830.5498], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5517683029174805\n",
      "logits: tensor([11123.7988], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7559097409248352\n",
      "logits: tensor([21315.6816], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16843998432159424\n",
      "logits: tensor([10662.8145], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6831426024436951\n",
      "logits: tensor([19388.7578], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07037336379289627\n",
      "logits: tensor([11520.6416], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3639928698539734\n",
      "logits: tensor([12898.9277], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28790339827537537\n",
      "logits: tensor([15412.0703], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15517225861549377\n",
      "logits: tensor([16739.5137], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0824071541428566\n",
      "logits: tensor([18276.8047], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.008987020701169968\n",
      "logits: tensor([20856.5508], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15140415728092194\n",
      "logits: tensor([14298.2881], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21622532606124878\n",
      "logits: tensor([11288.5850], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7819215059280396\n",
      "logits: tensor([8380.6250], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3228952884674072\n",
      "logits: tensor([12392.8486], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9562312960624695\n",
      "logits: tensor([11749.0322], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8546038269996643\n",
      "logits: tensor([11922.9385], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8820552229881287\n",
      "logits: tensor([19907.1133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09898963570594788\n",
      "logits: tensor([9410.6973], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48549386858940125\n",
      "logits: tensor([14284.9648], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21695564687252045\n",
      "logits: tensor([15349.0361], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15264301002025604\n",
      "Training, Epoch: 1, Batch 1700: Loss = 7645101.0\n",
      "logits: tensor([10947.7881], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3956177532672882\n",
      "logits: tensor([10269.7061], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4330518841743469\n",
      "logits: tensor([9277.6992], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.46449992060661316\n",
      "logits: tensor([8674.3311], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.36925727128982544\n",
      "logits: tensor([5638.1108], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11001502722501755\n",
      "logits: tensor([13620.6152], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2533726394176483\n",
      "logits: tensor([13680.0898], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24477864801883698\n",
      "logits: tensor([8354.1885], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3187222480773926\n",
      "logits: tensor([9630.8760], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47207409143447876\n",
      "logits: tensor([13831.8193], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23640228807926178\n",
      "logits: tensor([19867.5742], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08906055241823196\n",
      "logits: tensor([16762.1133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08116833120584488\n",
      "logits: tensor([25046.2109], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3729325830936432\n",
      "logits: tensor([16034.6865], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11479108035564423\n",
      "logits: tensor([24986.4180], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.37939709424972534\n",
      "logits: tensor([17585.7969], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.029160670936107635\n",
      "logits: tensor([18367.7227], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0068447170779109\n",
      "logits: tensor([10643.4248], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6800819039344788\n",
      "logits: tensor([10695.6104], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6883195042610168\n",
      "logits: tensor([15530.3193], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.148690328001976\n",
      "logits: tensor([14009.6328], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22658593952655792\n",
      "logits: tensor([10245.8164], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6173188090324402\n",
      "logits: tensor([13678.2568], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2502129375934601\n",
      "logits: tensor([7884.1406], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24452441930770874\n",
      "logits: tensor([15032.6357], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17597134411334991\n",
      "logits: tensor([14437.7949], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20294888317584991\n",
      "logits: tensor([13826.4814], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24208787083625793\n",
      "logits: tensor([13353.2812], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.26282039284706116\n",
      "logits: tensor([15992.1875], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11713727563619614\n",
      "logits: tensor([15763.9658], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12973645329475403\n",
      "logits: tensor([18680.4082], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02398488111793995\n",
      "logits: tensor([19313.7988], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05870480835437775\n",
      "logits: tensor([13484.4414], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1285409927368164\n",
      "logits: tensor([21881.1191], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20796635746955872\n",
      "logits: tensor([16319.2646], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10544351488351822\n",
      "logits: tensor([20412.3574], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12688207626342773\n",
      "logits: tensor([19939.3418], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0929945632815361\n",
      "logits: tensor([10140.2178], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.600649893283844\n",
      "logits: tensor([19553.0117], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07181750237941742\n",
      "logits: tensor([16322.2676], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10527890920639038\n",
      "logits: tensor([10605.5078], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6740966439247131\n",
      "logits: tensor([15330.2363], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15965807437896729\n",
      "logits: tensor([15813.3867], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13317370414733887\n",
      "logits: tensor([14554.4092], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19651108980178833\n",
      "logits: tensor([15394.0293], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15616120398044586\n",
      "logits: tensor([18349.4707], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01299861166626215\n",
      "logits: tensor([17237.9629], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05508417263627052\n",
      "logits: tensor([20715.6191], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14362390339374542\n",
      "logits: tensor([12093.2861], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9089449048042297\n",
      "logits: tensor([19016.7734], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0424230732023716\n",
      "Training, Epoch: 1, Batch 1750: Loss = 598949.0\n",
      "logits: tensor([19059.7832], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05221203714609146\n",
      "logits: tensor([12483.2314], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3157194256782532\n",
      "logits: tensor([13967.9443], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.2048627138137817\n",
      "logits: tensor([15914.9336], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1276073157787323\n",
      "logits: tensor([15538.1006], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14826378226280212\n",
      "logits: tensor([16624.7676], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08221513777971268\n",
      "logits: tensor([16828.2051], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07754544913768768\n",
      "logits: tensor([15424.1211], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14849787950515747\n",
      "logits: tensor([12781.6143], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29936328530311584\n",
      "logits: tensor([18031.3926], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.004561170004308224\n",
      "logits: tensor([18252.6270], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.007652267813682556\n",
      "logits: tensor([22451.6055], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2394605576992035\n",
      "logits: tensor([10326.3945], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6300382018089294\n",
      "logits: tensor([24640.0723], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3602767884731293\n",
      "logits: tensor([13265.4336], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0939702987670898\n",
      "logits: tensor([20893.5859], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14530238509178162\n",
      "logits: tensor([17495.3398], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03415443003177643\n",
      "logits: tensor([11423.4199], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8032054305076599\n",
      "logits: tensor([13717.3564], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2480696588754654\n",
      "logits: tensor([11705.9160], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3537646532058716\n",
      "logits: tensor([13664.6631], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.250958114862442\n",
      "logits: tensor([13637.9551], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2471047341823578\n",
      "logits: tensor([15255.7178], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16374288499355316\n",
      "logits: tensor([14804.7266], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18269209563732147\n",
      "logits: tensor([9307.7979], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.46925103664398193\n",
      "logits: tensor([17420.3359], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03829508647322655\n",
      "logits: tensor([10888.9395], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.718836784362793\n",
      "logits: tensor([9888.0713], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5608481168746948\n",
      "logits: tensor([13737.5762], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1684987545013428\n",
      "logits: tensor([11657.1523], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8401004672050476\n",
      "logits: tensor([16034.6172], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12104674428701401\n",
      "logits: tensor([12929.4932], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28621599078178406\n",
      "logits: tensor([15041.1201], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17550626397132874\n",
      "logits: tensor([7144.4507], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12776316702365875\n",
      "logits: tensor([12051.6846], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.33467617630958557\n",
      "logits: tensor([20926.3750], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1552588790655136\n",
      "logits: tensor([20657.2793], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13234901428222656\n",
      "logits: tensor([11203.5752], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7685025930404663\n",
      "logits: tensor([18482.9863], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.013163008727133274\n",
      "logits: tensor([17142.0742], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.053656768053770065\n",
      "logits: tensor([8228.7646], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29892387986183167\n",
      "logits: tensor([9605.0381], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5161709189414978\n",
      "logits: tensor([8680.8994], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.37029409408569336\n",
      "logits: tensor([12754.8447], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29585760831832886\n",
      "logits: tensor([15216.3652], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16590002179145813\n",
      "logits: tensor([13939.0439], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23048286139965057\n",
      "logits: tensor([8208.1455], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2956691086292267\n",
      "logits: tensor([8560.6338], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.35130998492240906\n",
      "logits: tensor([15874.5439], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12363188713788986\n",
      "logits: tensor([14714.8613], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18765318393707275\n",
      "Training, Epoch: 1, Batch 1800: Loss = 11554237.0\n",
      "logits: tensor([8457.8330], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3350827097892761\n",
      "logits: tensor([13543.9590], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2575746178627014\n",
      "logits: tensor([14659.5898], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19070449471473694\n",
      "logits: tensor([20428.8594], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.127793088555336\n",
      "logits: tensor([13074.3750], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0638114213943481\n",
      "logits: tensor([13007.3633], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0532335042953491\n",
      "logits: tensor([23183.3086], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2798548638820648\n",
      "logits: tensor([9362.0117], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47780877351760864\n",
      "logits: tensor([11099.7285], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7521101832389832\n",
      "logits: tensor([16481.1152], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09014558792114258\n",
      "logits: tensor([8899.7324], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4048372507095337\n",
      "logits: tensor([8854.0879], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3976321816444397\n",
      "logits: tensor([12759.6797], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29559069871902466\n",
      "logits: tensor([10424.7607], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.42855653166770935\n",
      "logits: tensor([4922.4336], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22298584878444672\n",
      "logits: tensor([6927.9014], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09358049929141998\n",
      "logits: tensor([8437.2637], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3318358063697815\n",
      "logits: tensor([11075.5625], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.388563871383667\n",
      "logits: tensor([8432.7324], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3311205208301544\n",
      "logits: tensor([13092.7266], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.282309353351593\n",
      "logits: tensor([13920.3770], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2315133959054947\n",
      "logits: tensor([16298.4863], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10658250004053116\n",
      "logits: tensor([8824.0059], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.39288368821144104\n",
      "logits: tensor([9686.1172], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5289693474769592\n",
      "logits: tensor([17530.9707], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03218739852309227\n",
      "logits: tensor([17590.4473], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.035762395709753036\n",
      "logits: tensor([15102.4531], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17214423418045044\n",
      "logits: tensor([21459.8691], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18471089005470276\n",
      "logits: tensor([22063.1191], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21801382303237915\n",
      "logits: tensor([21575.6797], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1826920211315155\n",
      "logits: tensor([18376.0078], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.014463616535067558\n",
      "logits: tensor([17808.1504], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01688545010983944\n",
      "logits: tensor([13954.4307], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2296334207057953\n",
      "logits: tensor([9332.7285], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4731863737106323\n",
      "logits: tensor([11323.5547], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7874415516853333\n",
      "logits: tensor([14190.3896], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21660710871219635\n",
      "logits: tensor([9553.6084], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5080526471138\n",
      "logits: tensor([17134.6680], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05406563729047775\n",
      "logits: tensor([17790.2559], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.024809690192341805\n",
      "logits: tensor([16886.9766], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06773965805768967\n",
      "logits: tensor([9717.8174], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5339732766151428\n",
      "logits: tensor([12187.2891], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.32719001173973083\n",
      "logits: tensor([14093.4043])\n",
      "MAPE loss: 0.2274562269449234\n",
      "Valing, Epoch: 1, Batch 0: Loss = 17217946.0\n",
      "logits: tensor([16272.0811])\n",
      "MAPE loss: 0.10168550163507462\n",
      "logits: tensor([14557.2344])\n",
      "MAPE loss: 0.19635511934757233\n",
      "logits: tensor([11533.3340])\n",
      "MAPE loss: 0.36778900027275085\n",
      "logits: tensor([7792.7485])\n",
      "MAPE loss: 0.23009803891181946\n",
      "logits: tensor([7230.6880])\n",
      "MAPE loss: 0.14137586951255798\n",
      "logits: tensor([6277.4526])\n",
      "MAPE loss: 0.009093888103961945\n",
      "logits: tensor([15471.5674])\n",
      "MAPE loss: 0.14587856829166412\n",
      "logits: tensor([15814.8818])\n",
      "MAPE loss: 0.12692558765411377\n",
      "logits: tensor([16334.1338])\n",
      "MAPE loss: 0.0982598289847374\n",
      "logits: tensor([9264.2529])\n",
      "MAPE loss: 0.4623773992061615\n",
      "logits: tensor([9380.4727])\n",
      "MAPE loss: 0.4807228744029999\n",
      "logits: tensor([15907.8760])\n",
      "MAPE loss: 0.1217917650938034\n",
      "logits: tensor([8390.3994])\n",
      "MAPE loss: 0.3244381844997406\n",
      "logits: tensor([15827.5713])\n",
      "MAPE loss: 0.1323961764574051\n",
      "logits: tensor([12185.8535])\n",
      "MAPE loss: 0.9235568046569824\n",
      "logits: tensor([15225.0107])\n",
      "MAPE loss: 0.1594899445772171\n",
      "logits: tensor([13932.0469])\n",
      "MAPE loss: 0.23630119860172272\n",
      "logits: tensor([15036.8926])\n",
      "MAPE loss: 0.1698751598596573\n",
      "logits: tensor([14855.3574])\n",
      "MAPE loss: 0.1856890320777893\n",
      "logits: tensor([8675.1553])\n",
      "MAPE loss: 0.3693873882293701\n",
      "logits: tensor([16985.7852])\n",
      "MAPE loss: 0.062284842133522034\n",
      "logits: tensor([16487.3203])\n",
      "MAPE loss: 0.08980303257703781\n",
      "logits: tensor([16377.4414])\n",
      "MAPE loss: 0.10225450247526169\n",
      "logits: tensor([16823.1602])\n",
      "MAPE loss: 0.0778219923377037\n",
      "logits: tensor([15609.1846])\n",
      "MAPE loss: 0.1443672478199005\n",
      "logits: tensor([13170.4971])\n",
      "MAPE loss: 0.27804630994796753\n",
      "logits: tensor([15069.7568])\n",
      "MAPE loss: 0.16806086897850037\n",
      "logits: tensor([8639.7744])\n",
      "MAPE loss: 0.3638024628162384\n",
      "logits: tensor([10673.8721])\n",
      "MAPE loss: 0.6848880648612976\n",
      "logits: tensor([15249.7471])\n",
      "MAPE loss: 0.15812434256076813\n",
      "logits: tensor([9836.7676])\n",
      "MAPE loss: 0.552749752998352\n",
      "logits: tensor([14734.3740])\n",
      "MAPE loss: 0.19232085347175598\n",
      "logits: tensor([7753.7339])\n",
      "MAPE loss: 0.22393950819969177\n",
      "logits: tensor([13131.5156])\n",
      "MAPE loss: 0.2801831066608429\n",
      "logits: tensor([14428.8115])\n",
      "MAPE loss: 0.20907056331634521\n",
      "logits: tensor([14866.7227])\n",
      "MAPE loss: 0.18506602942943573\n",
      "logits: tensor([9391.5684])\n",
      "MAPE loss: 0.48247435688972473\n",
      "logits: tensor([13985.2432])\n",
      "MAPE loss: 0.23338519036769867\n",
      "logits: tensor([16014.7998])\n",
      "MAPE loss: 0.11588893830776215\n",
      "logits: tensor([11008.1514])\n",
      "MAPE loss: 0.396577388048172\n",
      "logits: tensor([9302.4180])\n",
      "MAPE loss: 0.46840181946754456\n",
      "logits: tensor([9449.7197])\n",
      "MAPE loss: 0.4916536211967468\n",
      "logits: tensor([14257.2549])\n",
      "MAPE loss: 0.2184745967388153\n",
      "logits: tensor([9943.3516])\n",
      "MAPE loss: 0.45494544506073\n",
      "logits: tensor([12257.3408])\n",
      "MAPE loss: 0.32810184359550476\n",
      "logits: tensor([15060.5557])\n",
      "MAPE loss: 0.1744408756494522\n",
      "logits: tensor([16183.6396])\n",
      "MAPE loss: 0.11287793517112732\n",
      "logits: tensor([13662.7812])\n",
      "MAPE loss: 0.24573418498039246\n",
      "logits: tensor([8205.7451])\n",
      "MAPE loss: 0.2952902317047119\n",
      "logits: tensor([11071.7119])\n",
      "MAPE loss: 0.7476877570152283\n",
      "Valing, Epoch: 1, Batch 50: Loss = 22435844.0\n",
      "logits: tensor([15737.8857])\n",
      "MAPE loss: 0.13117621839046478\n",
      "logits: tensor([12775.1533])\n",
      "MAPE loss: 0.2997174561023712\n",
      "logits: tensor([14345.4932])\n",
      "MAPE loss: 0.21363773941993713\n",
      "logits: tensor([15717.4375])\n",
      "MAPE loss: 0.1323050856590271\n",
      "logits: tensor([15551.8271])\n",
      "MAPE loss: 0.1475113481283188\n",
      "logits: tensor([11949.2227])\n",
      "MAPE loss: 0.34499165415763855\n",
      "logits: tensor([14875.4482])\n",
      "MAPE loss: 0.17878784239292145\n",
      "logits: tensor([14120.4346])\n",
      "MAPE loss: 0.22046902775764465\n",
      "logits: tensor([13674.8906])\n",
      "MAPE loss: 0.2503974735736847\n",
      "logits: tensor([6946.0166])\n",
      "MAPE loss: 0.09644002467393875\n",
      "logits: tensor([15360.2031])\n",
      "MAPE loss: 0.1580154150724411\n",
      "logits: tensor([9004.2139])\n",
      "MAPE loss: 0.4213297963142395\n",
      "logits: tensor([14276.5928])\n",
      "MAPE loss: 0.211848184466362\n",
      "logits: tensor([15171.8154])\n",
      "MAPE loss: 0.1624266356229782\n",
      "logits: tensor([14527.7295])\n",
      "MAPE loss: 0.19798396527767181\n",
      "logits: tensor([13975.4834])\n",
      "MAPE loss: 0.23392018675804138\n",
      "logits: tensor([13128.1436])\n",
      "MAPE loss: 0.2803679406642914\n",
      "logits: tensor([15087.4668])\n",
      "MAPE loss: 0.16708317399024963\n",
      "logits: tensor([7880.4932])\n",
      "MAPE loss: 0.24394866824150085\n",
      "logits: tensor([14860.9971])\n",
      "MAPE loss: 0.18537987768650055\n",
      "logits: tensor([10321.9160])\n",
      "MAPE loss: 0.6293312311172485\n",
      "logits: tensor([16050.2314])\n",
      "MAPE loss: 0.11393290758132935\n",
      "logits: tensor([10357.7617])\n",
      "MAPE loss: 0.6349895596504211\n",
      "logits: tensor([6778.7026])\n",
      "MAPE loss: 0.07002924382686615\n",
      "logits: tensor([13665.8604])\n",
      "MAPE loss: 0.2508924603462219\n",
      "logits: tensor([16738.2461])\n",
      "MAPE loss: 0.07595045119524002\n",
      "logits: tensor([13819.7021])\n",
      "MAPE loss: 0.2424594908952713\n",
      "logits: tensor([11549.1514])\n",
      "MAPE loss: 0.36692196130752563\n",
      "logits: tensor([15040.6182])\n",
      "MAPE loss: 0.1696694940328598\n",
      "logits: tensor([9972.3662])\n",
      "MAPE loss: 0.574154257774353\n",
      "logits: tensor([15641.0449])\n",
      "MAPE loss: 0.13652241230010986\n",
      "logits: tensor([16200.3662])\n",
      "MAPE loss: 0.11196105182170868\n",
      "logits: tensor([15164.9443])\n",
      "MAPE loss: 0.16871871054172516\n",
      "logits: tensor([17095.6797])\n",
      "MAPE loss: 0.056218020617961884\n",
      "logits: tensor([13990.2969])\n",
      "MAPE loss: 0.2276533991098404\n",
      "logits: tensor([14307.6270])\n",
      "MAPE loss: 0.21013490855693817\n",
      "logits: tensor([16263.2627])\n",
      "MAPE loss: 0.10851331800222397\n",
      "logits: tensor([12929.1416])\n",
      "MAPE loss: 0.28623539209365845\n",
      "logits: tensor([7708.4927])\n",
      "MAPE loss: 0.2167981117963791\n",
      "logits: tensor([13897.8359])\n",
      "MAPE loss: 0.2327577918767929\n",
      "logits: tensor([10709.0645])\n",
      "MAPE loss: 0.4129721522331238\n",
      "logits: tensor([14483.8760])\n",
      "MAPE loss: 0.20040492713451385\n",
      "logits: tensor([13044.3779])\n",
      "MAPE loss: 0.27987369894981384\n",
      "logits: tensor([7123.7407])\n",
      "MAPE loss: 0.12449406087398529\n",
      "logits: tensor([14847.4268])\n",
      "MAPE loss: 0.18612375855445862\n",
      "logits: tensor([16705.5840])\n",
      "MAPE loss: 0.0842670425772667\n",
      "logits: tensor([12842.9014])\n",
      "MAPE loss: 0.29099637269973755\n",
      "logits: tensor([16005.9863])\n",
      "MAPE loss: 0.11637549847364426\n",
      "logits: tensor([14636.1260])\n",
      "MAPE loss: 0.1977064162492752\n",
      "logits: tensor([13164.4795])\n",
      "MAPE loss: 0.27837616205215454\n",
      "Valing, Epoch: 1, Batch 100: Loss = 25789902.0\n",
      "logits: tensor([14530.3486])\n",
      "MAPE loss: 0.19783936440944672\n",
      "logits: tensor([10924.9600])\n",
      "MAPE loss: 0.7245227098464966\n",
      "logits: tensor([13786.7275])\n",
      "MAPE loss: 0.2388916164636612\n",
      "logits: tensor([16314.4795])\n",
      "MAPE loss: 0.09934486448764801\n",
      "logits: tensor([12923.3232])\n",
      "MAPE loss: 0.29159536957740784\n",
      "logits: tensor([8486.7285])\n",
      "MAPE loss: 0.3396438956260681\n",
      "logits: tensor([11366.1191])\n",
      "MAPE loss: 0.7941604256629944\n",
      "logits: tensor([15226.9111])\n",
      "MAPE loss: 0.15938502550125122\n",
      "logits: tensor([11637.8193])\n",
      "MAPE loss: 0.8370487093925476\n",
      "logits: tensor([10981.0293])\n",
      "MAPE loss: 0.39806410670280457\n",
      "logits: tensor([16463.9062])\n",
      "MAPE loss: 0.09751484543085098\n",
      "logits: tensor([16203.5381])\n",
      "MAPE loss: 0.1054694801568985\n",
      "logits: tensor([14707.1650])\n",
      "MAPE loss: 0.19381234049797058\n",
      "logits: tensor([15245.7471])\n",
      "MAPE loss: 0.15834517776966095\n",
      "logits: tensor([15603.5879])\n",
      "MAPE loss: 0.1385902613401413\n",
      "logits: tensor([8873.6885])\n",
      "MAPE loss: 0.40072616934776306\n",
      "logits: tensor([15726.2500])\n",
      "MAPE loss: 0.13795019686222076\n",
      "logits: tensor([9477.8770])\n",
      "MAPE loss: 0.4960983097553253\n",
      "logits: tensor([16525.5703])\n",
      "MAPE loss: 0.08769141137599945\n",
      "logits: tensor([12268.3438])\n",
      "MAPE loss: 0.3274987041950226\n",
      "logits: tensor([13284.6748])\n",
      "MAPE loss: 0.27178752422332764\n",
      "logits: tensor([8941.2285])\n",
      "MAPE loss: 0.41138747334480286\n",
      "logits: tensor([11990.5791])\n",
      "MAPE loss: 0.34272465109825134\n",
      "logits: tensor([14863.7891])\n",
      "MAPE loss: 0.1794314980506897\n",
      "logits: tensor([8307.1523])\n",
      "MAPE loss: 0.3112975060939789\n",
      "logits: tensor([16316.7803])\n",
      "MAPE loss: 0.10557969659566879\n",
      "logits: tensor([9145.0693])\n",
      "MAPE loss: 0.4435640871524811\n",
      "logits: tensor([12277.4795])\n",
      "MAPE loss: 0.32699793577194214\n",
      "logits: tensor([15166.1748])\n",
      "MAPE loss: 0.1686512678861618\n",
      "logits: tensor([15871.9395])\n",
      "MAPE loss: 0.1237756758928299\n",
      "logits: tensor([10254.1123])\n",
      "MAPE loss: 0.6186283230781555\n",
      "logits: tensor([9953.6533])\n",
      "MAPE loss: 0.5712003707885742\n",
      "logits: tensor([10371.5439])\n",
      "MAPE loss: 0.6371650695800781\n",
      "logits: tensor([13926.3252])\n",
      "MAPE loss: 0.23661483824253082\n",
      "logits: tensor([8744.2861])\n",
      "MAPE loss: 0.38029980659484863\n",
      "logits: tensor([12007.1455])\n",
      "MAPE loss: 0.3418165445327759\n",
      "logits: tensor([13940.1064])\n",
      "MAPE loss: 0.2304242104291916\n",
      "logits: tensor([8544.8203])\n",
      "MAPE loss: 0.3488138020038605\n",
      "logits: tensor([14744.4141])\n",
      "MAPE loss: 0.1917704939842224\n",
      "logits: tensor([13947.2412])\n",
      "MAPE loss: 0.23003032803535461\n",
      "logits: tensor([13877.3418])\n",
      "MAPE loss: 0.23929990828037262\n",
      "logits: tensor([14085.8154])\n",
      "MAPE loss: 0.22787222266197205\n",
      "logits: tensor([7705.7222])\n",
      "MAPE loss: 0.21636077761650085\n",
      "logits: tensor([14691.9004])\n",
      "MAPE loss: 0.1889207661151886\n",
      "logits: tensor([9972.4834])\n",
      "MAPE loss: 0.5741727352142334\n",
      "logits: tensor([7359.3921])\n",
      "MAPE loss: 0.16169200837612152\n",
      "logits: tensor([11991.3281])\n",
      "MAPE loss: 0.3426835834980011\n",
      "logits: tensor([15977.1064])\n",
      "MAPE loss: 0.11796983331441879\n",
      "logits: tensor([9191.3379])\n",
      "MAPE loss: 0.4508676528930664\n",
      "logits: tensor([9291.4512])\n",
      "MAPE loss: 0.4666706919670105\n",
      "Valing, Epoch: 1, Batch 150: Loss = 8740231.0\n",
      "logits: tensor([10471.3232])\n",
      "MAPE loss: 0.6529154181480408\n",
      "logits: tensor([13717.4316])\n",
      "MAPE loss: 0.24271716177463531\n",
      "logits: tensor([9708.6465])\n",
      "MAPE loss: 0.5325256586074829\n",
      "logits: tensor([14613.3877])\n",
      "MAPE loss: 0.19325512647628784\n",
      "logits: tensor([9235.5186])\n",
      "MAPE loss: 0.4578416347503662\n",
      "logits: tensor([15389.2627])\n",
      "MAPE loss: 0.15042226016521454\n",
      "logits: tensor([15651.2607])\n",
      "MAPE loss: 0.13595843315124512\n",
      "logits: tensor([10054.6045])\n",
      "MAPE loss: 0.5871356725692749\n",
      "logits: tensor([9430.1455])\n",
      "MAPE loss: 0.48856380581855774\n",
      "logits: tensor([10891.8896])\n",
      "MAPE loss: 0.7193024754524231\n",
      "logits: tensor([16966.9082])\n",
      "MAPE loss: 0.06994230300188065\n",
      "logits: tensor([7500.7007])\n",
      "MAPE loss: 0.18399780988693237\n",
      "logits: tensor([17724.7656])\n",
      "MAPE loss: 0.028399601578712463\n",
      "logits: tensor([13795.0850])\n",
      "MAPE loss: 0.24380889534950256\n",
      "logits: tensor([15984.5947])\n",
      "MAPE loss: 0.11755643784999847\n",
      "logits: tensor([11437.3018])\n",
      "MAPE loss: 0.8053966760635376\n",
      "logits: tensor([14817.1221])\n",
      "MAPE loss: 0.1820077896118164\n",
      "logits: tensor([15217.5381])\n",
      "MAPE loss: 0.15990246832370758\n",
      "logits: tensor([9081.9912])\n",
      "MAPE loss: 0.4336071014404297\n",
      "logits: tensor([16033.5771])\n",
      "MAPE loss: 0.12110375612974167\n",
      "logits: tensor([10170.3154])\n",
      "MAPE loss: 0.6054008603096008\n",
      "logits: tensor([13186.8154])\n",
      "MAPE loss: 0.27715179324150085\n",
      "logits: tensor([14416.7051])\n",
      "MAPE loss: 0.20973418653011322\n",
      "logits: tensor([15581.6885])\n",
      "MAPE loss: 0.13979923725128174\n",
      "logits: tensor([15464.4570])\n",
      "MAPE loss: 0.14627109467983246\n",
      "logits: tensor([8814.9336])\n",
      "MAPE loss: 0.3914515972137451\n",
      "logits: tensor([15398.2373])\n",
      "MAPE loss: 0.1559305340051651\n",
      "logits: tensor([14903.3115])\n",
      "MAPE loss: 0.17724962532520294\n",
      "logits: tensor([14952.8740])\n",
      "MAPE loss: 0.17451348900794983\n",
      "logits: tensor([9995.6406])\n",
      "MAPE loss: 0.5778281092643738\n",
      "logits: tensor([15925.4023])\n",
      "MAPE loss: 0.12082420289516449\n",
      "logits: tensor([10028.2422])\n",
      "MAPE loss: 0.582974374294281\n",
      "logits: tensor([12512.3809])\n",
      "MAPE loss: 0.3141215741634369\n",
      "logits: tensor([14400.8096])\n",
      "MAPE loss: 0.2049906849861145\n",
      "logits: tensor([17027.0156])\n",
      "MAPE loss: 0.06000867858529091\n",
      "logits: tensor([14369.8389])\n",
      "MAPE loss: 0.20670044422149658\n",
      "logits: tensor([15481.6260])\n",
      "MAPE loss: 0.1453232765197754\n",
      "logits: tensor([10745.2656])\n",
      "MAPE loss: 0.6961576342582703\n",
      "logits: tensor([15735.0068])\n",
      "MAPE loss: 0.13133515417575836\n",
      "logits: tensor([14513.7002])\n",
      "MAPE loss: 0.20441730320453644\n",
      "logits: tensor([14756.1396])\n",
      "MAPE loss: 0.18537437915802002\n",
      "logits: tensor([7568.7124])\n",
      "MAPE loss: 0.19473356008529663\n",
      "logits: tensor([8485.4814])\n",
      "MAPE loss: 0.3394470512866974\n",
      "logits: tensor([15459.5342])\n",
      "MAPE loss: 0.14654286205768585\n",
      "logits: tensor([9172.1621])\n",
      "MAPE loss: 0.44784072041511536\n",
      "logits: tensor([11553.4697])\n",
      "MAPE loss: 0.8237339854240417\n",
      "logits: tensor([16516.1172])\n",
      "MAPE loss: 0.08821327239274979\n",
      "logits: tensor([9091.9619])\n",
      "MAPE loss: 0.43518099188804626\n",
      "logits: tensor([17065.8555])\n",
      "MAPE loss: 0.057864490896463394\n",
      "logits: tensor([13813.5039])\n",
      "MAPE loss: 0.2374134063720703\n",
      "Valing, Epoch: 1, Batch 200: Loss = 18494384.0\n",
      "logits: tensor([9225.4443])\n",
      "MAPE loss: 0.4562514126300812\n",
      "logits: tensor([15089.4814])\n",
      "MAPE loss: 0.16697195172309875\n",
      "logits: tensor([14221.5654])\n",
      "MAPE loss: 0.21488600969314575\n",
      "logits: tensor([12412.1025])\n",
      "MAPE loss: 0.31961843371391296\n",
      "logits: tensor([16979.9453])\n",
      "MAPE loss: 0.0692276582121849\n",
      "logits: tensor([15137.6104])\n",
      "MAPE loss: 0.16431495547294617\n",
      "logits: tensor([9563.9717])\n",
      "MAPE loss: 0.5096884965896606\n",
      "logits: tensor([13985.5322])\n",
      "MAPE loss: 0.2279164344072342\n",
      "logits: tensor([7415.9116])\n",
      "MAPE loss: 0.17061372101306915\n",
      "logits: tensor([15846.9883])\n",
      "MAPE loss: 0.12515312433242798\n",
      "logits: tensor([10069.4844])\n",
      "MAPE loss: 0.5894845128059387\n",
      "logits: tensor([14182.4717])\n",
      "MAPE loss: 0.222573921084404\n",
      "logits: tensor([13050.8154])\n",
      "MAPE loss: 0.2846067547798157\n",
      "logits: tensor([15563.7334])\n",
      "MAPE loss: 0.14685870707035065\n",
      "logits: tensor([9723.6494])\n",
      "MAPE loss: 0.5348938703536987\n",
      "logits: tensor([16900.4492])\n",
      "MAPE loss: 0.06699588894844055\n",
      "logits: tensor([12920.4434])\n",
      "MAPE loss: 0.29175323247909546\n",
      "logits: tensor([15199.4580])\n",
      "MAPE loss: 0.16090059280395508\n",
      "logits: tensor([13983.3740])\n",
      "MAPE loss: 0.2280355840921402\n",
      "logits: tensor([17314.9336])\n",
      "MAPE loss: 0.050864946097135544\n",
      "logits: tensor([10865.6357])\n",
      "MAPE loss: 0.7151582837104797\n",
      "logits: tensor([17048.8379])\n",
      "MAPE loss: 0.05880396068096161\n",
      "logits: tensor([9597.1885])\n",
      "MAPE loss: 0.5149317979812622\n",
      "logits: tensor([11852.0166])\n",
      "MAPE loss: 0.3456990420818329\n",
      "logits: tensor([9604.8467])\n",
      "MAPE loss: 0.5161406993865967\n",
      "logits: tensor([15339.7891])\n",
      "MAPE loss: 0.15913443267345428\n",
      "logits: tensor([11370.1387])\n",
      "MAPE loss: 0.3767347037792206\n",
      "logits: tensor([7187.9829])\n",
      "MAPE loss: 0.13463479280471802\n",
      "logits: tensor([15089.7373])\n",
      "MAPE loss: 0.17284126579761505\n",
      "logits: tensor([16770.2109])\n",
      "MAPE loss: 0.074185810983181\n",
      "logits: tensor([9338.5029])\n",
      "MAPE loss: 0.47409787774086\n",
      "logits: tensor([7166.0645])\n",
      "MAPE loss: 0.1311749368906021\n",
      "logits: tensor([9029.7441])\n",
      "MAPE loss: 0.4253598153591156\n",
      "logits: tensor([11640.3623])\n",
      "MAPE loss: 0.8374501466751099\n",
      "logits: tensor([15145.9678])\n",
      "MAPE loss: 0.16385357081890106\n",
      "logits: tensor([13294.4893])\n",
      "MAPE loss: 0.27124953269958496\n",
      "logits: tensor([14684.8066])\n",
      "MAPE loss: 0.19503793120384216\n",
      "logits: tensor([13052.6963])\n",
      "MAPE loss: 0.27941447496414185\n",
      "logits: tensor([15724.3086])\n",
      "MAPE loss: 0.1319257616996765\n",
      "logits: tensor([10752.0830])\n",
      "MAPE loss: 0.4106140434741974\n",
      "logits: tensor([16807.4629])\n",
      "MAPE loss: 0.07868245244026184\n",
      "logits: tensor([14997.0146])\n",
      "MAPE loss: 0.17792394757270813\n",
      "logits: tensor([12550.3438])\n",
      "MAPE loss: 0.3120405972003937\n",
      "logits: tensor([13025.2715])\n",
      "MAPE loss: 0.28092846274375916\n",
      "logits: tensor([15935.3330])\n",
      "MAPE loss: 0.12648910284042358\n",
      "logits: tensor([13909.3076])\n",
      "MAPE loss: 0.23754766583442688\n",
      "logits: tensor([15384.5986])\n",
      "MAPE loss: 0.15667815506458282\n",
      "logits: tensor([9912.9160])\n",
      "MAPE loss: 0.5647699236869812\n",
      "logits: tensor([15223.1299])\n",
      "MAPE loss: 0.16552922129631042\n",
      "logits: tensor([7838.3784])\n",
      "MAPE loss: 0.2373007833957672\n",
      "Valing, Epoch: 1, Batch 250: Loss = 2259957.25\n",
      "logits: tensor([7063.7153])\n",
      "MAPE loss: 0.11501895636320114\n",
      "logits: tensor([14767.6885])\n",
      "MAPE loss: 0.18473681807518005\n",
      "logits: tensor([14641.5303])\n",
      "MAPE loss: 0.19741016626358032\n",
      "logits: tensor([14976.1631])\n",
      "MAPE loss: 0.17906694114208221\n",
      "logits: tensor([15816.5381])\n",
      "MAPE loss: 0.13300096988677979\n",
      "logits: tensor([10377.2891])\n",
      "MAPE loss: 0.43115872144699097\n",
      "logits: tensor([16905.4668])\n",
      "MAPE loss: 0.07331027090549469\n",
      "logits: tensor([13769.0820])\n",
      "MAPE loss: 0.2398657649755478\n",
      "logits: tensor([13068.8916])\n",
      "MAPE loss: 0.2785203754901886\n",
      "logits: tensor([14359.5146])\n",
      "MAPE loss: 0.21286913752555847\n",
      "logits: tensor([15030.5518])\n",
      "MAPE loss: 0.17608557641506195\n",
      "logits: tensor([16777.1719])\n",
      "MAPE loss: 0.07380152493715286\n",
      "logits: tensor([15158.5889])\n",
      "MAPE loss: 0.16315682232379913\n",
      "logits: tensor([13940.7627])\n",
      "MAPE loss: 0.23038797080516815\n",
      "logits: tensor([12568.1318])\n",
      "MAPE loss: 0.30616527795791626\n",
      "logits: tensor([11773.3174])\n",
      "MAPE loss: 0.8584372997283936\n",
      "logits: tensor([15261.1621])\n",
      "MAPE loss: 0.16344444453716278\n",
      "logits: tensor([9136.0664])\n",
      "MAPE loss: 0.44214293360710144\n",
      "logits: tensor([13945.8594])\n",
      "MAPE loss: 0.23010660707950592\n",
      "logits: tensor([11305.4160])\n",
      "MAPE loss: 0.7845783233642578\n",
      "logits: tensor([16000.6475])\n",
      "MAPE loss: 0.11667023599147797\n",
      "logits: tensor([13386.4746])\n",
      "MAPE loss: 0.26098793745040894\n",
      "logits: tensor([15066.0742])\n",
      "MAPE loss: 0.16826416552066803\n",
      "logits: tensor([10118.2373])\n",
      "MAPE loss: 0.5971802473068237\n",
      "logits: tensor([16779.5742])\n",
      "MAPE loss: 0.07366889715194702\n",
      "logits: tensor([9938.3779])\n",
      "MAPE loss: 0.5687891244888306\n",
      "logits: tensor([11267.3193])\n",
      "MAPE loss: 0.7785646915435791\n",
      "logits: tensor([13478.5488])\n",
      "MAPE loss: 0.2559049129486084\n",
      "logits: tensor([9903.0068])\n",
      "MAPE loss: 0.5632057189941406\n",
      "logits: tensor([15013.0908])\n",
      "MAPE loss: 0.17704270780086517\n",
      "logits: tensor([9280.5205])\n",
      "MAPE loss: 0.4649452567100525\n",
      "logits: tensor([16960.7344])\n",
      "MAPE loss: 0.06366779655218124\n",
      "logits: tensor([7491.0439])\n",
      "MAPE loss: 0.18247348070144653\n",
      "logits: tensor([16067.3027])\n",
      "MAPE loss: 0.11925505846738815\n",
      "logits: tensor([15424.2646])\n",
      "MAPE loss: 0.15450382232666016\n",
      "logits: tensor([15342.4658])\n",
      "MAPE loss: 0.15300573408603668\n",
      "logits: tensor([13551.0430])\n",
      "MAPE loss: 0.25718629360198975\n",
      "logits: tensor([16765.4805])\n",
      "MAPE loss: 0.07444695383310318\n",
      "logits: tensor([8678.5020])\n",
      "MAPE loss: 0.3699156641960144\n",
      "logits: tensor([14667.8369])\n",
      "MAPE loss: 0.19024920463562012\n",
      "logits: tensor([14398.6475])\n",
      "MAPE loss: 0.20511004328727722\n",
      "logits: tensor([9440.3330])\n",
      "MAPE loss: 0.4901719391345978\n",
      "logits: tensor([8417.6396])\n",
      "MAPE loss: 0.32873812317848206\n",
      "logits: tensor([15092.4121])\n",
      "MAPE loss: 0.16681016981601715\n",
      "logits: tensor([8392.2959])\n",
      "MAPE loss: 0.324737548828125\n",
      "logits: tensor([16504.9551])\n",
      "MAPE loss: 0.08882948756217957\n",
      "logits: tensor([14738.0547])\n",
      "MAPE loss: 0.186372771859169\n",
      "logits: tensor([8302.8652])\n",
      "MAPE loss: 0.3106207847595215\n",
      "logits: tensor([8749.6045])\n",
      "MAPE loss: 0.3811393082141876\n",
      "logits: tensor([9027.5762])\n",
      "MAPE loss: 0.4250175952911377\n",
      "Valing, Epoch: 1, Batch 300: Loss = 7249627.0\n",
      "logits: tensor([14121.4883])\n",
      "MAPE loss: 0.225916787981987\n",
      "logits: tensor([16553.7090])\n",
      "MAPE loss: 0.08613798767328262\n",
      "logits: tensor([9542.1084])\n",
      "MAPE loss: 0.5062373280525208\n",
      "logits: tensor([15341.8525])\n",
      "MAPE loss: 0.1530395895242691\n",
      "logits: tensor([14566.2959])\n",
      "MAPE loss: 0.2015342116355896\n",
      "logits: tensor([15044.7549])\n",
      "MAPE loss: 0.1753070205450058\n",
      "logits: tensor([7031.6099])\n",
      "MAPE loss: 0.10995105654001236\n",
      "logits: tensor([8612.1924])\n",
      "MAPE loss: 0.3594485819339752\n",
      "logits: tensor([9450.5537])\n",
      "MAPE loss: 0.49178528785705566\n",
      "logits: tensor([13889.6123])\n",
      "MAPE loss: 0.23321178555488586\n",
      "logits: tensor([17136.9453])\n",
      "MAPE loss: 0.06062154844403267\n",
      "logits: tensor([3113.8813])\n",
      "MAPE loss: 0.8280954957008362\n",
      "logits: tensor([9798.2705])\n",
      "MAPE loss: 0.5466729402542114\n",
      "logits: tensor([11523.1377])\n",
      "MAPE loss: 0.8189460039138794\n",
      "logits: tensor([8791.5381])\n",
      "MAPE loss: 0.3877585828304291\n",
      "logits: tensor([12312.8486])\n",
      "MAPE loss: 0.9436032176017761\n",
      "logits: tensor([15683.1494])\n",
      "MAPE loss: 0.13419799506664276\n",
      "logits: tensor([13425.3486])\n",
      "MAPE loss: 0.258841872215271\n",
      "logits: tensor([15365.6553])\n",
      "MAPE loss: 0.15172553062438965\n",
      "logits: tensor([16683.8750])\n",
      "MAPE loss: 0.07895205914974213\n",
      "logits: tensor([16589.9805])\n",
      "MAPE loss: 0.09060396254062653\n",
      "logits: tensor([10419.3193])\n",
      "MAPE loss: 0.6447064876556396\n",
      "logits: tensor([13146.4590])\n",
      "MAPE loss: 0.2742381989955902\n",
      "logits: tensor([10077.8857])\n",
      "MAPE loss: 0.5908106565475464\n",
      "logits: tensor([9256.6924])\n",
      "MAPE loss: 0.46118396520614624\n",
      "logits: tensor([10024.1436])\n",
      "MAPE loss: 0.5823273658752441\n",
      "logits: tensor([8877.7266])\n",
      "MAPE loss: 0.4013635814189911\n",
      "logits: tensor([16550.8926])\n",
      "MAPE loss: 0.09274660050868988\n",
      "logits: tensor([7599.5591])\n",
      "MAPE loss: 0.19960276782512665\n",
      "logits: tensor([14541.0381])\n",
      "MAPE loss: 0.20291875302791595\n",
      "logits: tensor([12379.2529])\n",
      "MAPE loss: 0.3165924847126007\n",
      "logits: tensor([10908.3750])\n",
      "MAPE loss: 0.7219047546386719\n",
      "logits: tensor([10747.0479])\n",
      "MAPE loss: 0.6964389681816101\n",
      "logits: tensor([15581.5938])\n",
      "MAPE loss: 0.13980446755886078\n",
      "logits: tensor([9955.9209])\n",
      "MAPE loss: 0.5715582966804504\n",
      "logits: tensor([10080.3955])\n",
      "MAPE loss: 0.5912068486213684\n",
      "logits: tensor([11012.9092])\n",
      "MAPE loss: 0.7384056448936462\n",
      "logits: tensor([15422.8643])\n",
      "MAPE loss: 0.14856725931167603\n",
      "logits: tensor([12306.8320])\n",
      "MAPE loss: 0.942653477191925\n",
      "logits: tensor([15722.4990])\n",
      "MAPE loss: 0.13202565908432007\n",
      "logits: tensor([16379.7812])\n",
      "MAPE loss: 0.09573981910943985\n",
      "logits: tensor([8464.2939])\n",
      "MAPE loss: 0.33610257506370544\n",
      "logits: tensor([15746.1338])\n",
      "MAPE loss: 0.13686025142669678\n",
      "logits: tensor([13087.6738])\n",
      "MAPE loss: 0.2774834930896759\n",
      "logits: tensor([17326.9316])\n",
      "MAPE loss: 0.05020726099610329\n",
      "logits: tensor([9937.2529])\n",
      "MAPE loss: 0.5686115622520447\n",
      "logits: tensor([10027.3711])\n",
      "MAPE loss: 0.5828368663787842\n",
      "logits: tensor([14936.1240])\n",
      "MAPE loss: 0.17543818056583405\n",
      "logits: tensor([11892.1045])\n",
      "MAPE loss: 0.8771880269050598\n",
      "logits: tensor([10589.8965])\n",
      "MAPE loss: 0.6716324090957642\n",
      "Valing, Epoch: 1, Batch 350: Loss = 18103608.0\n",
      "logits: tensor([15470.9062])\n",
      "MAPE loss: 0.14591506123542786\n",
      "logits: tensor([10073.0020])\n",
      "MAPE loss: 0.5900397300720215\n",
      "logits: tensor([15178.7168])\n",
      "MAPE loss: 0.16796375811100006\n",
      "logits: tensor([15343.4922])\n",
      "MAPE loss: 0.15294906497001648\n",
      "logits: tensor([10410.7812])\n",
      "MAPE loss: 0.6433587670326233\n",
      "logits: tensor([13415.7373])\n",
      "MAPE loss: 0.2593724727630615\n",
      "logits: tensor([14873.0312])\n",
      "MAPE loss: 0.17892128229141235\n",
      "logits: tensor([9242.2549])\n",
      "MAPE loss: 0.4589049816131592\n",
      "logits: tensor([10058.6289])\n",
      "MAPE loss: 0.587770938873291\n",
      "logits: tensor([14634.0518])\n",
      "MAPE loss: 0.19211435317993164\n",
      "logits: tensor([16887.8887])\n",
      "MAPE loss: 0.07427383214235306\n",
      "logits: tensor([10735.7764])\n",
      "MAPE loss: 0.6946597695350647\n",
      "logits: tensor([9868.4844])\n",
      "MAPE loss: 0.5577563047409058\n",
      "logits: tensor([9849.4482])\n",
      "MAPE loss: 0.554751455783844\n",
      "logits: tensor([16696.4727])\n",
      "MAPE loss: 0.07825659215450287\n",
      "logits: tensor([9502.6787])\n",
      "MAPE loss: 0.5000132918357849\n",
      "logits: tensor([16178.1562])\n",
      "MAPE loss: 0.11317851394414902\n",
      "logits: tensor([17700.5762])\n",
      "MAPE loss: 0.0297255702316761\n",
      "logits: tensor([8982.6533])\n",
      "MAPE loss: 0.4179264307022095\n",
      "logits: tensor([8649.7451])\n",
      "MAPE loss: 0.365376353263855\n",
      "logits: tensor([10245.6689])\n",
      "MAPE loss: 0.617295503616333\n",
      "logits: tensor([8916.4854])\n",
      "MAPE loss: 0.40748172998428345\n",
      "logits: tensor([14973.8447])\n",
      "MAPE loss: 0.17919403314590454\n",
      "logits: tensor([15979.5557])\n",
      "MAPE loss: 0.12406499683856964\n",
      "logits: tensor([15661.0020])\n",
      "MAPE loss: 0.14152683317661285\n",
      "logits: tensor([14926.4775])\n",
      "MAPE loss: 0.17597073316574097\n",
      "logits: tensor([16468.0293])\n",
      "MAPE loss: 0.0908680111169815\n",
      "logits: tensor([15838.9502])\n",
      "MAPE loss: 0.13177242875099182\n",
      "logits: tensor([15937.8301])\n",
      "MAPE loss: 0.12013812363147736\n",
      "logits: tensor([14903.6436])\n",
      "MAPE loss: 0.1772312968969345\n",
      "logits: tensor([15103.2002])\n",
      "MAPE loss: 0.16621460020542145\n",
      "logits: tensor([14984.7178])\n",
      "MAPE loss: 0.17275552451610565\n",
      "logits: tensor([16960.9258])\n",
      "MAPE loss: 0.06365722417831421\n",
      "logits: tensor([14467.2510])\n",
      "MAPE loss: 0.2013227343559265\n",
      "logits: tensor([11507.6328])\n",
      "MAPE loss: 0.8164985775947571\n",
      "logits: tensor([9365.0205])\n",
      "MAPE loss: 0.47828373312950134\n",
      "logits: tensor([9325.2764])\n",
      "MAPE loss: 0.4720100462436676\n",
      "logits: tensor([16890.6582])\n",
      "MAPE loss: 0.06753641366958618\n",
      "logits: tensor([15485.9854])\n",
      "MAPE loss: 0.14508260786533356\n",
      "logits: tensor([15599.0312])\n",
      "MAPE loss: 0.13884180784225464\n",
      "logits: tensor([15387.7939])\n",
      "MAPE loss: 0.15650299191474915\n",
      "logits: tensor([7807.2783])\n",
      "MAPE loss: 0.2323915809392929\n",
      "logits: tensor([16150.3877])\n",
      "MAPE loss: 0.10840369015932083\n",
      "logits: tensor([12125.4365])\n",
      "MAPE loss: 0.33533230423927307\n",
      "logits: tensor([15122.5127])\n",
      "MAPE loss: 0.17104464769363403\n",
      "logits: tensor([8521.0586])\n",
      "MAPE loss: 0.3450629711151123\n",
      "logits: tensor([15321.4893])\n",
      "MAPE loss: 0.16013754904270172\n",
      "logits: tensor([14582.8613])\n",
      "MAPE loss: 0.2006261646747589\n",
      "logits: tensor([16198.3398])\n",
      "MAPE loss: 0.10575645416975021\n",
      "logits: tensor([14816.8564])\n",
      "MAPE loss: 0.18779949843883514\n",
      "Valing, Epoch: 1, Batch 400: Loss = 11737469.0\n",
      "logits: tensor([16946.0977])\n",
      "MAPE loss: 0.07108305394649506\n",
      "logits: tensor([10077.2812])\n",
      "MAPE loss: 0.590715229511261\n",
      "logits: tensor([9744.3135])\n",
      "MAPE loss: 0.538155734539032\n",
      "logits: tensor([13919.1719])\n",
      "MAPE loss: 0.23157991468906403\n",
      "logits: tensor([15557.9688])\n",
      "MAPE loss: 0.14717470109462738\n",
      "logits: tensor([15819.6162])\n",
      "MAPE loss: 0.12666422128677368\n",
      "logits: tensor([11053.6143])\n",
      "MAPE loss: 0.7448309659957886\n",
      "logits: tensor([15029.4229])\n",
      "MAPE loss: 0.17028753459453583\n",
      "logits: tensor([13426.1729])\n",
      "MAPE loss: 0.26403117179870605\n",
      "logits: tensor([14166.0420])\n",
      "MAPE loss: 0.22347453236579895\n",
      "logits: tensor([9462.3350])\n",
      "MAPE loss: 0.49364498257637024\n",
      "logits: tensor([7205.0283])\n",
      "MAPE loss: 0.1373254358768463\n",
      "logits: tensor([12381.0596])\n",
      "MAPE loss: 0.3213200867176056\n",
      "logits: tensor([15078.1074])\n",
      "MAPE loss: 0.1675998717546463\n",
      "logits: tensor([14907.9727])\n",
      "MAPE loss: 0.1769922971725464\n",
      "logits: tensor([17241.8535])\n",
      "MAPE loss: 0.048148367553949356\n",
      "logits: tensor([10639.0625])\n",
      "MAPE loss: 0.6793933510780334\n",
      "logits: tensor([14655.4668])\n",
      "MAPE loss: 0.19664622843265533\n",
      "logits: tensor([9910.2705])\n",
      "MAPE loss: 0.5643523335456848\n",
      "logits: tensor([14773.9912])\n",
      "MAPE loss: 0.190149188041687\n",
      "logits: tensor([15390.8086])\n",
      "MAPE loss: 0.15633773803710938\n",
      "logits: tensor([11804.3574])\n",
      "MAPE loss: 0.8633370399475098\n",
      "logits: tensor([16195.1709])\n",
      "MAPE loss: 0.10593139380216599\n",
      "logits: tensor([17273.0625])\n",
      "MAPE loss: 0.053160153329372406\n",
      "logits: tensor([7952.2637])\n",
      "MAPE loss: 0.25527775287628174\n",
      "logits: tensor([16333.3965])\n",
      "MAPE loss: 0.09830053150653839\n",
      "logits: tensor([5394.0903])\n",
      "MAPE loss: 0.14853405952453613\n",
      "logits: tensor([7327.1714])\n",
      "MAPE loss: 0.15660592913627625\n",
      "logits: tensor([13381.4717])\n",
      "MAPE loss: 0.2612641155719757\n",
      "logits: tensor([14893.3193])\n",
      "MAPE loss: 0.18360809981822968\n",
      "logits: tensor([8169.1450])\n",
      "MAPE loss: 0.28951284289360046\n",
      "logits: tensor([14213.4736])\n",
      "MAPE loss: 0.22087451815605164\n",
      "logits: tensor([11671.6748])\n",
      "MAPE loss: 0.8423928618431091\n",
      "logits: tensor([7941.5967])\n",
      "MAPE loss: 0.25359395146369934\n",
      "logits: tensor([14601.4775])\n",
      "MAPE loss: 0.19391264021396637\n",
      "logits: tensor([14027.7314])\n",
      "MAPE loss: 0.23105615377426147\n",
      "logits: tensor([13747.0029])\n",
      "MAPE loss: 0.24644456803798676\n",
      "logits: tensor([8440.5264])\n",
      "MAPE loss: 0.33235082030296326\n",
      "logits: tensor([15523.0674])\n",
      "MAPE loss: 0.14303545653820038\n",
      "logits: tensor([12019.4805])\n",
      "MAPE loss: 0.8972945213317871\n",
      "logits: tensor([17595.3105])\n",
      "MAPE loss: 0.0354958102107048\n",
      "logits: tensor([9114.8965])\n",
      "MAPE loss: 0.43880122900009155\n",
      "logits: tensor([10479.4141])\n",
      "MAPE loss: 0.6541925668716431\n",
      "logits: tensor([14264.6475])\n",
      "MAPE loss: 0.21250763535499573\n",
      "logits: tensor([13161.4697])\n",
      "MAPE loss: 0.2785411477088928\n",
      "logits: tensor([14263.2979])\n",
      "MAPE loss: 0.21814334392547607\n",
      "logits: tensor([13863.3486])\n",
      "MAPE loss: 0.24006696045398712\n",
      "logits: tensor([12712.3877])\n",
      "MAPE loss: 0.3031580150127411\n",
      "logits: tensor([14287.2373])\n",
      "MAPE loss: 0.21683108806610107\n",
      "logits: tensor([16439.3320])\n",
      "MAPE loss: 0.09886190295219421\n",
      "Valing, Epoch: 1, Batch 450: Loss = 3252696.75\n",
      "logits: tensor([11649.3486])\n",
      "MAPE loss: 0.8388686180114746\n",
      "logits: tensor([15375.2588])\n",
      "MAPE loss: 0.15719012916088104\n",
      "logits: tensor([8859.9531])\n",
      "MAPE loss: 0.39855802059173584\n",
      "logits: tensor([16769.2578])\n",
      "MAPE loss: 0.08077669888734818\n",
      "logits: tensor([10554.6514])\n",
      "MAPE loss: 0.6660689115524292\n",
      "logits: tensor([15743.4346])\n",
      "MAPE loss: 0.13086989521980286\n",
      "logits: tensor([16076.5010])\n",
      "MAPE loss: 0.11248267441987991\n",
      "logits: tensor([9511.0625])\n",
      "MAPE loss: 0.5013366937637329\n",
      "logits: tensor([15823.9639])\n",
      "MAPE loss: 0.13259391486644745\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = main(epochs=epochs, train_dl=train_dataloader, val_dl=val_dataloader, model=nn_model, optimizer=optimizer, criterion=loss_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Curve Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=0<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "0",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "0",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899,
          900,
          901,
          902,
          903,
          904,
          905,
          906,
          907,
          908,
          909,
          910,
          911,
          912,
          913,
          914,
          915,
          916,
          917,
          918,
          919,
          920,
          921,
          922,
          923,
          924,
          925,
          926,
          927,
          928,
          929,
          930,
          931,
          932,
          933,
          934,
          935,
          936,
          937,
          938,
          939,
          940,
          941,
          942,
          943,
          944,
          945,
          946,
          947,
          948,
          949,
          950,
          951,
          952,
          953,
          954,
          955,
          956,
          957,
          958,
          959,
          960,
          961,
          962,
          963,
          964,
          965,
          966,
          967,
          968,
          969,
          970,
          971,
          972,
          973,
          974,
          975,
          976,
          977,
          978,
          979,
          980,
          981,
          982,
          983,
          984,
          985,
          986,
          987,
          988,
          989,
          990,
          991,
          992,
          993,
          994,
          995,
          996,
          997,
          998,
          999,
          1000,
          1001,
          1002,
          1003,
          1004,
          1005,
          1006,
          1007,
          1008,
          1009,
          1010,
          1011,
          1012,
          1013,
          1014,
          1015,
          1016,
          1017,
          1018,
          1019,
          1020,
          1021,
          1022,
          1023,
          1024,
          1025,
          1026,
          1027,
          1028,
          1029,
          1030,
          1031,
          1032,
          1033,
          1034,
          1035,
          1036,
          1037,
          1038,
          1039,
          1040,
          1041,
          1042,
          1043,
          1044,
          1045,
          1046,
          1047,
          1048,
          1049,
          1050,
          1051,
          1052,
          1053,
          1054,
          1055,
          1056,
          1057,
          1058,
          1059,
          1060,
          1061,
          1062,
          1063,
          1064,
          1065,
          1066,
          1067,
          1068,
          1069,
          1070,
          1071,
          1072,
          1073,
          1074,
          1075,
          1076,
          1077,
          1078,
          1079,
          1080,
          1081,
          1082,
          1083,
          1084,
          1085,
          1086,
          1087,
          1088,
          1089,
          1090,
          1091,
          1092,
          1093,
          1094,
          1095,
          1096,
          1097,
          1098,
          1099,
          1100,
          1101,
          1102,
          1103,
          1104,
          1105,
          1106,
          1107,
          1108,
          1109,
          1110,
          1111,
          1112,
          1113,
          1114,
          1115,
          1116,
          1117,
          1118,
          1119,
          1120,
          1121,
          1122,
          1123,
          1124,
          1125,
          1126,
          1127,
          1128,
          1129,
          1130,
          1131,
          1132,
          1133,
          1134,
          1135,
          1136,
          1137,
          1138,
          1139,
          1140,
          1141,
          1142,
          1143,
          1144,
          1145,
          1146,
          1147,
          1148,
          1149,
          1150,
          1151,
          1152,
          1153,
          1154,
          1155,
          1156,
          1157,
          1158,
          1159,
          1160,
          1161,
          1162,
          1163,
          1164,
          1165,
          1166,
          1167,
          1168,
          1169,
          1170,
          1171,
          1172,
          1173,
          1174,
          1175,
          1176,
          1177,
          1178,
          1179,
          1180,
          1181,
          1182,
          1183,
          1184,
          1185,
          1186,
          1187,
          1188,
          1189,
          1190,
          1191,
          1192,
          1193,
          1194,
          1195,
          1196,
          1197,
          1198,
          1199,
          1200,
          1201,
          1202,
          1203,
          1204,
          1205,
          1206,
          1207,
          1208,
          1209,
          1210,
          1211,
          1212,
          1213,
          1214,
          1215,
          1216,
          1217,
          1218,
          1219,
          1220,
          1221,
          1222,
          1223,
          1224,
          1225,
          1226,
          1227,
          1228,
          1229,
          1230,
          1231,
          1232,
          1233,
          1234,
          1235,
          1236,
          1237,
          1238,
          1239,
          1240,
          1241,
          1242,
          1243,
          1244,
          1245,
          1246,
          1247,
          1248,
          1249,
          1250,
          1251,
          1252,
          1253,
          1254,
          1255,
          1256,
          1257,
          1258,
          1259,
          1260,
          1261,
          1262,
          1263,
          1264,
          1265,
          1266,
          1267,
          1268,
          1269,
          1270,
          1271,
          1272,
          1273,
          1274,
          1275,
          1276,
          1277,
          1278,
          1279,
          1280,
          1281,
          1282,
          1283,
          1284,
          1285,
          1286,
          1287,
          1288,
          1289,
          1290,
          1291,
          1292,
          1293,
          1294,
          1295,
          1296,
          1297,
          1298,
          1299,
          1300,
          1301,
          1302,
          1303,
          1304,
          1305,
          1306,
          1307,
          1308,
          1309,
          1310,
          1311,
          1312,
          1313,
          1314,
          1315,
          1316,
          1317,
          1318,
          1319,
          1320,
          1321,
          1322,
          1323,
          1324,
          1325,
          1326,
          1327,
          1328,
          1329,
          1330,
          1331,
          1332,
          1333,
          1334,
          1335,
          1336,
          1337,
          1338,
          1339,
          1340,
          1341,
          1342,
          1343,
          1344,
          1345,
          1346,
          1347,
          1348,
          1349,
          1350,
          1351,
          1352,
          1353,
          1354,
          1355,
          1356,
          1357,
          1358,
          1359,
          1360,
          1361,
          1362,
          1363,
          1364,
          1365,
          1366,
          1367,
          1368,
          1369,
          1370,
          1371,
          1372,
          1373,
          1374,
          1375,
          1376,
          1377,
          1378,
          1379,
          1380,
          1381,
          1382,
          1383,
          1384,
          1385,
          1386,
          1387,
          1388,
          1389,
          1390,
          1391,
          1392,
          1393,
          1394,
          1395,
          1396,
          1397,
          1398,
          1399,
          1400,
          1401,
          1402,
          1403,
          1404,
          1405,
          1406,
          1407,
          1408,
          1409,
          1410,
          1411,
          1412,
          1413,
          1414,
          1415,
          1416,
          1417,
          1418,
          1419,
          1420,
          1421,
          1422,
          1423,
          1424,
          1425,
          1426,
          1427,
          1428,
          1429,
          1430,
          1431,
          1432,
          1433,
          1434,
          1435,
          1436,
          1437,
          1438,
          1439,
          1440,
          1441,
          1442,
          1443,
          1444,
          1445,
          1446,
          1447,
          1448,
          1449,
          1450,
          1451,
          1452,
          1453,
          1454,
          1455,
          1456,
          1457,
          1458,
          1459,
          1460,
          1461,
          1462,
          1463,
          1464,
          1465,
          1466,
          1467,
          1468,
          1469,
          1470,
          1471,
          1472,
          1473,
          1474,
          1475,
          1476,
          1477,
          1478,
          1479,
          1480,
          1481,
          1482,
          1483,
          1484,
          1485,
          1486,
          1487,
          1488,
          1489,
          1490,
          1491,
          1492,
          1493,
          1494,
          1495,
          1496,
          1497,
          1498,
          1499,
          1500,
          1501,
          1502,
          1503,
          1504,
          1505,
          1506,
          1507,
          1508,
          1509,
          1510,
          1511,
          1512,
          1513,
          1514,
          1515,
          1516,
          1517,
          1518,
          1519,
          1520,
          1521,
          1522,
          1523,
          1524,
          1525,
          1526,
          1527,
          1528,
          1529,
          1530,
          1531,
          1532,
          1533,
          1534,
          1535,
          1536,
          1537,
          1538,
          1539,
          1540,
          1541,
          1542,
          1543,
          1544,
          1545,
          1546,
          1547,
          1548,
          1549,
          1550,
          1551,
          1552,
          1553,
          1554,
          1555,
          1556,
          1557,
          1558,
          1559,
          1560,
          1561,
          1562,
          1563,
          1564,
          1565,
          1566,
          1567,
          1568,
          1569,
          1570,
          1571,
          1572,
          1573,
          1574,
          1575,
          1576,
          1577,
          1578,
          1579,
          1580,
          1581,
          1582,
          1583,
          1584,
          1585,
          1586,
          1587,
          1588,
          1589,
          1590,
          1591,
          1592,
          1593,
          1594,
          1595,
          1596,
          1597,
          1598,
          1599,
          1600,
          1601,
          1602,
          1603,
          1604,
          1605,
          1606,
          1607,
          1608,
          1609,
          1610,
          1611,
          1612,
          1613,
          1614,
          1615,
          1616,
          1617,
          1618,
          1619,
          1620,
          1621,
          1622,
          1623,
          1624,
          1625,
          1626,
          1627,
          1628,
          1629,
          1630,
          1631,
          1632,
          1633,
          1634,
          1635,
          1636,
          1637,
          1638,
          1639,
          1640,
          1641,
          1642,
          1643,
          1644,
          1645,
          1646,
          1647,
          1648,
          1649,
          1650,
          1651,
          1652,
          1653,
          1654,
          1655,
          1656,
          1657,
          1658,
          1659,
          1660,
          1661,
          1662,
          1663,
          1664,
          1665,
          1666,
          1667,
          1668,
          1669,
          1670,
          1671,
          1672,
          1673,
          1674,
          1675,
          1676,
          1677,
          1678,
          1679,
          1680,
          1681,
          1682,
          1683,
          1684,
          1685,
          1686,
          1687,
          1688,
          1689,
          1690,
          1691,
          1692,
          1693,
          1694,
          1695,
          1696,
          1697,
          1698,
          1699,
          1700,
          1701,
          1702,
          1703,
          1704,
          1705,
          1706,
          1707,
          1708,
          1709,
          1710,
          1711,
          1712,
          1713,
          1714,
          1715,
          1716,
          1717,
          1718,
          1719,
          1720,
          1721,
          1722,
          1723,
          1724,
          1725,
          1726,
          1727,
          1728,
          1729,
          1730,
          1731,
          1732,
          1733,
          1734,
          1735,
          1736,
          1737,
          1738,
          1739,
          1740,
          1741,
          1742,
          1743,
          1744,
          1745,
          1746,
          1747,
          1748,
          1749,
          1750,
          1751,
          1752,
          1753,
          1754,
          1755,
          1756,
          1757,
          1758,
          1759,
          1760,
          1761,
          1762,
          1763,
          1764,
          1765,
          1766,
          1767,
          1768,
          1769,
          1770,
          1771,
          1772,
          1773,
          1774,
          1775,
          1776,
          1777,
          1778,
          1779,
          1780,
          1781,
          1782,
          1783,
          1784,
          1785,
          1786,
          1787,
          1788,
          1789,
          1790,
          1791,
          1792,
          1793,
          1794,
          1795,
          1796,
          1797,
          1798,
          1799,
          1800,
          1801,
          1802,
          1803,
          1804,
          1805,
          1806,
          1807,
          1808,
          1809,
          1810,
          1811,
          1812,
          1813,
          1814,
          1815,
          1816,
          1817,
          1818,
          1819,
          1820,
          1821,
          1822,
          1823,
          1824,
          1825,
          1826,
          1827,
          1828,
          1829,
          1830,
          1831,
          1832,
          1833,
          1834,
          1835,
          1836,
          1837,
          1838,
          1839,
          1840,
          1841,
          1842
         ],
         "xaxis": "x",
         "y": [
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0.9999980926513672,
          0.9959495067596436,
          0.995998203754425,
          0.9882700443267822,
          0.9687219262123108,
          0.9571363925933838,
          0.8990718722343445,
          0.9589588642120361,
          0.8178107142448425,
          0.6824737191200256,
          0.5749967098236084,
          0.31330057978630066,
          0.6883383989334106,
          0.5141180753707886,
          0.29037222266197205,
          0.1724323332309723,
          0.09134173393249512,
          1.4354146718978882,
          0.4184063673019409,
          1.4579787254333496,
          0.7492532730102539,
          0.40117889642715454,
          0.6103396415710449,
          0.6597050428390503,
          0.3172221779823303,
          0.008655711077153683,
          0.1986829787492752,
          0.7041105031967163,
          0.7953142523765564,
          0.42931801080703735,
          0.7959065437316895,
          0.6454038619995117,
          0.4639735221862793,
          0.6813997030258179,
          0.39128533005714417,
          0.8721253275871277,
          0.3763536810874939,
          0.6014694571495056,
          0.47429704666137695,
          0.5404453873634338,
          0.6753824949264526,
          0.6827726364135742,
          0.5729360580444336,
          0.7664186954498291,
          0.6291307806968689,
          0.7192888259887695,
          0.8290761113166809,
          0.5564042329788208,
          0.6523182392120361,
          0.7281613349914551,
          0.5175566077232361,
          0.276323139667511,
          0.4762223958969116,
          0.2256925106048584,
          0.11132447421550751,
          0.4558753967285156,
          1.431932806968689,
          1.611569881439209,
          0.5629307627677917,
          1.540830373764038,
          0.5319552421569824,
          0.740668773651123,
          1.6887050867080688,
          0.012478525750339031,
          0.98356693983078,
          0.41611790657043457,
          0.6826203465461731,
          0.3536430895328522,
          0.26272913813591003,
          0.48524823784828186,
          0.0630868524312973,
          0.40607109665870667,
          0.5037765502929688,
          0.599260151386261,
          0.08482667058706284,
          0.25158724188804626,
          0.10508459061384201,
          0.17358353734016418,
          0.06651187688112259,
          0.10716409236192703,
          0.5690475106239319,
          0.18244513869285583,
          0.06382271647453308,
          0.3794248104095459,
          0.4592756927013397,
          0.8374348878860474,
          0.4130665361881256,
          0.41314226388931274,
          0.1798219084739685,
          0.21483688056468964,
          0.6855130195617676,
          0.5726779699325562,
          0.3924623429775238,
          0.22376856207847595,
          0.6307779550552368,
          0.4058266878128052,
          0.3571530282497406,
          0.1165238618850708,
          0.13180197775363922,
          0.45629316568374634,
          0.7403337955474854,
          0.21392525732517242,
          1.5081803798675537,
          0.1566266119480133,
          0.05723034590482712,
          0.1113613024353981,
          0.16307125985622406,
          0.24206800758838654,
          0.5714796781539917,
          0.16076302528381348,
          0.466741144657135,
          0.4362477660179138,
          0.515506386756897,
          0.38964489102363586,
          0.5487061738967896,
          0.18817347288131714,
          0.23413066565990448,
          0.29124271869659424,
          0.3391405940055847,
          0.5649381875991821,
          0.23180446028709412,
          0.47078269720077515,
          0.017540697008371353,
          0.4055916965007782,
          0.1480475813150406,
          0.41061195731163025,
          0.27082785964012146,
          0.07195794582366943,
          1.0357112884521484,
          0.2869359850883484,
          0.31865283846855164,
          0.048009056597948074,
          0.3728293478488922,
          0.21721307933330536,
          0.028319304808974266,
          0.17710751295089722,
          0.14288534224033356,
          0.021024057641625404,
          0.11903691291809082,
          1.0798641443252563,
          0.09742940962314606,
          0.053019098937511444,
          0.10900647938251495,
          0.09487147629261017,
          0.7876511812210083,
          0.10762067884206772,
          0.6576665043830872,
          0.701732873916626,
          0.03274970129132271,
          0.8203733563423157,
          0.8834065198898315,
          0.30130764842033386,
          0.5208625197410583,
          0.33651772141456604,
          0.31657877564430237,
          0.19720838963985443,
          0.5531362295150757,
          0.4406850337982178,
          0.21826742589473724,
          0.3509606420993805,
          1.0852497816085815,
          0.23365294933319092,
          0.7679125070571899,
          0.7629110217094421,
          0.1017422080039978,
          0.2344544231891632,
          0.09089291840791702,
          1.2175025939941406,
          0.03811103105545044,
          0.008173576556146145,
          0.0331343337893486,
          0.048663314431905746,
          0.00043852010276168585,
          0.25170278549194336,
          0.17631806433200836,
          0.041196953505277634,
          0.04482210427522659,
          0.004752502776682377,
          0.35827547311782837,
          0.09772178530693054,
          1.123837947845459,
          0.09402664005756378,
          0.19274835288524628,
          0.17264419794082642,
          0.09364889562129974,
          0.03586860001087189,
          0.0249194148927927,
          0.7429245710372925,
          0.21830753982067108,
          0.031913284212350845,
          0.09267058968544006,
          0.12039312720298767,
          1.2279466390609741,
          1.134667158126831,
          0.19253043830394745,
          0.02675705961883068,
          0.1695006936788559,
          0.21770857274532318,
          0.08545447140932083,
          1.0116106271743774,
          0.21516819298267365,
          0.07685799896717072,
          0.8016572594642639,
          0.724222719669342,
          0.08769474923610687,
          0.19067716598510742,
          0.1824621558189392,
          0.13659311830997467,
          0.22663527727127075,
          1.1724098920822144,
          1.0151842832565308,
          0.8963614702224731,
          0.07096122205257416,
          0.17198064923286438,
          0.21710962057113647,
          0.19679848849773407,
          0.7570735812187195,
          0.15697471797466278,
          0.19396843016147614,
          0.08061017841100693,
          0.9392257332801819,
          1.1334959268569946,
          0.0025813060346990824,
          0.2950649559497833,
          0.017542961984872818,
          0.0019107486587017775,
          0.18652302026748657,
          0.19619351625442505,
          0.017321206629276276,
          0.053473684936761856,
          1.0782840251922607,
          0.034801509231328964,
          0.00815409142524004,
          0.0031155175529420376,
          0.0066991038620471954,
          0.7981045246124268,
          1.0251891613006592,
          0.015300869010388851,
          0.8268088698387146,
          0.7145583033561707,
          0.20470112562179565,
          0.20922473073005676,
          0.23797538876533508,
          0.22271041572093964,
          0.7712717652320862,
          0.6857587099075317,
          0.19405771791934967,
          0.1621669977903366,
          0.8766315579414368,
          0.7599745988845825,
          0.10471393913030624,
          0.1473589390516281,
          0.12335386127233505,
          0.011890357360243797,
          0.065118707716465,
          0.19911476969718933,
          0.840680718421936,
          1.0474196672439575,
          0.16292867064476013,
          0.8029760718345642,
          0.15936167538166046,
          0.1754247397184372,
          0.0703129842877388,
          0.23702308535575867,
          0.9995768666267395,
          0.03211306408047676,
          0.25771331787109375,
          0.04713255539536476,
          0.0431966558098793,
          0.17693237960338593,
          0.29035240411758423,
          0.23404903709888458,
          0.6518005728721619,
          0.1695796549320221,
          0.032931167632341385,
          1.0796105861663818,
          0.06582071632146835,
          1.0223792791366577,
          0.048683978617191315,
          0.7916808724403381,
          0.12111997604370117,
          0.6415676474571228,
          0.07056626677513123,
          0.6353577971458435,
          0.23770928382873535,
          0.255258172750473,
          0.10166453570127487,
          0.06083094701170921,
          0.062081772834062576,
          0.10215311497449875,
          0.19647465646266937,
          0.7312673330307007,
          0.08935976773500443,
          0.04855809733271599,
          0.07949826866388321,
          0.026928136125206947,
          0.10143932700157166,
          0.8290857076644897,
          0.1251012086868286,
          0.1823342740535736,
          0.7523046135902405,
          0.25340336561203003,
          0.03164612129330635,
          0.15239953994750977,
          0.03889426589012146,
          0.8427509665489197,
          0.9310051202774048,
          0.8690045475959778,
          0.056332096457481384,
          0.1938609927892685,
          0.5335689187049866,
          1.118828535079956,
          0.6375423073768616,
          0.451144814491272,
          0.2889082133769989,
          0.2954142391681671,
          0.3042561411857605,
          0.39238467812538147,
          0.6716550588607788,
          0.20734187960624695,
          0.18593230843544006,
          0.7135193347930908,
          0.5814241766929626,
          0.4800598621368408,
          0.06507502496242523,
          0.06752365082502365,
          0.7646846771240234,
          0.3559871017932892,
          0.5576416254043579,
          0.5762004256248474,
          0.6594963073730469,
          0.09031012654304504,
          0.08954188227653503,
          0.33126863837242126,
          0.7082669138908386,
          0.3580593764781952,
          0.12269283086061478,
          0.6257997751235962,
          0.21091501414775848,
          0.5955921411514282,
          0.0980469286441803,
          0.06441006064414978,
          0.2386559247970581,
          0.09684146195650101,
          0.17570720613002777,
          0.12089946866035461,
          0.7346071600914001,
          0.9591757655143738,
          0.16817942261695862,
          1.0571095943450928,
          0.5526207685470581,
          0.4280295670032501,
          0.3384212255477905,
          0.0025896085426211357,
          0.07057557255029678,
          0.4883105456829071,
          0.04570680111646652,
          0.23804841935634613,
          0.058986399322748184,
          0.04770553484559059,
          0.053902629762887955,
          0.2410161942243576,
          0.023899339139461517,
          0.09479673951864243,
          0.3798517882823944,
          0.03767099604010582,
          0.01170374732464552,
          0.005851498804986477,
          0.14446671307086945,
          0.081962950527668,
          1.2325000762939453,
          0.0028901512268930674,
          0.03983330354094505,
          0.515255868434906,
          0.07262720912694931,
          0.7296263575553894,
          0.2817762494087219,
          0.6371276378631592,
          0.2976006269454956,
          0.337764173746109,
          0.6592564582824707,
          0.6604741215705872,
          0.01577661745250225,
          0.04345858097076416,
          0.18053038418293,
          0.24316178262233734,
          0.8345266580581665,
          0.08290477842092514,
          0.13819508254528046,
          0.11864865571260452,
          0.027965549379587173,
          1.0019768476486206,
          0.666621208190918,
          0.2057916522026062,
          0.12730494141578674,
          0.2700154483318329,
          0.13970771431922913,
          0.3306591510772705,
          0.25187069177627563,
          0.6005555391311646,
          0.573756992816925,
          0.13360200822353363,
          0.5306568741798401,
          0.274504154920578,
          0.20729979872703552,
          0.2541283965110779,
          0.08970415592193604,
          0.07523040473461151,
          0.011593044735491276,
          1.0576364994049072,
          0.2393411248922348,
          0.2557557225227356,
          0.0577721931040287,
          0.2082924246788025,
          0.009972654283046722,
          0.7426232099533081,
          0.11429019272327423,
          0.15629513561725616,
          0.48169511556625366,
          0.15781165659427643,
          0.2682228684425354,
          0.2455201894044876,
          0.22569377720355988,
          0.19219419360160828,
          0.22701767086982727,
          0.05806127190589905,
          0.02098836936056614,
          0.21291875839233398,
          0.04308980330824852,
          0.10443760454654694,
          0.25675344467163086,
          0.3371351659297943,
          0.6711438894271851,
          0.780327558517456,
          0.6226648092269897,
          0.6327990293502808,
          0.17465735971927643,
          0.3990223705768585,
          0.29786425828933716,
          0.24509479105472565,
          0.40730226039886475,
          0.28835129737854004,
          0.31970444321632385,
          0.1659824550151825,
          0.2917052209377289,
          0.137528195977211,
          0.03881988301873207,
          0.8001617193222046,
          0.9635415077209473,
          0.153132826089859,
          0.05298632010817528,
          1.3265807628631592,
          0.31735432147979736,
          0.8376113772392273,
          0.14216472208499908,
          0.022807467728853226,
          0.016259146854281425,
          0.04261835291981697,
          0.8075008392333984,
          0.8419156074523926,
          0.15420414507389069,
          0.5340118408203125,
          0.27877485752105713,
          0.28284260630607605,
          0.3167014718055725,
          0.3862975239753723,
          0.3831481337547302,
          0.21019037067890167,
          0.2338685840368271,
          0.5588244199752808,
          0.011343481950461864,
          0.06917209923267365,
          0.24964243173599243,
          0.05831917002797127,
          0.17834630608558655,
          0.11394486576318741,
          0.08571891486644745,
          0.17218680679798126,
          0.023320773616433144,
          0.3015729486942291,
          0.18723393976688385,
          0.14936155080795288,
          0.5978842377662659,
          0.20159898698329926,
          0.17534957826137543,
          0.24277707934379578,
          0.0464891716837883,
          0.9865997433662415,
          0.3132554888725281,
          0.835465133190155,
          0.991323709487915,
          0.3280368149280548,
          0.14261473715305328,
          0.6158342957496643,
          0.3792257606983185,
          0.24153350293636322,
          0.18847733736038208,
          0.4110121428966522,
          0.5694034099578857,
          0.18507646024227142,
          0.5723808407783508,
          0.09399634599685669,
          0.20088188350200653,
          0.17613273859024048,
          0.8778368830680847,
          0.011061122640967369,
          0.008211981505155563,
          0.6196691393852234,
          0.12294316291809082,
          0.13637027144432068,
          0.6760186553001404,
          0.10051901638507843,
          0.08836143463850021,
          0.22852760553359985,
          0.10313934832811356,
          0.44749724864959717,
          0.22301897406578064,
          0.10972879827022552,
          0.6925389170646667,
          0.06040288507938385,
          0.7664642333984375,
          0.4684554636478424,
          0.12052652984857559,
          0.3084184527397156,
          0.7734545469284058,
          0.19990818202495575,
          0.12224182486534119,
          0.2643905282020569,
          0.036580901592969894,
          0.010092992335557938,
          0.7439142465591431,
          0.10214963555335999,
          0.14054252207279205,
          0.04000196233391762,
          0.032917581498622894,
          0.8532448410987854,
          0.002024979330599308,
          0.21130916476249695,
          0.034988440573215485,
          0.028747448697686195,
          0.05817827582359314,
          0.9891570806503296,
          1.0141016244888306,
          0.08078622817993164,
          0.585713803768158,
          0.09588931500911713,
          0.5948130488395691,
          0.02006097510457039,
          0.2072257697582245,
          0.09919016063213348,
          0.18377533555030823,
          0.6550503969192505,
          0.7309881448745728,
          0.3553849458694458,
          0.6119757294654846,
          0.23763953149318695,
          0.13246066868305206,
          0.030195781961083412,
          0.732119619846344,
          0.0010502055520191789,
          0.051125429570674896,
          0.2798636257648468,
          0.8282862305641174,
          0.18204331398010254,
          0.05606760457158089,
          0.1240038126707077,
          0.9111230373382568,
          0.2787002921104431,
          0.13567297160625458,
          0.7293914556503296,
          0.48759743571281433,
          0.5059120655059814,
          0.15919342637062073,
          0.2641078233718872,
          0.29309797286987305,
          0.35013991594314575,
          0.1712852120399475,
          0.4588058590888977,
          0.20271500945091248,
          0.22319208085536957,
          0.13082042336463928,
          0.038068655878305435,
          0.18371054530143738,
          0.8533415198326111,
          0.11096186935901642,
          0.623550534248352,
          0.8339968323707581,
          1.2548846006393433,
          0.288998544216156,
          0.08319008350372314,
          0.13276879489421844,
          0.10155676305294037,
          0.34830111265182495,
          0.07691386342048645,
          0.028386326506733894,
          0.05776812136173248,
          0.2924756407737732,
          0.16645552217960358,
          0.539566695690155,
          0.3314798176288605,
          0.1709761619567871,
          0.1043376699090004,
          0.36130163073539734,
          0.25367096066474915,
          0.7233545184135437,
          0.17370076477527618,
          0.22758013010025024,
          0.2689511179924011,
          0.2258489578962326,
          0.1944153606891632,
          0.7488791942596436,
          0.3053763806819916,
          0.250927209854126,
          0.4757113456726074,
          1.0283634662628174,
          0.27347859740257263,
          0.5062769651412964,
          0.5896751880645752,
          0.4187679588794708,
          0.4386385977268219,
          0.17120465636253357,
          0.5318048000335693,
          0.33652764558792114,
          0.2495487779378891,
          0.4569970369338989,
          0.34132149815559387,
          0.43916958570480347,
          0.39521047472953796,
          0.4545850157737732,
          0.3826017379760742,
          0.44276896119117737,
          0.283373087644577,
          0.2754233181476593,
          0.743312418460846,
          0.19950929284095764,
          0.7217593789100647,
          0.14583748579025269,
          0.03789527714252472,
          0.026190444827079773,
          0.01805555634200573,
          0.8899060487747192,
          0.009221969172358513,
          0.8102777600288391,
          0.0029039152432233095,
          0.010937879793345928,
          0.9488371014595032,
          0.07762670516967773,
          0.7959593534469604,
          0.05540437996387482,
          0.9721390604972839,
          0.15736715495586395,
          0.9795740842819214,
          0.7962244749069214,
          0.3350363075733185,
          0.23236584663391113,
          0.3054921627044678,
          0.34376269578933716,
          0.261193186044693,
          0.6253960728645325,
          0.22473116219043732,
          0.41036683320999146,
          0.37807175517082214,
          0.2093670666217804,
          0.29495203495025635,
          0.22910378873348236,
          0.543772280216217,
          0.3174619972705841,
          0.12670521438121796,
          0.002533971332013607,
          0.0639781728386879,
          0.1077049970626831,
          1.0734423398971558,
          0.24704962968826294,
          0.9026177525520325,
          0.9140462279319763,
          0.03134595975279808,
          0.7163019180297852,
          0.5919954776763916,
          0.14497655630111694,
          0.19731129705905914,
          0.19975604116916656,
          0.2282078117132187,
          1.3453333377838135,
          0.21579496562480927,
          0.3220595419406891,
          0.13220885396003723,
          0.32039493322372437,
          0.6646725535392761,
          0.3996403217315674,
          0.4823901653289795,
          0.4015149474143982,
          0.25150325894355774,
          0.13995987176895142,
          0.024017035961151123,
          0.0336247980594635,
          0.2945430278778076,
          0.16738717257976532,
          0.1517440229654312,
          0.6617228984832764,
          0.01991724595427513,
          1.0451611280441284,
          0.8568317890167236,
          0.218733549118042,
          0.17763353884220123,
          0.5652983784675598,
          0.7597984075546265,
          0.24509219825267792,
          0.17116867005825043,
          0.2267174869775772,
          0.3994024693965912,
          0.012946762144565582,
          0.29690074920654297,
          0.2930079400539398,
          0.3182367980480194,
          0.24136734008789062,
          0.20268824696540833,
          0.20143157243728638,
          0.8579631447792053,
          0.0047120158560574055,
          0.13673578202724457,
          0.6904844045639038,
          0.7508642077445984,
          0.18132208287715912,
          0.04892006143927574,
          0.03584187477827072,
          0.05402326211333275,
          0.006475153379142284,
          0.05235058814287186,
          0.03407423570752144,
          0.17785899341106415,
          0.8743635416030884,
          0.08313239365816116,
          0.027617493644356728,
          0.9159854650497437,
          0.13896392285823822,
          0.559279203414917,
          0.11705613881349564,
          0.46579432487487793,
          0.2399977296590805,
          0.21678005158901215,
          0.2901661992073059,
          0.25810056924819946,
          0.2523331642150879,
          0.6578670740127563,
          0.4786592423915863,
          0.20210696756839752,
          0.5902025103569031,
          0.13835886120796204,
          0.20740841329097748,
          0.4920083284378052,
          1.1181532144546509,
          0.2564089298248291,
          0.2504911720752716,
          0.03374049440026283,
          0.534963071346283,
          0.1572098731994629,
          0.5697134137153625,
          0.2915656864643097,
          0.004769693594425917,
          0.31544578075408936,
          0.2837491035461426,
          0.2978735566139221,
          0.18173247575759888,
          0.43532639741897583,
          0.10414308309555054,
          0.009942772798240185,
          0.952263593673706,
          0.7165393233299255,
          0.6793308854103088,
          0.35393673181533813,
          0.22924675047397614,
          0.012732510454952717,
          0.01574823074042797,
          0.04505155235528946,
          0.08358310163021088,
          0.2419092357158661,
          0.6982511878013611,
          0.054401326924562454,
          0.020688941702246666,
          0.22429344058036804,
          0.15256239473819733,
          0.23810669779777527,
          0.017849024385213852,
          0.25252461433410645,
          0.011966906487941742,
          0.12045265734195709,
          0.8343738913536072,
          0.2603759169578552,
          0.03406791761517525,
          0.06007067859172821,
          0.01851499453186989,
          0.15018817782402039,
          0.9667823910713196,
          0.5299224853515625,
          0.865648090839386,
          0.14182665944099426,
          0.12734119594097137,
          0.13458822667598724,
          0.11756951361894608,
          0.6415097117424011,
          0.19277988374233246,
          0.1341944932937622,
          0.3099607229232788,
          0.05043908581137657,
          0.06625239551067352,
          1.1569112539291382,
          0.09535984694957733,
          0.6266617774963379,
          0.06209011375904083,
          0.012493995949625969,
          0.5499275326728821,
          0.13447785377502441,
          0.9861064553260803,
          0.28539976477622986,
          0.35160908102989197,
          0.679683268070221,
          0.7360788583755493,
          0.602406919002533,
          0.043570030480623245,
          0.09389784932136536,
          0.4646274149417877,
          0.11952083557844162,
          0.3685723841190338,
          0.6781254410743713,
          0.1060040146112442,
          0.16319994628429413,
          0.39014947414398193,
          0.07028043270111084,
          0.05027228221297264,
          0.30093514919281006,
          0.6732214093208313,
          0.15199995040893555,
          0.10545796900987625,
          0.16353511810302734,
          0.26659220457077026,
          0.7051487565040588,
          0.2892754375934601,
          0.3175418972969055,
          0.15720510482788086,
          1.1165066957473755,
          0.31302547454833984,
          0.20191365480422974,
          0.00526843685656786,
          0.05024176836013794,
          0.1215343102812767,
          0.3108733594417572,
          0.6441465020179749,
          0.04629006236791611,
          0.12031010538339615,
          0.5629860758781433,
          0.6372162699699402,
          0.6583964228630066,
          0.29429641366004944,
          0.0925595685839653,
          0.3934643864631653,
          0.0015247835544869304,
          0.14341041445732117,
          0.01666279323399067,
          0.04951201379299164,
          0.050754040479660034,
          0.0038048927672207355,
          0.0015357369557023048,
          0.03395506367087364,
          0.20586548745632172,
          0.5970548987388611,
          0.5598406195640564,
          0.9431166648864746,
          0.1958274096250534,
          0.06741851568222046,
          0.14376629889011383,
          0.028347140178084373,
          0.12193161994218826,
          0.08911748975515366,
          0.16477297246456146,
          0.021602528169751167,
          0.001356802531518042,
          1.0796780586242676,
          0.6890944242477417,
          0.2611924111843109,
          0.17298221588134766,
          0.07740209251642227,
          0.24296455085277557,
          0.22101828455924988,
          0.6411542296409607,
          0.048669371753931046,
          0.3639965355396271,
          0.9825250506401062,
          0.5938409566879272,
          0.07778280228376389,
          0.5163757801055908,
          0.06890624016523361,
          0.029117217287421227,
          0.7460135221481323,
          0.720578134059906,
          0.09507400542497635,
          0.02783045917749405,
          0.2958163619041443,
          0.1473313271999359,
          0.6828467845916748,
          0.6740323901176453,
          0.9059827327728271,
          0.30223214626312256,
          0.21455766260623932,
          0.9585104584693909,
          0.049058910459280014,
          0.5789238214492798,
          0.13029035925865173,
          0.18099486827850342,
          0.18438200652599335,
          0.07338478416204453,
          0.8079588413238525,
          0.17019493877887726,
          0.007295478135347366,
          0.001251024892553687,
          0.7265142202377319,
          0.0520239919424057,
          0.3445029556751251,
          0.06594223529100418,
          0.15008100867271423,
          0.14892905950546265,
          0.061299871653318405,
          0.9253913760185242,
          0.12159544229507446,
          0.0021685557439923286,
          0.21844077110290527,
          1.1269522905349731,
          0.08960293233394623,
          0.34462857246398926,
          0.4357874095439911,
          0.34438592195510864,
          0.038367435336112976,
          0.12808963656425476,
          0.07986295223236084,
          0.06813304871320724,
          0.37019282579421997,
          0.05652359500527382,
          0.38779619336128235,
          0.03915627673268318,
          0.05872654542326927,
          0.5343281626701355,
          0.10401235520839691,
          0.19511094689369202,
          0.12502998113632202,
          0.804163932800293,
          0.05089917033910751,
          0.5463695526123047,
          0.10940252244472504,
          0.14274007081985474,
          0.1591421365737915,
          0.8238866329193115,
          0.6153528690338135,
          0.09308288991451263,
          0.315684050321579,
          0.16683055460453033,
          0.13129320740699768,
          0.005928162485361099,
          0.41280969977378845,
          0.4729127585887909,
          0.14706015586853027,
          0.07955415546894073,
          0.019980106502771378,
          0.09132277965545654,
          0.4249765872955322,
          0.08341086655855179,
          0.21096310019493103,
          0.009159107692539692,
          0.10451717674732208,
          0.1414441466331482,
          0.2058262974023819,
          0.11435536295175552,
          1.1961122751235962,
          0.017193589359521866,
          0.08198992908000946,
          0.18407677114009857,
          0.047257199883461,
          0.24363118410110474,
          0.07948686927556992,
          0.020430702716112137,
          0.6983646750450134,
          0.46441373229026794,
          0.177943155169487,
          0.27709048986434937,
          0.4486686587333679,
          0.5075920224189758,
          0.32587501406669617,
          0.6381773948669434,
          0.36595237255096436,
          0.21494325995445251,
          0.26641348004341125,
          0.4115246534347534,
          0.1659962236881256,
          0.10612224042415619,
          0.0885961577296257,
          0.23929369449615479,
          0.11847221106290817,
          1.1098062992095947,
          0.20650817453861237,
          0.06390922516584396,
          1.0949913263320923,
          0.2386358082294464,
          0.02828247658908367,
          1.0384165048599243,
          0.08384327590465546,
          0.18975771963596344,
          0.10391147434711456,
          0.19144880771636963,
          0.28871411085128784,
          0.17795954644680023,
          0.1628723293542862,
          0.09569054841995239,
          0.008507096208631992,
          0.22598059475421906,
          0.31626367568969727,
          0.055171482264995575,
          0.029963744804263115,
          1.234931468963623,
          1.4964978694915771,
          0.17444688081741333,
          0.01524473074823618,
          0.1090262159705162,
          0.03861748054623604,
          0.05185244232416153,
          0.031171342357993126,
          0.07344131171703339,
          0.07678576558828354,
          0.049492619931697845,
          0.054980214685201645,
          0.9944836497306824,
          1.8926246166229248,
          0.62649005651474,
          0.18838654458522797,
          0.24143384397029877,
          0.1689794659614563,
          0.1514146775007248,
          0.16607598960399628,
          0.28706350922584534,
          0.3107492923736572,
          0.2690642774105072,
          0.5256274938583374,
          0.042024072259664536,
          0.3878307342529297,
          0.35756975412368774,
          0.3400556445121765,
          0.30548691749572754,
          0.021315600723028183,
          0.058671340346336365,
          0.37606093287467957,
          0.8822898864746094,
          0.9711802005767822,
          0.39962953329086304,
          0.2732570767402649,
          0.20278358459472656,
          0.14855965971946716,
          0.06570166349411011,
          0.00013115150795783848,
          0.4663008749485016,
          0.3740731179714203,
          0.5259240865707397,
          0.493988573551178,
          0.4739338755607605,
          0.41954997181892395,
          0.0008576773689128458,
          0.08556582033634186,
          0.48481833934783936,
          0.5877096056938171,
          0.041007183492183685,
          0.16534388065338135,
          0.41538941860198975,
          0.36803731322288513,
          0.3248218894004822,
          0.3008863031864166,
          0.25827741622924805,
          0.015121073462069035,
          0.024490751326084137,
          0.20497219264507294,
          0.1928764432668686,
          0.07039741426706314,
          0.14249195158481598,
          0.1745840609073639,
          0.16598674654960632,
          0.17308443784713745,
          0.06423893570899963,
          1.0745877027511597,
          0.1390000432729721,
          0.1699141263961792,
          0.7988857626914978,
          0.057656027376651764,
          0.805493175983429,
          0.2488866150379181,
          0.29366883635520935,
          0.15716330707073212,
          0.6103699207305908,
          0.27726438641548157,
          0.21839317679405212,
          0.32327666878700256,
          0.15889610350131989,
          0.16526328027248383,
          0.3559901714324951,
          0.0749242976307869,
          0.04753711074590683,
          0.1590573936700821,
          0.10553184151649475,
          0.05454917997121811,
          0.16756495833396912,
          0.057399339973926544,
          0.07398144155740738,
          0.07714428007602692,
          0.2786199450492859,
          0.04285089299082756,
          0.1471557915210724,
          1.015918493270874,
          0.12411370873451233,
          0.5489974021911621,
          0.1223478764295578,
          0.7118943929672241,
          0.478849321603775,
          0.3044520914554596,
          0.636779248714447,
          0.43531185388565063,
          0.2472093105316162,
          0.708311915397644,
          0.4472355246543884,
          0.31425076723098755,
          0.2583584487438202,
          0.24963192641735077,
          0.34071066975593567,
          0.1702674776315689,
          0.09662740677595139,
          0.013580123893916607,
          0.10559559613466263,
          0.7950740456581116,
          0.4545226991176605,
          0.6298590302467346,
          0.05604582652449608,
          0.7346600294113159,
          0.1078125461935997,
          0.36368730664253235,
          0.17060653865337372,
          0.4776504635810852,
          0.265762597322464,
          0.5083538293838501,
          0.2533233165740967,
          0.11555235087871552,
          0.1639682948589325,
          0.1784767061471939,
          0.082845039665699,
          0.3113870620727539,
          0.04854537546634674,
          0.07309143245220184,
          0.836095929145813,
          1.078353762626648,
          0.10811659693717957,
          0.05461715906858444,
          0.1744527667760849,
          0.21954180300235748,
          0.15801477432250977,
          0.0801095962524414,
          0.08665187656879425,
          0.39640432596206665,
          0.026219917461276054,
          0.4272555708885193,
          0.050597693771123886,
          0.8536881804466248,
          0.5891577005386353,
          0.1257634311914444,
          0.022086048498749733,
          0.22061842679977417,
          0.2076481282711029,
          0.24785982072353363,
          0.2876395881175995,
          0.6929485201835632,
          0.21137337386608124,
          0.08254799991846085,
          0.04657683148980141,
          0.36834144592285156,
          0.026359835639595985,
          0.151997372508049,
          0.1356344372034073,
          0.7453398704528809,
          0.08511754125356674,
          0.5986241698265076,
          0.20790202915668488,
          0.2025633007287979,
          0.09630352258682251,
          0.4453207850456238,
          0.09597218036651611,
          1.0802721977233887,
          0.06205517798662186,
          0.7237504124641418,
          0.8618267774581909,
          0.43321162462234497,
          0.2791348099708557,
          0.2706667482852936,
          0.20701982080936432,
          0.05051474645733833,
          0.6313040852546692,
          0.5597807765007019,
          0.2580052316188812,
          0.25609317421913147,
          0.3008258640766144,
          0.3308529257774353,
          0.2140311598777771,
          0.1004478931427002,
          0.23050646483898163,
          0.36392179131507874,
          0.7712518572807312,
          0.16907204687595367,
          0.04857480898499489,
          0.1898755431175232,
          0.14231978356838226,
          0.24657166004180908,
          0.5264875292778015,
          0.20954975485801697,
          0.18658170104026794,
          0.4241452217102051,
          0.06701087951660156,
          0.02041638270020485,
          0.21228951215744019,
          0.009162619709968567,
          0.6889986991882324,
          0.1488085240125656,
          0.11613645404577255,
          0.10279614478349686,
          0.23155727982521057,
          0.19540376961231232,
          1.082322359085083,
          0.24189874529838562,
          0.1652717888355255,
          0.2996384799480438,
          0.20212876796722412,
          0.08887326717376709,
          0.0934198796749115,
          0.0561956986784935,
          0.7317627668380737,
          0.7314953207969666,
          0.23353806138038635,
          0.8901482224464417,
          0.00693502277135849,
          0.12450925260782242,
          0.07701656222343445,
          0.19243808090686798,
          0.07523353397846222,
          0.6584015488624573,
          0.01769445464015007,
          0.012101084925234318,
          0.011649733409285545,
          0.024015501141548157,
          0.07694782316684723,
          0.027457160875201225,
          0.07125989347696304,
          0.22033576667308807,
          0.10290174931287766,
          0.6525420546531677,
          0.04589635506272316,
          0.5671921372413635,
          0.7083176374435425,
          0.12535318732261658,
          0.3580564856529236,
          0.024630123749375343,
          0.6197289824485779,
          0.050138041377067566,
          0.9175820350646973,
          0.1143815740942955,
          0.07015428692102432,
          0.4878246486186981,
          1.0589927434921265,
          0.004830979276448488,
          0.03700185567140579,
          0.3685949742794037,
          0.650381326675415,
          0.17133109271526337,
          0.647819459438324,
          0.787084698677063,
          0.22494958341121674,
          0.1544618308544159,
          0.15919288992881775,
          0.14063794910907745,
          0.057591576129198074,
          0.9887834191322327,
          0.8409129977226257,
          0.03211306408047676,
          0.20574283599853516,
          0.02560172602534294,
          0.2144692838191986,
          0.21497339010238647,
          0.81803297996521,
          0.13921625912189484,
          0.0940389558672905,
          1.3714815378189087,
          0.09575377404689789,
          0.10642841458320618,
          0.1320059895515442,
          0.005843255203217268,
          0.15096904337406158,
          0.04644305258989334,
          0.15315039455890656,
          0.06736508011817932,
          0.1280674785375595,
          0.06310193240642548,
          0.9144790768623352,
          0.01641085557639599,
          0.08718549460172653,
          0.8578034043312073,
          0.5097532272338867,
          0.23700502514839172,
          0.5500152707099915,
          0.03539009392261505,
          0.13233733177185059,
          0.7856185436248779,
          0.5049079060554504,
          0.24045607447624207,
          0.16773340106010437,
          0.09998787939548492,
          0.26304754614830017,
          0.3235062062740326,
          0.8443149924278259,
          0.02331092394888401,
          0.14039389789104462,
          1.0609129667282104,
          0.13737335801124573,
          0.06212267279624939,
          0.1326909065246582,
          0.6728473901748657,
          0.0019336551195010543,
          0.7893379330635071,
          0.10333175957202911,
          0.12892991304397583,
          0.7119354009628296,
          0.17911303043365479,
          1.0350943803787231,
          0.0855315551161766,
          0.4362213611602783,
          0.0642746239900589,
          0.23018650710582733,
          0.012117886915802956,
          0.15248709917068481,
          0.6589511036872864,
          0.28224998712539673,
          0.1428292989730835,
          0.22289679944515228,
          0.07369719445705414,
          0.10110101103782654,
          1.146138072013855,
          0.07092618197202682,
          0.006739214062690735,
          0.8601244688034058,
          0.07344666868448257,
          0.05312246456742287,
          0.10747990757226944,
          0.7085656523704529,
          0.01158913690596819,
          0.3534153699874878,
          0.25354036688804626,
          0.12737226486206055,
          0.053196538239717484,
          0.29163411259651184,
          0.3766038119792938,
          0.26709267497062683,
          0.15089400112628937,
          0.1284368336200714,
          0.12551875412464142,
          0.7353489398956299,
          0.12307669967412949,
          0.0561092272400856,
          0.7129446864128113,
          1.1789536476135254,
          0.1239243671298027,
          0.23054273426532745,
          0.07127779722213745,
          0.14379583299160004,
          0.02172243408858776,
          0.261716365814209,
          0.21608737111091614,
          0.5159099102020264,
          0.6865127086639404,
          0.16696031391620636,
          0.10152263939380646,
          0.1538783460855484,
          0.08955395966768265,
          0.16187500953674316,
          0.11057796329259872,
          0.9304785132408142,
          0.8626866340637207,
          0.5319041013717651,
          0.13234885036945343,
          0.11208459734916687,
          0.24562901258468628,
          0.1215549185872078,
          0.027622222900390625,
          0.06506012380123138,
          0.021107470616698265,
          0.8326721787452698,
          1.1097270250320435,
          0.026809189468622208,
          0.6359030604362488,
          0.12701988220214844,
          1.0975145101547241,
          0.5955807566642761,
          0.15062955021858215,
          0.22410863637924194,
          0.48778286576271057,
          0.34225696325302124,
          0.2484501153230667,
          0.19003410637378693,
          0.26395300030708313,
          0.17623533308506012,
          0.8248059749603271,
          0.6597566604614258,
          0.7226476073265076,
          1.1417121887207031,
          0.6750497817993164,
          0.05135069042444229,
          0.06983888149261475,
          0.10203684866428375,
          0.7620711922645569,
          0.32476797699928284,
          0.7197478413581848,
          0.23462063074111938,
          0.33080682158470154,
          0.2745411992073059,
          0.06799671053886414,
          0.37860289216041565,
          0.36376243829727173,
          0.3332700729370117,
          0.11948077380657196,
          0.1617431640625,
          0.11970251053571701,
          0.01648901030421257,
          0.8476110696792603,
          0.13873335719108582,
          0.3829972445964813,
          0.3620341718196869,
          1.1687136888504028,
          0.30157470703125,
          0.060427017509937286,
          0.2754964828491211,
          0.15387371182441711,
          0.4257010221481323,
          0.30257612466812134,
          0.3814627230167389,
          0.4442179799079895,
          0.7312708497047424,
          0.16821834444999695,
          0.427852600812912,
          0.8138460516929626,
          0.28428980708122253,
          0.1901991218328476,
          0.7106160521507263,
          0.12534931302070618,
          0.07450421154499054,
          0.6222350001335144,
          0.40543797612190247,
          0.1234433576464653,
          0.16885137557983398,
          0.860496461391449,
          0.13267137110233307,
          0.00578608363866806,
          0.4733338952064514,
          0.16999982297420502,
          0.2309703379869461,
          0.2669704854488373,
          0.11652687937021255,
          0.01934693567454815,
          0.6107990741729736,
          0.8884637951850891,
          0.7833611369132996,
          0.17928367853164673,
          0.1410541981458664,
          0.18826061487197876,
          0.764009952545166,
          0.020598046481609344,
          0.30577531456947327,
          0.7826794385910034,
          0.18849731981754303,
          0.1423071026802063,
          0.9445472359657288,
          0.6195011138916016,
          0.6315297484397888,
          0.2292955070734024,
          0.19085325300693512,
          0.25894325971603394,
          0.3263501226902008,
          0.5228330492973328,
          0.488543301820755,
          0.2730502784252167,
          0.24404636025428772,
          0.8932403326034546,
          0.17301395535469055,
          1.1379220485687256,
          1.2937484979629517,
          0.6510657668113708,
          0.0563923679292202,
          0.7447429895401001,
          0.08523745834827423,
          0.03261071443557739,
          0.09862912446260452,
          0.28537458181381226,
          0.2393149584531784,
          0.04920816421508789,
          0.6411406397819519,
          0.10702589899301529,
          0.24425551295280457,
          0.2387966364622116,
          0.14039209485054016,
          0.6013922691345215,
          0.039647091180086136,
          0.023879453539848328,
          0.25561007857322693,
          0.1522538661956787,
          0.23100267350673676,
          0.9712973833084106,
          0.0606074184179306,
          0.02732076309621334,
          0.1645769476890564,
          0.5701223611831665,
          0.08427539467811584,
          0.0536864697933197,
          0.00156625104136765,
          0.15819008648395538,
          0.7002730369567871,
          0.018483798950910568,
          0.12457238137722015,
          0.6966771483421326,
          0.477642297744751,
          0.1847095936536789,
          0.2074361890554428,
          0.4368351995944977,
          0.47527191042900085,
          0.2786775827407837,
          0.025204645469784737,
          0.08190687000751495,
          0.08322468400001526,
          0.011795070953667164,
          0.4871172606945038,
          0.9784839153289795,
          0.9233792424201965,
          0.11580278724431992,
          0.40567490458488464,
          0.049238238483667374,
          0.1218109056353569,
          0.13673050701618195,
          0.11489439755678177,
          0.8740137219429016,
          0.2591091990470886,
          0.44468459486961365,
          0.2486964613199234,
          0.21990570425987244,
          0.2882240414619446,
          0.6022837162017822,
          0.9877641797065735,
          0.02853032574057579,
          0.15117999911308289,
          0.7479007840156555,
          0.5576969385147095,
          1.1097415685653687,
          0.2939562499523163,
          0.27900081872940063,
          0.20377561450004578,
          0.21008358895778656,
          0.32630792260169983,
          0.1158573105931282,
          0.08927199989557266,
          0.803666353225708,
          0.27725157141685486,
          0.014878307469189167,
          0.16298669576644897,
          0.22959673404693604,
          0.8761779069900513,
          0.17026817798614502,
          0.7502856850624084,
          0.17640510201454163,
          0.21763773262500763,
          0.04608245939016342,
          0.05169006064534187,
          0.10982253402471542,
          1.11521315574646,
          0.8829208016395569,
          0.057442039251327515,
          0.08701880276203156,
          0.11525380611419678,
          0.16633912920951843,
          0.16337324678897858,
          0.3190089166164398,
          0.33037105202674866,
          0.0030053777154535055,
          0.26723963022232056,
          0.3072321116924286,
          0.10942362993955612,
          0.9892011880874634,
          0.9035965800285339,
          0.04138079285621643,
          0.004539928399026394,
          0.9391651749610901,
          0.04217575863003731,
          0.9498793482780457,
          0.09626413136720657,
          0.11287027597427368,
          0.22873857617378235,
          0.9529819488525391,
          0.2858114242553711,
          0.2825987637042999,
          0.08798971772193909,
          0.10596336424350739,
          0.24909217655658722,
          0.18482959270477295,
          0.9530266523361206,
          0.0943354144692421,
          0.08520083874464035,
          0.1410057246685028,
          0.06694822758436203,
          0.06576568633317947,
          0.2552083432674408,
          0.17828768491744995,
          0.6960862874984741,
          0.10532210767269135,
          0.04598889499902725,
          0.07413475960493088,
          0.05225667357444763,
          0.005234176758676767,
          0.7588677406311035,
          0.10729077458381653,
          0.37016090750694275,
          0.450315922498703,
          0.19426505267620087,
          0.026335330680012703,
          0.08951503783464432,
          0.7876192331314087,
          1.0820307731628418,
          0.5415980815887451,
          1.246716022491455,
          0.0564902238547802,
          0.10627417266368866,
          0.6656918525695801,
          0.5867730975151062,
          0.2952658534049988,
          0.38612642884254456,
          0.1611829400062561,
          0.3063209652900696,
          0.1864665448665619,
          0.008066635578870773,
          0.42556920647621155,
          0.07953047752380371,
          0.21998216211795807,
          0.30376318097114563,
          0.15980078279972076,
          0.34577676653862,
          0.13157446682453156,
          0.03290286287665367,
          0.3267925977706909,
          0.4773952066898346,
          0.11334402114152908,
          0.003458399558439851,
          0.49691468477249146,
          0.20063404738903046,
          0.25757792592048645,
          0.08830340206623077,
          0.2946467101573944,
          0.11667923629283905,
          0.2943245470523834,
          0.015376885421574116,
          0.5517683029174805,
          0.7559097409248352,
          0.16843998432159424,
          0.6831426024436951,
          0.07037336379289627,
          0.3639928698539734,
          0.28790339827537537,
          0.15517225861549377,
          0.0824071541428566,
          0.008987020701169968,
          0.15140415728092194,
          0.21622532606124878,
          0.7819215059280396,
          0.3228952884674072,
          0.9562312960624695,
          0.8546038269996643,
          0.8820552229881287,
          0.09898963570594788,
          0.48549386858940125,
          0.21695564687252045,
          0.15264301002025604,
          0.3956177532672882,
          0.4330518841743469,
          0.46449992060661316,
          0.36925727128982544,
          0.11001502722501755,
          0.2533726394176483,
          0.24477864801883698,
          0.3187222480773926,
          0.47207409143447876,
          0.23640228807926178,
          0.08906055241823196,
          0.08116833120584488,
          0.3729325830936432,
          0.11479108035564423,
          0.37939709424972534,
          0.029160670936107635,
          0.0068447170779109,
          0.6800819039344788,
          0.6883195042610168,
          0.148690328001976,
          0.22658593952655792,
          0.6173188090324402,
          0.2502129375934601,
          0.24452441930770874,
          0.17597134411334991,
          0.20294888317584991,
          0.24208787083625793,
          0.26282039284706116,
          0.11713727563619614,
          0.12973645329475403,
          0.02398488111793995,
          0.05870480835437775,
          1.1285409927368164,
          0.20796635746955872,
          0.10544351488351822,
          0.12688207626342773,
          0.0929945632815361,
          0.600649893283844,
          0.07181750237941742,
          0.10527890920639038,
          0.6740966439247131,
          0.15965807437896729,
          0.13317370414733887,
          0.19651108980178833,
          0.15616120398044586,
          0.01299861166626215,
          0.05508417263627052,
          0.14362390339374542,
          0.9089449048042297,
          0.0424230732023716,
          0.05221203714609146,
          0.3157194256782532,
          1.2048627138137817,
          0.1276073157787323,
          0.14826378226280212,
          0.08221513777971268,
          0.07754544913768768,
          0.14849787950515747,
          0.29936328530311584,
          0.004561170004308224,
          0.007652267813682556,
          0.2394605576992035,
          0.6300382018089294,
          0.3602767884731293,
          1.0939702987670898,
          0.14530238509178162,
          0.03415443003177643,
          0.8032054305076599,
          0.2480696588754654,
          0.3537646532058716,
          0.250958114862442,
          0.2471047341823578,
          0.16374288499355316,
          0.18269209563732147,
          0.46925103664398193,
          0.03829508647322655,
          0.718836784362793,
          0.5608481168746948,
          1.1684987545013428,
          0.8401004672050476,
          0.12104674428701401,
          0.28621599078178406,
          0.17550626397132874,
          0.12776316702365875,
          0.33467617630958557,
          0.1552588790655136,
          0.13234901428222656,
          0.7685025930404663,
          0.013163008727133274,
          0.053656768053770065,
          0.29892387986183167,
          0.5161709189414978,
          0.37029409408569336,
          0.29585760831832886,
          0.16590002179145813,
          0.23048286139965057,
          0.2956691086292267,
          0.35130998492240906,
          0.12363188713788986,
          0.18765318393707275,
          0.3350827097892761,
          0.2575746178627014,
          0.19070449471473694,
          0.127793088555336,
          1.0638114213943481,
          1.0532335042953491,
          0.2798548638820648,
          0.47780877351760864,
          0.7521101832389832,
          0.09014558792114258,
          0.4048372507095337,
          0.3976321816444397,
          0.29559069871902466,
          0.42855653166770935,
          0.22298584878444672,
          0.09358049929141998,
          0.3318358063697815,
          0.388563871383667,
          0.3311205208301544,
          0.282309353351593,
          0.2315133959054947,
          0.10658250004053116,
          0.39288368821144104,
          0.5289693474769592,
          0.03218739852309227,
          0.035762395709753036,
          0.17214423418045044,
          0.18471089005470276,
          0.21801382303237915,
          0.1826920211315155,
          0.014463616535067558,
          0.01688545010983944,
          0.2296334207057953,
          0.4731863737106323,
          0.7874415516853333,
          0.21660710871219635,
          0.5080526471138,
          0.05406563729047775,
          0.024809690192341805,
          0.06773965805768967,
          0.5339732766151428,
          0.32719001173973083
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"92255cc3-4857-4bb4-bf95-09a1f89a99d3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"92255cc3-4857-4bb4-bf95-09a1f89a99d3\")) {                    Plotly.newPlot(                        \"92255cc3-4857-4bb4-bf95-09a1f89a99d3\",                        [{\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842],\"xaxis\":\"x\",\"y\":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.9999980926513672,0.9959495067596436,0.995998203754425,0.9882700443267822,0.9687219262123108,0.9571363925933838,0.8990718722343445,0.9589588642120361,0.8178107142448425,0.6824737191200256,0.5749967098236084,0.31330057978630066,0.6883383989334106,0.5141180753707886,0.29037222266197205,0.1724323332309723,0.09134173393249512,1.4354146718978882,0.4184063673019409,1.4579787254333496,0.7492532730102539,0.40117889642715454,0.6103396415710449,0.6597050428390503,0.3172221779823303,0.008655711077153683,0.1986829787492752,0.7041105031967163,0.7953142523765564,0.42931801080703735,0.7959065437316895,0.6454038619995117,0.4639735221862793,0.6813997030258179,0.39128533005714417,0.8721253275871277,0.3763536810874939,0.6014694571495056,0.47429704666137695,0.5404453873634338,0.6753824949264526,0.6827726364135742,0.5729360580444336,0.7664186954498291,0.6291307806968689,0.7192888259887695,0.8290761113166809,0.5564042329788208,0.6523182392120361,0.7281613349914551,0.5175566077232361,0.276323139667511,0.4762223958969116,0.2256925106048584,0.11132447421550751,0.4558753967285156,1.431932806968689,1.611569881439209,0.5629307627677917,1.540830373764038,0.5319552421569824,0.740668773651123,1.6887050867080688,0.012478525750339031,0.98356693983078,0.41611790657043457,0.6826203465461731,0.3536430895328522,0.26272913813591003,0.48524823784828186,0.0630868524312973,0.40607109665870667,0.5037765502929688,0.599260151386261,0.08482667058706284,0.25158724188804626,0.10508459061384201,0.17358353734016418,0.06651187688112259,0.10716409236192703,0.5690475106239319,0.18244513869285583,0.06382271647453308,0.3794248104095459,0.4592756927013397,0.8374348878860474,0.4130665361881256,0.41314226388931274,0.1798219084739685,0.21483688056468964,0.6855130195617676,0.5726779699325562,0.3924623429775238,0.22376856207847595,0.6307779550552368,0.4058266878128052,0.3571530282497406,0.1165238618850708,0.13180197775363922,0.45629316568374634,0.7403337955474854,0.21392525732517242,1.5081803798675537,0.1566266119480133,0.05723034590482712,0.1113613024353981,0.16307125985622406,0.24206800758838654,0.5714796781539917,0.16076302528381348,0.466741144657135,0.4362477660179138,0.515506386756897,0.38964489102363586,0.5487061738967896,0.18817347288131714,0.23413066565990448,0.29124271869659424,0.3391405940055847,0.5649381875991821,0.23180446028709412,0.47078269720077515,0.017540697008371353,0.4055916965007782,0.1480475813150406,0.41061195731163025,0.27082785964012146,0.07195794582366943,1.0357112884521484,0.2869359850883484,0.31865283846855164,0.048009056597948074,0.3728293478488922,0.21721307933330536,0.028319304808974266,0.17710751295089722,0.14288534224033356,0.021024057641625404,0.11903691291809082,1.0798641443252563,0.09742940962314606,0.053019098937511444,0.10900647938251495,0.09487147629261017,0.7876511812210083,0.10762067884206772,0.6576665043830872,0.701732873916626,0.03274970129132271,0.8203733563423157,0.8834065198898315,0.30130764842033386,0.5208625197410583,0.33651772141456604,0.31657877564430237,0.19720838963985443,0.5531362295150757,0.4406850337982178,0.21826742589473724,0.3509606420993805,1.0852497816085815,0.23365294933319092,0.7679125070571899,0.7629110217094421,0.1017422080039978,0.2344544231891632,0.09089291840791702,1.2175025939941406,0.03811103105545044,0.008173576556146145,0.0331343337893486,0.048663314431905746,0.00043852010276168585,0.25170278549194336,0.17631806433200836,0.041196953505277634,0.04482210427522659,0.004752502776682377,0.35827547311782837,0.09772178530693054,1.123837947845459,0.09402664005756378,0.19274835288524628,0.17264419794082642,0.09364889562129974,0.03586860001087189,0.0249194148927927,0.7429245710372925,0.21830753982067108,0.031913284212350845,0.09267058968544006,0.12039312720298767,1.2279466390609741,1.134667158126831,0.19253043830394745,0.02675705961883068,0.1695006936788559,0.21770857274532318,0.08545447140932083,1.0116106271743774,0.21516819298267365,0.07685799896717072,0.8016572594642639,0.724222719669342,0.08769474923610687,0.19067716598510742,0.1824621558189392,0.13659311830997467,0.22663527727127075,1.1724098920822144,1.0151842832565308,0.8963614702224731,0.07096122205257416,0.17198064923286438,0.21710962057113647,0.19679848849773407,0.7570735812187195,0.15697471797466278,0.19396843016147614,0.08061017841100693,0.9392257332801819,1.1334959268569946,0.0025813060346990824,0.2950649559497833,0.017542961984872818,0.0019107486587017775,0.18652302026748657,0.19619351625442505,0.017321206629276276,0.053473684936761856,1.0782840251922607,0.034801509231328964,0.00815409142524004,0.0031155175529420376,0.0066991038620471954,0.7981045246124268,1.0251891613006592,0.015300869010388851,0.8268088698387146,0.7145583033561707,0.20470112562179565,0.20922473073005676,0.23797538876533508,0.22271041572093964,0.7712717652320862,0.6857587099075317,0.19405771791934967,0.1621669977903366,0.8766315579414368,0.7599745988845825,0.10471393913030624,0.1473589390516281,0.12335386127233505,0.011890357360243797,0.065118707716465,0.19911476969718933,0.840680718421936,1.0474196672439575,0.16292867064476013,0.8029760718345642,0.15936167538166046,0.1754247397184372,0.0703129842877388,0.23702308535575867,0.9995768666267395,0.03211306408047676,0.25771331787109375,0.04713255539536476,0.0431966558098793,0.17693237960338593,0.29035240411758423,0.23404903709888458,0.6518005728721619,0.1695796549320221,0.032931167632341385,1.0796105861663818,0.06582071632146835,1.0223792791366577,0.048683978617191315,0.7916808724403381,0.12111997604370117,0.6415676474571228,0.07056626677513123,0.6353577971458435,0.23770928382873535,0.255258172750473,0.10166453570127487,0.06083094701170921,0.062081772834062576,0.10215311497449875,0.19647465646266937,0.7312673330307007,0.08935976773500443,0.04855809733271599,0.07949826866388321,0.026928136125206947,0.10143932700157166,0.8290857076644897,0.1251012086868286,0.1823342740535736,0.7523046135902405,0.25340336561203003,0.03164612129330635,0.15239953994750977,0.03889426589012146,0.8427509665489197,0.9310051202774048,0.8690045475959778,0.056332096457481384,0.1938609927892685,0.5335689187049866,1.118828535079956,0.6375423073768616,0.451144814491272,0.2889082133769989,0.2954142391681671,0.3042561411857605,0.39238467812538147,0.6716550588607788,0.20734187960624695,0.18593230843544006,0.7135193347930908,0.5814241766929626,0.4800598621368408,0.06507502496242523,0.06752365082502365,0.7646846771240234,0.3559871017932892,0.5576416254043579,0.5762004256248474,0.6594963073730469,0.09031012654304504,0.08954188227653503,0.33126863837242126,0.7082669138908386,0.3580593764781952,0.12269283086061478,0.6257997751235962,0.21091501414775848,0.5955921411514282,0.0980469286441803,0.06441006064414978,0.2386559247970581,0.09684146195650101,0.17570720613002777,0.12089946866035461,0.7346071600914001,0.9591757655143738,0.16817942261695862,1.0571095943450928,0.5526207685470581,0.4280295670032501,0.3384212255477905,0.0025896085426211357,0.07057557255029678,0.4883105456829071,0.04570680111646652,0.23804841935634613,0.058986399322748184,0.04770553484559059,0.053902629762887955,0.2410161942243576,0.023899339139461517,0.09479673951864243,0.3798517882823944,0.03767099604010582,0.01170374732464552,0.005851498804986477,0.14446671307086945,0.081962950527668,1.2325000762939453,0.0028901512268930674,0.03983330354094505,0.515255868434906,0.07262720912694931,0.7296263575553894,0.2817762494087219,0.6371276378631592,0.2976006269454956,0.337764173746109,0.6592564582824707,0.6604741215705872,0.01577661745250225,0.04345858097076416,0.18053038418293,0.24316178262233734,0.8345266580581665,0.08290477842092514,0.13819508254528046,0.11864865571260452,0.027965549379587173,1.0019768476486206,0.666621208190918,0.2057916522026062,0.12730494141578674,0.2700154483318329,0.13970771431922913,0.3306591510772705,0.25187069177627563,0.6005555391311646,0.573756992816925,0.13360200822353363,0.5306568741798401,0.274504154920578,0.20729979872703552,0.2541283965110779,0.08970415592193604,0.07523040473461151,0.011593044735491276,1.0576364994049072,0.2393411248922348,0.2557557225227356,0.0577721931040287,0.2082924246788025,0.009972654283046722,0.7426232099533081,0.11429019272327423,0.15629513561725616,0.48169511556625366,0.15781165659427643,0.2682228684425354,0.2455201894044876,0.22569377720355988,0.19219419360160828,0.22701767086982727,0.05806127190589905,0.02098836936056614,0.21291875839233398,0.04308980330824852,0.10443760454654694,0.25675344467163086,0.3371351659297943,0.6711438894271851,0.780327558517456,0.6226648092269897,0.6327990293502808,0.17465735971927643,0.3990223705768585,0.29786425828933716,0.24509479105472565,0.40730226039886475,0.28835129737854004,0.31970444321632385,0.1659824550151825,0.2917052209377289,0.137528195977211,0.03881988301873207,0.8001617193222046,0.9635415077209473,0.153132826089859,0.05298632010817528,1.3265807628631592,0.31735432147979736,0.8376113772392273,0.14216472208499908,0.022807467728853226,0.016259146854281425,0.04261835291981697,0.8075008392333984,0.8419156074523926,0.15420414507389069,0.5340118408203125,0.27877485752105713,0.28284260630607605,0.3167014718055725,0.3862975239753723,0.3831481337547302,0.21019037067890167,0.2338685840368271,0.5588244199752808,0.011343481950461864,0.06917209923267365,0.24964243173599243,0.05831917002797127,0.17834630608558655,0.11394486576318741,0.08571891486644745,0.17218680679798126,0.023320773616433144,0.3015729486942291,0.18723393976688385,0.14936155080795288,0.5978842377662659,0.20159898698329926,0.17534957826137543,0.24277707934379578,0.0464891716837883,0.9865997433662415,0.3132554888725281,0.835465133190155,0.991323709487915,0.3280368149280548,0.14261473715305328,0.6158342957496643,0.3792257606983185,0.24153350293636322,0.18847733736038208,0.4110121428966522,0.5694034099578857,0.18507646024227142,0.5723808407783508,0.09399634599685669,0.20088188350200653,0.17613273859024048,0.8778368830680847,0.011061122640967369,0.008211981505155563,0.6196691393852234,0.12294316291809082,0.13637027144432068,0.6760186553001404,0.10051901638507843,0.08836143463850021,0.22852760553359985,0.10313934832811356,0.44749724864959717,0.22301897406578064,0.10972879827022552,0.6925389170646667,0.06040288507938385,0.7664642333984375,0.4684554636478424,0.12052652984857559,0.3084184527397156,0.7734545469284058,0.19990818202495575,0.12224182486534119,0.2643905282020569,0.036580901592969894,0.010092992335557938,0.7439142465591431,0.10214963555335999,0.14054252207279205,0.04000196233391762,0.032917581498622894,0.8532448410987854,0.002024979330599308,0.21130916476249695,0.034988440573215485,0.028747448697686195,0.05817827582359314,0.9891570806503296,1.0141016244888306,0.08078622817993164,0.585713803768158,0.09588931500911713,0.5948130488395691,0.02006097510457039,0.2072257697582245,0.09919016063213348,0.18377533555030823,0.6550503969192505,0.7309881448745728,0.3553849458694458,0.6119757294654846,0.23763953149318695,0.13246066868305206,0.030195781961083412,0.732119619846344,0.0010502055520191789,0.051125429570674896,0.2798636257648468,0.8282862305641174,0.18204331398010254,0.05606760457158089,0.1240038126707077,0.9111230373382568,0.2787002921104431,0.13567297160625458,0.7293914556503296,0.48759743571281433,0.5059120655059814,0.15919342637062073,0.2641078233718872,0.29309797286987305,0.35013991594314575,0.1712852120399475,0.4588058590888977,0.20271500945091248,0.22319208085536957,0.13082042336463928,0.038068655878305435,0.18371054530143738,0.8533415198326111,0.11096186935901642,0.623550534248352,0.8339968323707581,1.2548846006393433,0.288998544216156,0.08319008350372314,0.13276879489421844,0.10155676305294037,0.34830111265182495,0.07691386342048645,0.028386326506733894,0.05776812136173248,0.2924756407737732,0.16645552217960358,0.539566695690155,0.3314798176288605,0.1709761619567871,0.1043376699090004,0.36130163073539734,0.25367096066474915,0.7233545184135437,0.17370076477527618,0.22758013010025024,0.2689511179924011,0.2258489578962326,0.1944153606891632,0.7488791942596436,0.3053763806819916,0.250927209854126,0.4757113456726074,1.0283634662628174,0.27347859740257263,0.5062769651412964,0.5896751880645752,0.4187679588794708,0.4386385977268219,0.17120465636253357,0.5318048000335693,0.33652764558792114,0.2495487779378891,0.4569970369338989,0.34132149815559387,0.43916958570480347,0.39521047472953796,0.4545850157737732,0.3826017379760742,0.44276896119117737,0.283373087644577,0.2754233181476593,0.743312418460846,0.19950929284095764,0.7217593789100647,0.14583748579025269,0.03789527714252472,0.026190444827079773,0.01805555634200573,0.8899060487747192,0.009221969172358513,0.8102777600288391,0.0029039152432233095,0.010937879793345928,0.9488371014595032,0.07762670516967773,0.7959593534469604,0.05540437996387482,0.9721390604972839,0.15736715495586395,0.9795740842819214,0.7962244749069214,0.3350363075733185,0.23236584663391113,0.3054921627044678,0.34376269578933716,0.261193186044693,0.6253960728645325,0.22473116219043732,0.41036683320999146,0.37807175517082214,0.2093670666217804,0.29495203495025635,0.22910378873348236,0.543772280216217,0.3174619972705841,0.12670521438121796,0.002533971332013607,0.0639781728386879,0.1077049970626831,1.0734423398971558,0.24704962968826294,0.9026177525520325,0.9140462279319763,0.03134595975279808,0.7163019180297852,0.5919954776763916,0.14497655630111694,0.19731129705905914,0.19975604116916656,0.2282078117132187,1.3453333377838135,0.21579496562480927,0.3220595419406891,0.13220885396003723,0.32039493322372437,0.6646725535392761,0.3996403217315674,0.4823901653289795,0.4015149474143982,0.25150325894355774,0.13995987176895142,0.024017035961151123,0.0336247980594635,0.2945430278778076,0.16738717257976532,0.1517440229654312,0.6617228984832764,0.01991724595427513,1.0451611280441284,0.8568317890167236,0.218733549118042,0.17763353884220123,0.5652983784675598,0.7597984075546265,0.24509219825267792,0.17116867005825043,0.2267174869775772,0.3994024693965912,0.012946762144565582,0.29690074920654297,0.2930079400539398,0.3182367980480194,0.24136734008789062,0.20268824696540833,0.20143157243728638,0.8579631447792053,0.0047120158560574055,0.13673578202724457,0.6904844045639038,0.7508642077445984,0.18132208287715912,0.04892006143927574,0.03584187477827072,0.05402326211333275,0.006475153379142284,0.05235058814287186,0.03407423570752144,0.17785899341106415,0.8743635416030884,0.08313239365816116,0.027617493644356728,0.9159854650497437,0.13896392285823822,0.559279203414917,0.11705613881349564,0.46579432487487793,0.2399977296590805,0.21678005158901215,0.2901661992073059,0.25810056924819946,0.2523331642150879,0.6578670740127563,0.4786592423915863,0.20210696756839752,0.5902025103569031,0.13835886120796204,0.20740841329097748,0.4920083284378052,1.1181532144546509,0.2564089298248291,0.2504911720752716,0.03374049440026283,0.534963071346283,0.1572098731994629,0.5697134137153625,0.2915656864643097,0.004769693594425917,0.31544578075408936,0.2837491035461426,0.2978735566139221,0.18173247575759888,0.43532639741897583,0.10414308309555054,0.009942772798240185,0.952263593673706,0.7165393233299255,0.6793308854103088,0.35393673181533813,0.22924675047397614,0.012732510454952717,0.01574823074042797,0.04505155235528946,0.08358310163021088,0.2419092357158661,0.6982511878013611,0.054401326924562454,0.020688941702246666,0.22429344058036804,0.15256239473819733,0.23810669779777527,0.017849024385213852,0.25252461433410645,0.011966906487941742,0.12045265734195709,0.8343738913536072,0.2603759169578552,0.03406791761517525,0.06007067859172821,0.01851499453186989,0.15018817782402039,0.9667823910713196,0.5299224853515625,0.865648090839386,0.14182665944099426,0.12734119594097137,0.13458822667598724,0.11756951361894608,0.6415097117424011,0.19277988374233246,0.1341944932937622,0.3099607229232788,0.05043908581137657,0.06625239551067352,1.1569112539291382,0.09535984694957733,0.6266617774963379,0.06209011375904083,0.012493995949625969,0.5499275326728821,0.13447785377502441,0.9861064553260803,0.28539976477622986,0.35160908102989197,0.679683268070221,0.7360788583755493,0.602406919002533,0.043570030480623245,0.09389784932136536,0.4646274149417877,0.11952083557844162,0.3685723841190338,0.6781254410743713,0.1060040146112442,0.16319994628429413,0.39014947414398193,0.07028043270111084,0.05027228221297264,0.30093514919281006,0.6732214093208313,0.15199995040893555,0.10545796900987625,0.16353511810302734,0.26659220457077026,0.7051487565040588,0.2892754375934601,0.3175418972969055,0.15720510482788086,1.1165066957473755,0.31302547454833984,0.20191365480422974,0.00526843685656786,0.05024176836013794,0.1215343102812767,0.3108733594417572,0.6441465020179749,0.04629006236791611,0.12031010538339615,0.5629860758781433,0.6372162699699402,0.6583964228630066,0.29429641366004944,0.0925595685839653,0.3934643864631653,0.0015247835544869304,0.14341041445732117,0.01666279323399067,0.04951201379299164,0.050754040479660034,0.0038048927672207355,0.0015357369557023048,0.03395506367087364,0.20586548745632172,0.5970548987388611,0.5598406195640564,0.9431166648864746,0.1958274096250534,0.06741851568222046,0.14376629889011383,0.028347140178084373,0.12193161994218826,0.08911748975515366,0.16477297246456146,0.021602528169751167,0.001356802531518042,1.0796780586242676,0.6890944242477417,0.2611924111843109,0.17298221588134766,0.07740209251642227,0.24296455085277557,0.22101828455924988,0.6411542296409607,0.048669371753931046,0.3639965355396271,0.9825250506401062,0.5938409566879272,0.07778280228376389,0.5163757801055908,0.06890624016523361,0.029117217287421227,0.7460135221481323,0.720578134059906,0.09507400542497635,0.02783045917749405,0.2958163619041443,0.1473313271999359,0.6828467845916748,0.6740323901176453,0.9059827327728271,0.30223214626312256,0.21455766260623932,0.9585104584693909,0.049058910459280014,0.5789238214492798,0.13029035925865173,0.18099486827850342,0.18438200652599335,0.07338478416204453,0.8079588413238525,0.17019493877887726,0.007295478135347366,0.001251024892553687,0.7265142202377319,0.0520239919424057,0.3445029556751251,0.06594223529100418,0.15008100867271423,0.14892905950546265,0.061299871653318405,0.9253913760185242,0.12159544229507446,0.0021685557439923286,0.21844077110290527,1.1269522905349731,0.08960293233394623,0.34462857246398926,0.4357874095439911,0.34438592195510864,0.038367435336112976,0.12808963656425476,0.07986295223236084,0.06813304871320724,0.37019282579421997,0.05652359500527382,0.38779619336128235,0.03915627673268318,0.05872654542326927,0.5343281626701355,0.10401235520839691,0.19511094689369202,0.12502998113632202,0.804163932800293,0.05089917033910751,0.5463695526123047,0.10940252244472504,0.14274007081985474,0.1591421365737915,0.8238866329193115,0.6153528690338135,0.09308288991451263,0.315684050321579,0.16683055460453033,0.13129320740699768,0.005928162485361099,0.41280969977378845,0.4729127585887909,0.14706015586853027,0.07955415546894073,0.019980106502771378,0.09132277965545654,0.4249765872955322,0.08341086655855179,0.21096310019493103,0.009159107692539692,0.10451717674732208,0.1414441466331482,0.2058262974023819,0.11435536295175552,1.1961122751235962,0.017193589359521866,0.08198992908000946,0.18407677114009857,0.047257199883461,0.24363118410110474,0.07948686927556992,0.020430702716112137,0.6983646750450134,0.46441373229026794,0.177943155169487,0.27709048986434937,0.4486686587333679,0.5075920224189758,0.32587501406669617,0.6381773948669434,0.36595237255096436,0.21494325995445251,0.26641348004341125,0.4115246534347534,0.1659962236881256,0.10612224042415619,0.0885961577296257,0.23929369449615479,0.11847221106290817,1.1098062992095947,0.20650817453861237,0.06390922516584396,1.0949913263320923,0.2386358082294464,0.02828247658908367,1.0384165048599243,0.08384327590465546,0.18975771963596344,0.10391147434711456,0.19144880771636963,0.28871411085128784,0.17795954644680023,0.1628723293542862,0.09569054841995239,0.008507096208631992,0.22598059475421906,0.31626367568969727,0.055171482264995575,0.029963744804263115,1.234931468963623,1.4964978694915771,0.17444688081741333,0.01524473074823618,0.1090262159705162,0.03861748054623604,0.05185244232416153,0.031171342357993126,0.07344131171703339,0.07678576558828354,0.049492619931697845,0.054980214685201645,0.9944836497306824,1.8926246166229248,0.62649005651474,0.18838654458522797,0.24143384397029877,0.1689794659614563,0.1514146775007248,0.16607598960399628,0.28706350922584534,0.3107492923736572,0.2690642774105072,0.5256274938583374,0.042024072259664536,0.3878307342529297,0.35756975412368774,0.3400556445121765,0.30548691749572754,0.021315600723028183,0.058671340346336365,0.37606093287467957,0.8822898864746094,0.9711802005767822,0.39962953329086304,0.2732570767402649,0.20278358459472656,0.14855965971946716,0.06570166349411011,0.00013115150795783848,0.4663008749485016,0.3740731179714203,0.5259240865707397,0.493988573551178,0.4739338755607605,0.41954997181892395,0.0008576773689128458,0.08556582033634186,0.48481833934783936,0.5877096056938171,0.041007183492183685,0.16534388065338135,0.41538941860198975,0.36803731322288513,0.3248218894004822,0.3008863031864166,0.25827741622924805,0.015121073462069035,0.024490751326084137,0.20497219264507294,0.1928764432668686,0.07039741426706314,0.14249195158481598,0.1745840609073639,0.16598674654960632,0.17308443784713745,0.06423893570899963,1.0745877027511597,0.1390000432729721,0.1699141263961792,0.7988857626914978,0.057656027376651764,0.805493175983429,0.2488866150379181,0.29366883635520935,0.15716330707073212,0.6103699207305908,0.27726438641548157,0.21839317679405212,0.32327666878700256,0.15889610350131989,0.16526328027248383,0.3559901714324951,0.0749242976307869,0.04753711074590683,0.1590573936700821,0.10553184151649475,0.05454917997121811,0.16756495833396912,0.057399339973926544,0.07398144155740738,0.07714428007602692,0.2786199450492859,0.04285089299082756,0.1471557915210724,1.015918493270874,0.12411370873451233,0.5489974021911621,0.1223478764295578,0.7118943929672241,0.478849321603775,0.3044520914554596,0.636779248714447,0.43531185388565063,0.2472093105316162,0.708311915397644,0.4472355246543884,0.31425076723098755,0.2583584487438202,0.24963192641735077,0.34071066975593567,0.1702674776315689,0.09662740677595139,0.013580123893916607,0.10559559613466263,0.7950740456581116,0.4545226991176605,0.6298590302467346,0.05604582652449608,0.7346600294113159,0.1078125461935997,0.36368730664253235,0.17060653865337372,0.4776504635810852,0.265762597322464,0.5083538293838501,0.2533233165740967,0.11555235087871552,0.1639682948589325,0.1784767061471939,0.082845039665699,0.3113870620727539,0.04854537546634674,0.07309143245220184,0.836095929145813,1.078353762626648,0.10811659693717957,0.05461715906858444,0.1744527667760849,0.21954180300235748,0.15801477432250977,0.0801095962524414,0.08665187656879425,0.39640432596206665,0.026219917461276054,0.4272555708885193,0.050597693771123886,0.8536881804466248,0.5891577005386353,0.1257634311914444,0.022086048498749733,0.22061842679977417,0.2076481282711029,0.24785982072353363,0.2876395881175995,0.6929485201835632,0.21137337386608124,0.08254799991846085,0.04657683148980141,0.36834144592285156,0.026359835639595985,0.151997372508049,0.1356344372034073,0.7453398704528809,0.08511754125356674,0.5986241698265076,0.20790202915668488,0.2025633007287979,0.09630352258682251,0.4453207850456238,0.09597218036651611,1.0802721977233887,0.06205517798662186,0.7237504124641418,0.8618267774581909,0.43321162462234497,0.2791348099708557,0.2706667482852936,0.20701982080936432,0.05051474645733833,0.6313040852546692,0.5597807765007019,0.2580052316188812,0.25609317421913147,0.3008258640766144,0.3308529257774353,0.2140311598777771,0.1004478931427002,0.23050646483898163,0.36392179131507874,0.7712518572807312,0.16907204687595367,0.04857480898499489,0.1898755431175232,0.14231978356838226,0.24657166004180908,0.5264875292778015,0.20954975485801697,0.18658170104026794,0.4241452217102051,0.06701087951660156,0.02041638270020485,0.21228951215744019,0.009162619709968567,0.6889986991882324,0.1488085240125656,0.11613645404577255,0.10279614478349686,0.23155727982521057,0.19540376961231232,1.082322359085083,0.24189874529838562,0.1652717888355255,0.2996384799480438,0.20212876796722412,0.08887326717376709,0.0934198796749115,0.0561956986784935,0.7317627668380737,0.7314953207969666,0.23353806138038635,0.8901482224464417,0.00693502277135849,0.12450925260782242,0.07701656222343445,0.19243808090686798,0.07523353397846222,0.6584015488624573,0.01769445464015007,0.012101084925234318,0.011649733409285545,0.024015501141548157,0.07694782316684723,0.027457160875201225,0.07125989347696304,0.22033576667308807,0.10290174931287766,0.6525420546531677,0.04589635506272316,0.5671921372413635,0.7083176374435425,0.12535318732261658,0.3580564856529236,0.024630123749375343,0.6197289824485779,0.050138041377067566,0.9175820350646973,0.1143815740942955,0.07015428692102432,0.4878246486186981,1.0589927434921265,0.004830979276448488,0.03700185567140579,0.3685949742794037,0.650381326675415,0.17133109271526337,0.647819459438324,0.787084698677063,0.22494958341121674,0.1544618308544159,0.15919288992881775,0.14063794910907745,0.057591576129198074,0.9887834191322327,0.8409129977226257,0.03211306408047676,0.20574283599853516,0.02560172602534294,0.2144692838191986,0.21497339010238647,0.81803297996521,0.13921625912189484,0.0940389558672905,1.3714815378189087,0.09575377404689789,0.10642841458320618,0.1320059895515442,0.005843255203217268,0.15096904337406158,0.04644305258989334,0.15315039455890656,0.06736508011817932,0.1280674785375595,0.06310193240642548,0.9144790768623352,0.01641085557639599,0.08718549460172653,0.8578034043312073,0.5097532272338867,0.23700502514839172,0.5500152707099915,0.03539009392261505,0.13233733177185059,0.7856185436248779,0.5049079060554504,0.24045607447624207,0.16773340106010437,0.09998787939548492,0.26304754614830017,0.3235062062740326,0.8443149924278259,0.02331092394888401,0.14039389789104462,1.0609129667282104,0.13737335801124573,0.06212267279624939,0.1326909065246582,0.6728473901748657,0.0019336551195010543,0.7893379330635071,0.10333175957202911,0.12892991304397583,0.7119354009628296,0.17911303043365479,1.0350943803787231,0.0855315551161766,0.4362213611602783,0.0642746239900589,0.23018650710582733,0.012117886915802956,0.15248709917068481,0.6589511036872864,0.28224998712539673,0.1428292989730835,0.22289679944515228,0.07369719445705414,0.10110101103782654,1.146138072013855,0.07092618197202682,0.006739214062690735,0.8601244688034058,0.07344666868448257,0.05312246456742287,0.10747990757226944,0.7085656523704529,0.01158913690596819,0.3534153699874878,0.25354036688804626,0.12737226486206055,0.053196538239717484,0.29163411259651184,0.3766038119792938,0.26709267497062683,0.15089400112628937,0.1284368336200714,0.12551875412464142,0.7353489398956299,0.12307669967412949,0.0561092272400856,0.7129446864128113,1.1789536476135254,0.1239243671298027,0.23054273426532745,0.07127779722213745,0.14379583299160004,0.02172243408858776,0.261716365814209,0.21608737111091614,0.5159099102020264,0.6865127086639404,0.16696031391620636,0.10152263939380646,0.1538783460855484,0.08955395966768265,0.16187500953674316,0.11057796329259872,0.9304785132408142,0.8626866340637207,0.5319041013717651,0.13234885036945343,0.11208459734916687,0.24562901258468628,0.1215549185872078,0.027622222900390625,0.06506012380123138,0.021107470616698265,0.8326721787452698,1.1097270250320435,0.026809189468622208,0.6359030604362488,0.12701988220214844,1.0975145101547241,0.5955807566642761,0.15062955021858215,0.22410863637924194,0.48778286576271057,0.34225696325302124,0.2484501153230667,0.19003410637378693,0.26395300030708313,0.17623533308506012,0.8248059749603271,0.6597566604614258,0.7226476073265076,1.1417121887207031,0.6750497817993164,0.05135069042444229,0.06983888149261475,0.10203684866428375,0.7620711922645569,0.32476797699928284,0.7197478413581848,0.23462063074111938,0.33080682158470154,0.2745411992073059,0.06799671053886414,0.37860289216041565,0.36376243829727173,0.3332700729370117,0.11948077380657196,0.1617431640625,0.11970251053571701,0.01648901030421257,0.8476110696792603,0.13873335719108582,0.3829972445964813,0.3620341718196869,1.1687136888504028,0.30157470703125,0.060427017509937286,0.2754964828491211,0.15387371182441711,0.4257010221481323,0.30257612466812134,0.3814627230167389,0.4442179799079895,0.7312708497047424,0.16821834444999695,0.427852600812912,0.8138460516929626,0.28428980708122253,0.1901991218328476,0.7106160521507263,0.12534931302070618,0.07450421154499054,0.6222350001335144,0.40543797612190247,0.1234433576464653,0.16885137557983398,0.860496461391449,0.13267137110233307,0.00578608363866806,0.4733338952064514,0.16999982297420502,0.2309703379869461,0.2669704854488373,0.11652687937021255,0.01934693567454815,0.6107990741729736,0.8884637951850891,0.7833611369132996,0.17928367853164673,0.1410541981458664,0.18826061487197876,0.764009952545166,0.020598046481609344,0.30577531456947327,0.7826794385910034,0.18849731981754303,0.1423071026802063,0.9445472359657288,0.6195011138916016,0.6315297484397888,0.2292955070734024,0.19085325300693512,0.25894325971603394,0.3263501226902008,0.5228330492973328,0.488543301820755,0.2730502784252167,0.24404636025428772,0.8932403326034546,0.17301395535469055,1.1379220485687256,1.2937484979629517,0.6510657668113708,0.0563923679292202,0.7447429895401001,0.08523745834827423,0.03261071443557739,0.09862912446260452,0.28537458181381226,0.2393149584531784,0.04920816421508789,0.6411406397819519,0.10702589899301529,0.24425551295280457,0.2387966364622116,0.14039209485054016,0.6013922691345215,0.039647091180086136,0.023879453539848328,0.25561007857322693,0.1522538661956787,0.23100267350673676,0.9712973833084106,0.0606074184179306,0.02732076309621334,0.1645769476890564,0.5701223611831665,0.08427539467811584,0.0536864697933197,0.00156625104136765,0.15819008648395538,0.7002730369567871,0.018483798950910568,0.12457238137722015,0.6966771483421326,0.477642297744751,0.1847095936536789,0.2074361890554428,0.4368351995944977,0.47527191042900085,0.2786775827407837,0.025204645469784737,0.08190687000751495,0.08322468400001526,0.011795070953667164,0.4871172606945038,0.9784839153289795,0.9233792424201965,0.11580278724431992,0.40567490458488464,0.049238238483667374,0.1218109056353569,0.13673050701618195,0.11489439755678177,0.8740137219429016,0.2591091990470886,0.44468459486961365,0.2486964613199234,0.21990570425987244,0.2882240414619446,0.6022837162017822,0.9877641797065735,0.02853032574057579,0.15117999911308289,0.7479007840156555,0.5576969385147095,1.1097415685653687,0.2939562499523163,0.27900081872940063,0.20377561450004578,0.21008358895778656,0.32630792260169983,0.1158573105931282,0.08927199989557266,0.803666353225708,0.27725157141685486,0.014878307469189167,0.16298669576644897,0.22959673404693604,0.8761779069900513,0.17026817798614502,0.7502856850624084,0.17640510201454163,0.21763773262500763,0.04608245939016342,0.05169006064534187,0.10982253402471542,1.11521315574646,0.8829208016395569,0.057442039251327515,0.08701880276203156,0.11525380611419678,0.16633912920951843,0.16337324678897858,0.3190089166164398,0.33037105202674866,0.0030053777154535055,0.26723963022232056,0.3072321116924286,0.10942362993955612,0.9892011880874634,0.9035965800285339,0.04138079285621643,0.004539928399026394,0.9391651749610901,0.04217575863003731,0.9498793482780457,0.09626413136720657,0.11287027597427368,0.22873857617378235,0.9529819488525391,0.2858114242553711,0.2825987637042999,0.08798971772193909,0.10596336424350739,0.24909217655658722,0.18482959270477295,0.9530266523361206,0.0943354144692421,0.08520083874464035,0.1410057246685028,0.06694822758436203,0.06576568633317947,0.2552083432674408,0.17828768491744995,0.6960862874984741,0.10532210767269135,0.04598889499902725,0.07413475960493088,0.05225667357444763,0.005234176758676767,0.7588677406311035,0.10729077458381653,0.37016090750694275,0.450315922498703,0.19426505267620087,0.026335330680012703,0.08951503783464432,0.7876192331314087,1.0820307731628418,0.5415980815887451,1.246716022491455,0.0564902238547802,0.10627417266368866,0.6656918525695801,0.5867730975151062,0.2952658534049988,0.38612642884254456,0.1611829400062561,0.3063209652900696,0.1864665448665619,0.008066635578870773,0.42556920647621155,0.07953047752380371,0.21998216211795807,0.30376318097114563,0.15980078279972076,0.34577676653862,0.13157446682453156,0.03290286287665367,0.3267925977706909,0.4773952066898346,0.11334402114152908,0.003458399558439851,0.49691468477249146,0.20063404738903046,0.25757792592048645,0.08830340206623077,0.2946467101573944,0.11667923629283905,0.2943245470523834,0.015376885421574116,0.5517683029174805,0.7559097409248352,0.16843998432159424,0.6831426024436951,0.07037336379289627,0.3639928698539734,0.28790339827537537,0.15517225861549377,0.0824071541428566,0.008987020701169968,0.15140415728092194,0.21622532606124878,0.7819215059280396,0.3228952884674072,0.9562312960624695,0.8546038269996643,0.8820552229881287,0.09898963570594788,0.48549386858940125,0.21695564687252045,0.15264301002025604,0.3956177532672882,0.4330518841743469,0.46449992060661316,0.36925727128982544,0.11001502722501755,0.2533726394176483,0.24477864801883698,0.3187222480773926,0.47207409143447876,0.23640228807926178,0.08906055241823196,0.08116833120584488,0.3729325830936432,0.11479108035564423,0.37939709424972534,0.029160670936107635,0.0068447170779109,0.6800819039344788,0.6883195042610168,0.148690328001976,0.22658593952655792,0.6173188090324402,0.2502129375934601,0.24452441930770874,0.17597134411334991,0.20294888317584991,0.24208787083625793,0.26282039284706116,0.11713727563619614,0.12973645329475403,0.02398488111793995,0.05870480835437775,1.1285409927368164,0.20796635746955872,0.10544351488351822,0.12688207626342773,0.0929945632815361,0.600649893283844,0.07181750237941742,0.10527890920639038,0.6740966439247131,0.15965807437896729,0.13317370414733887,0.19651108980178833,0.15616120398044586,0.01299861166626215,0.05508417263627052,0.14362390339374542,0.9089449048042297,0.0424230732023716,0.05221203714609146,0.3157194256782532,1.2048627138137817,0.1276073157787323,0.14826378226280212,0.08221513777971268,0.07754544913768768,0.14849787950515747,0.29936328530311584,0.004561170004308224,0.007652267813682556,0.2394605576992035,0.6300382018089294,0.3602767884731293,1.0939702987670898,0.14530238509178162,0.03415443003177643,0.8032054305076599,0.2480696588754654,0.3537646532058716,0.250958114862442,0.2471047341823578,0.16374288499355316,0.18269209563732147,0.46925103664398193,0.03829508647322655,0.718836784362793,0.5608481168746948,1.1684987545013428,0.8401004672050476,0.12104674428701401,0.28621599078178406,0.17550626397132874,0.12776316702365875,0.33467617630958557,0.1552588790655136,0.13234901428222656,0.7685025930404663,0.013163008727133274,0.053656768053770065,0.29892387986183167,0.5161709189414978,0.37029409408569336,0.29585760831832886,0.16590002179145813,0.23048286139965057,0.2956691086292267,0.35130998492240906,0.12363188713788986,0.18765318393707275,0.3350827097892761,0.2575746178627014,0.19070449471473694,0.127793088555336,1.0638114213943481,1.0532335042953491,0.2798548638820648,0.47780877351760864,0.7521101832389832,0.09014558792114258,0.4048372507095337,0.3976321816444397,0.29559069871902466,0.42855653166770935,0.22298584878444672,0.09358049929141998,0.3318358063697815,0.388563871383667,0.3311205208301544,0.282309353351593,0.2315133959054947,0.10658250004053116,0.39288368821144104,0.5289693474769592,0.03218739852309227,0.035762395709753036,0.17214423418045044,0.18471089005470276,0.21801382303237915,0.1826920211315155,0.014463616535067558,0.01688545010983944,0.2296334207057953,0.4731863737106323,0.7874415516853333,0.21660710871219635,0.5080526471138,0.05406563729047775,0.024809690192341805,0.06773965805768967,0.5339732766151428,0.32719001173973083],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('92255cc3-4857-4bb4-bf95-09a1f89a99d3');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.line(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=0<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "0",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "0",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459
         ],
         "xaxis": "x",
         "y": [
          0.2274562269449234,
          0.10168550163507462,
          0.19635511934757233,
          0.36778900027275085,
          0.23009803891181946,
          0.14137586951255798,
          0.009093888103961945,
          0.14587856829166412,
          0.12692558765411377,
          0.0982598289847374,
          0.4623773992061615,
          0.4807228744029999,
          0.1217917650938034,
          0.3244381844997406,
          0.1323961764574051,
          0.9235568046569824,
          0.1594899445772171,
          0.23630119860172272,
          0.1698751598596573,
          0.1856890320777893,
          0.3693873882293701,
          0.062284842133522034,
          0.08980303257703781,
          0.10225450247526169,
          0.0778219923377037,
          0.1443672478199005,
          0.27804630994796753,
          0.16806086897850037,
          0.3638024628162384,
          0.6848880648612976,
          0.15812434256076813,
          0.552749752998352,
          0.19232085347175598,
          0.22393950819969177,
          0.2801831066608429,
          0.20907056331634521,
          0.18506602942943573,
          0.48247435688972473,
          0.23338519036769867,
          0.11588893830776215,
          0.396577388048172,
          0.46840181946754456,
          0.4916536211967468,
          0.2184745967388153,
          0.45494544506073,
          0.32810184359550476,
          0.1744408756494522,
          0.11287793517112732,
          0.24573418498039246,
          0.2952902317047119,
          0.7476877570152283,
          0.13117621839046478,
          0.2997174561023712,
          0.21363773941993713,
          0.1323050856590271,
          0.1475113481283188,
          0.34499165415763855,
          0.17878784239292145,
          0.22046902775764465,
          0.2503974735736847,
          0.09644002467393875,
          0.1580154150724411,
          0.4213297963142395,
          0.211848184466362,
          0.1624266356229782,
          0.19798396527767181,
          0.23392018675804138,
          0.2803679406642914,
          0.16708317399024963,
          0.24394866824150085,
          0.18537987768650055,
          0.6293312311172485,
          0.11393290758132935,
          0.6349895596504211,
          0.07002924382686615,
          0.2508924603462219,
          0.07595045119524002,
          0.2424594908952713,
          0.36692196130752563,
          0.1696694940328598,
          0.574154257774353,
          0.13652241230010986,
          0.11196105182170868,
          0.16871871054172516,
          0.056218020617961884,
          0.2276533991098404,
          0.21013490855693817,
          0.10851331800222397,
          0.28623539209365845,
          0.2167981117963791,
          0.2327577918767929,
          0.4129721522331238,
          0.20040492713451385,
          0.27987369894981384,
          0.12449406087398529,
          0.18612375855445862,
          0.0842670425772667,
          0.29099637269973755,
          0.11637549847364426,
          0.1977064162492752,
          0.27837616205215454,
          0.19783936440944672,
          0.7245227098464966,
          0.2388916164636612,
          0.09934486448764801,
          0.29159536957740784,
          0.3396438956260681,
          0.7941604256629944,
          0.15938502550125122,
          0.8370487093925476,
          0.39806410670280457,
          0.09751484543085098,
          0.1054694801568985,
          0.19381234049797058,
          0.15834517776966095,
          0.1385902613401413,
          0.40072616934776306,
          0.13795019686222076,
          0.4960983097553253,
          0.08769141137599945,
          0.3274987041950226,
          0.27178752422332764,
          0.41138747334480286,
          0.34272465109825134,
          0.1794314980506897,
          0.3112975060939789,
          0.10557969659566879,
          0.4435640871524811,
          0.32699793577194214,
          0.1686512678861618,
          0.1237756758928299,
          0.6186283230781555,
          0.5712003707885742,
          0.6371650695800781,
          0.23661483824253082,
          0.38029980659484863,
          0.3418165445327759,
          0.2304242104291916,
          0.3488138020038605,
          0.1917704939842224,
          0.23003032803535461,
          0.23929990828037262,
          0.22787222266197205,
          0.21636077761650085,
          0.1889207661151886,
          0.5741727352142334,
          0.16169200837612152,
          0.3426835834980011,
          0.11796983331441879,
          0.4508676528930664,
          0.4666706919670105,
          0.6529154181480408,
          0.24271716177463531,
          0.5325256586074829,
          0.19325512647628784,
          0.4578416347503662,
          0.15042226016521454,
          0.13595843315124512,
          0.5871356725692749,
          0.48856380581855774,
          0.7193024754524231,
          0.06994230300188065,
          0.18399780988693237,
          0.028399601578712463,
          0.24380889534950256,
          0.11755643784999847,
          0.8053966760635376,
          0.1820077896118164,
          0.15990246832370758,
          0.4336071014404297,
          0.12110375612974167,
          0.6054008603096008,
          0.27715179324150085,
          0.20973418653011322,
          0.13979923725128174,
          0.14627109467983246,
          0.3914515972137451,
          0.1559305340051651,
          0.17724962532520294,
          0.17451348900794983,
          0.5778281092643738,
          0.12082420289516449,
          0.582974374294281,
          0.3141215741634369,
          0.2049906849861145,
          0.06000867858529091,
          0.20670044422149658,
          0.1453232765197754,
          0.6961576342582703,
          0.13133515417575836,
          0.20441730320453644,
          0.18537437915802002,
          0.19473356008529663,
          0.3394470512866974,
          0.14654286205768585,
          0.44784072041511536,
          0.8237339854240417,
          0.08821327239274979,
          0.43518099188804626,
          0.057864490896463394,
          0.2374134063720703,
          0.4562514126300812,
          0.16697195172309875,
          0.21488600969314575,
          0.31961843371391296,
          0.0692276582121849,
          0.16431495547294617,
          0.5096884965896606,
          0.2279164344072342,
          0.17061372101306915,
          0.12515312433242798,
          0.5894845128059387,
          0.222573921084404,
          0.2846067547798157,
          0.14685870707035065,
          0.5348938703536987,
          0.06699588894844055,
          0.29175323247909546,
          0.16090059280395508,
          0.2280355840921402,
          0.050864946097135544,
          0.7151582837104797,
          0.05880396068096161,
          0.5149317979812622,
          0.3456990420818329,
          0.5161406993865967,
          0.15913443267345428,
          0.3767347037792206,
          0.13463479280471802,
          0.17284126579761505,
          0.074185810983181,
          0.47409787774086,
          0.1311749368906021,
          0.4253598153591156,
          0.8374501466751099,
          0.16385357081890106,
          0.27124953269958496,
          0.19503793120384216,
          0.27941447496414185,
          0.1319257616996765,
          0.4106140434741974,
          0.07868245244026184,
          0.17792394757270813,
          0.3120405972003937,
          0.28092846274375916,
          0.12648910284042358,
          0.23754766583442688,
          0.15667815506458282,
          0.5647699236869812,
          0.16552922129631042,
          0.2373007833957672,
          0.11501895636320114,
          0.18473681807518005,
          0.19741016626358032,
          0.17906694114208221,
          0.13300096988677979,
          0.43115872144699097,
          0.07331027090549469,
          0.2398657649755478,
          0.2785203754901886,
          0.21286913752555847,
          0.17608557641506195,
          0.07380152493715286,
          0.16315682232379913,
          0.23038797080516815,
          0.30616527795791626,
          0.8584372997283936,
          0.16344444453716278,
          0.44214293360710144,
          0.23010660707950592,
          0.7845783233642578,
          0.11667023599147797,
          0.26098793745040894,
          0.16826416552066803,
          0.5971802473068237,
          0.07366889715194702,
          0.5687891244888306,
          0.7785646915435791,
          0.2559049129486084,
          0.5632057189941406,
          0.17704270780086517,
          0.4649452567100525,
          0.06366779655218124,
          0.18247348070144653,
          0.11925505846738815,
          0.15450382232666016,
          0.15300573408603668,
          0.25718629360198975,
          0.07444695383310318,
          0.3699156641960144,
          0.19024920463562012,
          0.20511004328727722,
          0.4901719391345978,
          0.32873812317848206,
          0.16681016981601715,
          0.324737548828125,
          0.08882948756217957,
          0.186372771859169,
          0.3106207847595215,
          0.3811393082141876,
          0.4250175952911377,
          0.225916787981987,
          0.08613798767328262,
          0.5062373280525208,
          0.1530395895242691,
          0.2015342116355896,
          0.1753070205450058,
          0.10995105654001236,
          0.3594485819339752,
          0.49178528785705566,
          0.23321178555488586,
          0.06062154844403267,
          0.8280954957008362,
          0.5466729402542114,
          0.8189460039138794,
          0.3877585828304291,
          0.9436032176017761,
          0.13419799506664276,
          0.258841872215271,
          0.15172553062438965,
          0.07895205914974213,
          0.09060396254062653,
          0.6447064876556396,
          0.2742381989955902,
          0.5908106565475464,
          0.46118396520614624,
          0.5823273658752441,
          0.4013635814189911,
          0.09274660050868988,
          0.19960276782512665,
          0.20291875302791595,
          0.3165924847126007,
          0.7219047546386719,
          0.6964389681816101,
          0.13980446755886078,
          0.5715582966804504,
          0.5912068486213684,
          0.7384056448936462,
          0.14856725931167603,
          0.942653477191925,
          0.13202565908432007,
          0.09573981910943985,
          0.33610257506370544,
          0.13686025142669678,
          0.2774834930896759,
          0.05020726099610329,
          0.5686115622520447,
          0.5828368663787842,
          0.17543818056583405,
          0.8771880269050598,
          0.6716324090957642,
          0.14591506123542786,
          0.5900397300720215,
          0.16796375811100006,
          0.15294906497001648,
          0.6433587670326233,
          0.2593724727630615,
          0.17892128229141235,
          0.4589049816131592,
          0.587770938873291,
          0.19211435317993164,
          0.07427383214235306,
          0.6946597695350647,
          0.5577563047409058,
          0.554751455783844,
          0.07825659215450287,
          0.5000132918357849,
          0.11317851394414902,
          0.0297255702316761,
          0.4179264307022095,
          0.365376353263855,
          0.617295503616333,
          0.40748172998428345,
          0.17919403314590454,
          0.12406499683856964,
          0.14152683317661285,
          0.17597073316574097,
          0.0908680111169815,
          0.13177242875099182,
          0.12013812363147736,
          0.1772312968969345,
          0.16621460020542145,
          0.17275552451610565,
          0.06365722417831421,
          0.2013227343559265,
          0.8164985775947571,
          0.47828373312950134,
          0.4720100462436676,
          0.06753641366958618,
          0.14508260786533356,
          0.13884180784225464,
          0.15650299191474915,
          0.2323915809392929,
          0.10840369015932083,
          0.33533230423927307,
          0.17104464769363403,
          0.3450629711151123,
          0.16013754904270172,
          0.2006261646747589,
          0.10575645416975021,
          0.18779949843883514,
          0.07108305394649506,
          0.590715229511261,
          0.538155734539032,
          0.23157991468906403,
          0.14717470109462738,
          0.12666422128677368,
          0.7448309659957886,
          0.17028753459453583,
          0.26403117179870605,
          0.22347453236579895,
          0.49364498257637024,
          0.1373254358768463,
          0.3213200867176056,
          0.1675998717546463,
          0.1769922971725464,
          0.048148367553949356,
          0.6793933510780334,
          0.19664622843265533,
          0.5643523335456848,
          0.190149188041687,
          0.15633773803710938,
          0.8633370399475098,
          0.10593139380216599,
          0.053160153329372406,
          0.25527775287628174,
          0.09830053150653839,
          0.14853405952453613,
          0.15660592913627625,
          0.2612641155719757,
          0.18360809981822968,
          0.28951284289360046,
          0.22087451815605164,
          0.8423928618431091,
          0.25359395146369934,
          0.19391264021396637,
          0.23105615377426147,
          0.24644456803798676,
          0.33235082030296326,
          0.14303545653820038,
          0.8972945213317871,
          0.0354958102107048,
          0.43880122900009155,
          0.6541925668716431,
          0.21250763535499573,
          0.2785411477088928,
          0.21814334392547607,
          0.24006696045398712,
          0.3031580150127411,
          0.21683108806610107,
          0.09886190295219421,
          0.8388686180114746,
          0.15719012916088104,
          0.39855802059173584,
          0.08077669888734818,
          0.6660689115524292,
          0.13086989521980286,
          0.11248267441987991,
          0.5013366937637329,
          0.13259391486644745
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"531a0cc9-d63d-4fe3-ab33-ade9d703e89a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"531a0cc9-d63d-4fe3-ab33-ade9d703e89a\")) {                    Plotly.newPlot(                        \"531a0cc9-d63d-4fe3-ab33-ade9d703e89a\",                        [{\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459],\"xaxis\":\"x\",\"y\":[0.2274562269449234,0.10168550163507462,0.19635511934757233,0.36778900027275085,0.23009803891181946,0.14137586951255798,0.009093888103961945,0.14587856829166412,0.12692558765411377,0.0982598289847374,0.4623773992061615,0.4807228744029999,0.1217917650938034,0.3244381844997406,0.1323961764574051,0.9235568046569824,0.1594899445772171,0.23630119860172272,0.1698751598596573,0.1856890320777893,0.3693873882293701,0.062284842133522034,0.08980303257703781,0.10225450247526169,0.0778219923377037,0.1443672478199005,0.27804630994796753,0.16806086897850037,0.3638024628162384,0.6848880648612976,0.15812434256076813,0.552749752998352,0.19232085347175598,0.22393950819969177,0.2801831066608429,0.20907056331634521,0.18506602942943573,0.48247435688972473,0.23338519036769867,0.11588893830776215,0.396577388048172,0.46840181946754456,0.4916536211967468,0.2184745967388153,0.45494544506073,0.32810184359550476,0.1744408756494522,0.11287793517112732,0.24573418498039246,0.2952902317047119,0.7476877570152283,0.13117621839046478,0.2997174561023712,0.21363773941993713,0.1323050856590271,0.1475113481283188,0.34499165415763855,0.17878784239292145,0.22046902775764465,0.2503974735736847,0.09644002467393875,0.1580154150724411,0.4213297963142395,0.211848184466362,0.1624266356229782,0.19798396527767181,0.23392018675804138,0.2803679406642914,0.16708317399024963,0.24394866824150085,0.18537987768650055,0.6293312311172485,0.11393290758132935,0.6349895596504211,0.07002924382686615,0.2508924603462219,0.07595045119524002,0.2424594908952713,0.36692196130752563,0.1696694940328598,0.574154257774353,0.13652241230010986,0.11196105182170868,0.16871871054172516,0.056218020617961884,0.2276533991098404,0.21013490855693817,0.10851331800222397,0.28623539209365845,0.2167981117963791,0.2327577918767929,0.4129721522331238,0.20040492713451385,0.27987369894981384,0.12449406087398529,0.18612375855445862,0.0842670425772667,0.29099637269973755,0.11637549847364426,0.1977064162492752,0.27837616205215454,0.19783936440944672,0.7245227098464966,0.2388916164636612,0.09934486448764801,0.29159536957740784,0.3396438956260681,0.7941604256629944,0.15938502550125122,0.8370487093925476,0.39806410670280457,0.09751484543085098,0.1054694801568985,0.19381234049797058,0.15834517776966095,0.1385902613401413,0.40072616934776306,0.13795019686222076,0.4960983097553253,0.08769141137599945,0.3274987041950226,0.27178752422332764,0.41138747334480286,0.34272465109825134,0.1794314980506897,0.3112975060939789,0.10557969659566879,0.4435640871524811,0.32699793577194214,0.1686512678861618,0.1237756758928299,0.6186283230781555,0.5712003707885742,0.6371650695800781,0.23661483824253082,0.38029980659484863,0.3418165445327759,0.2304242104291916,0.3488138020038605,0.1917704939842224,0.23003032803535461,0.23929990828037262,0.22787222266197205,0.21636077761650085,0.1889207661151886,0.5741727352142334,0.16169200837612152,0.3426835834980011,0.11796983331441879,0.4508676528930664,0.4666706919670105,0.6529154181480408,0.24271716177463531,0.5325256586074829,0.19325512647628784,0.4578416347503662,0.15042226016521454,0.13595843315124512,0.5871356725692749,0.48856380581855774,0.7193024754524231,0.06994230300188065,0.18399780988693237,0.028399601578712463,0.24380889534950256,0.11755643784999847,0.8053966760635376,0.1820077896118164,0.15990246832370758,0.4336071014404297,0.12110375612974167,0.6054008603096008,0.27715179324150085,0.20973418653011322,0.13979923725128174,0.14627109467983246,0.3914515972137451,0.1559305340051651,0.17724962532520294,0.17451348900794983,0.5778281092643738,0.12082420289516449,0.582974374294281,0.3141215741634369,0.2049906849861145,0.06000867858529091,0.20670044422149658,0.1453232765197754,0.6961576342582703,0.13133515417575836,0.20441730320453644,0.18537437915802002,0.19473356008529663,0.3394470512866974,0.14654286205768585,0.44784072041511536,0.8237339854240417,0.08821327239274979,0.43518099188804626,0.057864490896463394,0.2374134063720703,0.4562514126300812,0.16697195172309875,0.21488600969314575,0.31961843371391296,0.0692276582121849,0.16431495547294617,0.5096884965896606,0.2279164344072342,0.17061372101306915,0.12515312433242798,0.5894845128059387,0.222573921084404,0.2846067547798157,0.14685870707035065,0.5348938703536987,0.06699588894844055,0.29175323247909546,0.16090059280395508,0.2280355840921402,0.050864946097135544,0.7151582837104797,0.05880396068096161,0.5149317979812622,0.3456990420818329,0.5161406993865967,0.15913443267345428,0.3767347037792206,0.13463479280471802,0.17284126579761505,0.074185810983181,0.47409787774086,0.1311749368906021,0.4253598153591156,0.8374501466751099,0.16385357081890106,0.27124953269958496,0.19503793120384216,0.27941447496414185,0.1319257616996765,0.4106140434741974,0.07868245244026184,0.17792394757270813,0.3120405972003937,0.28092846274375916,0.12648910284042358,0.23754766583442688,0.15667815506458282,0.5647699236869812,0.16552922129631042,0.2373007833957672,0.11501895636320114,0.18473681807518005,0.19741016626358032,0.17906694114208221,0.13300096988677979,0.43115872144699097,0.07331027090549469,0.2398657649755478,0.2785203754901886,0.21286913752555847,0.17608557641506195,0.07380152493715286,0.16315682232379913,0.23038797080516815,0.30616527795791626,0.8584372997283936,0.16344444453716278,0.44214293360710144,0.23010660707950592,0.7845783233642578,0.11667023599147797,0.26098793745040894,0.16826416552066803,0.5971802473068237,0.07366889715194702,0.5687891244888306,0.7785646915435791,0.2559049129486084,0.5632057189941406,0.17704270780086517,0.4649452567100525,0.06366779655218124,0.18247348070144653,0.11925505846738815,0.15450382232666016,0.15300573408603668,0.25718629360198975,0.07444695383310318,0.3699156641960144,0.19024920463562012,0.20511004328727722,0.4901719391345978,0.32873812317848206,0.16681016981601715,0.324737548828125,0.08882948756217957,0.186372771859169,0.3106207847595215,0.3811393082141876,0.4250175952911377,0.225916787981987,0.08613798767328262,0.5062373280525208,0.1530395895242691,0.2015342116355896,0.1753070205450058,0.10995105654001236,0.3594485819339752,0.49178528785705566,0.23321178555488586,0.06062154844403267,0.8280954957008362,0.5466729402542114,0.8189460039138794,0.3877585828304291,0.9436032176017761,0.13419799506664276,0.258841872215271,0.15172553062438965,0.07895205914974213,0.09060396254062653,0.6447064876556396,0.2742381989955902,0.5908106565475464,0.46118396520614624,0.5823273658752441,0.4013635814189911,0.09274660050868988,0.19960276782512665,0.20291875302791595,0.3165924847126007,0.7219047546386719,0.6964389681816101,0.13980446755886078,0.5715582966804504,0.5912068486213684,0.7384056448936462,0.14856725931167603,0.942653477191925,0.13202565908432007,0.09573981910943985,0.33610257506370544,0.13686025142669678,0.2774834930896759,0.05020726099610329,0.5686115622520447,0.5828368663787842,0.17543818056583405,0.8771880269050598,0.6716324090957642,0.14591506123542786,0.5900397300720215,0.16796375811100006,0.15294906497001648,0.6433587670326233,0.2593724727630615,0.17892128229141235,0.4589049816131592,0.587770938873291,0.19211435317993164,0.07427383214235306,0.6946597695350647,0.5577563047409058,0.554751455783844,0.07825659215450287,0.5000132918357849,0.11317851394414902,0.0297255702316761,0.4179264307022095,0.365376353263855,0.617295503616333,0.40748172998428345,0.17919403314590454,0.12406499683856964,0.14152683317661285,0.17597073316574097,0.0908680111169815,0.13177242875099182,0.12013812363147736,0.1772312968969345,0.16621460020542145,0.17275552451610565,0.06365722417831421,0.2013227343559265,0.8164985775947571,0.47828373312950134,0.4720100462436676,0.06753641366958618,0.14508260786533356,0.13884180784225464,0.15650299191474915,0.2323915809392929,0.10840369015932083,0.33533230423927307,0.17104464769363403,0.3450629711151123,0.16013754904270172,0.2006261646747589,0.10575645416975021,0.18779949843883514,0.07108305394649506,0.590715229511261,0.538155734539032,0.23157991468906403,0.14717470109462738,0.12666422128677368,0.7448309659957886,0.17028753459453583,0.26403117179870605,0.22347453236579895,0.49364498257637024,0.1373254358768463,0.3213200867176056,0.1675998717546463,0.1769922971725464,0.048148367553949356,0.6793933510780334,0.19664622843265533,0.5643523335456848,0.190149188041687,0.15633773803710938,0.8633370399475098,0.10593139380216599,0.053160153329372406,0.25527775287628174,0.09830053150653839,0.14853405952453613,0.15660592913627625,0.2612641155719757,0.18360809981822968,0.28951284289360046,0.22087451815605164,0.8423928618431091,0.25359395146369934,0.19391264021396637,0.23105615377426147,0.24644456803798676,0.33235082030296326,0.14303545653820038,0.8972945213317871,0.0354958102107048,0.43880122900009155,0.6541925668716431,0.21250763535499573,0.2785411477088928,0.21814334392547607,0.24006696045398712,0.3031580150127411,0.21683108806610107,0.09886190295219421,0.8388686180114746,0.15719012916088104,0.39855802059173584,0.08077669888734818,0.6660689115524292,0.13086989521980286,0.11248267441987991,0.5013366937637329,0.13259391486644745],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('531a0cc9-d63d-4fe3-ab33-ade9d703e89a');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.line(val_losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random val samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=ground_truth<br>vehicle_count=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "ground_truth",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "ground_truth",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          102,
          200,
          33,
          59,
          222,
          29,
          9,
          17,
          346,
          55,
          8,
          7,
          5,
          55,
          16,
          55,
          26,
          63,
          209,
          52,
          18,
          12,
          102,
          13,
          140,
          25,
          132,
          14,
          379,
          210,
          302,
          46,
          271,
          222,
          13,
          140,
          30,
          252,
          60,
          30,
          54,
          153,
          103,
          15,
          39,
          337,
          30,
          302,
          9,
          305,
          108,
          111,
          290,
          27,
          97,
          17,
          5,
          316,
          364,
          133,
          91,
          331,
          8,
          15,
          238,
          48,
          304,
          174,
          32,
          165,
          25,
          379,
          91,
          111,
          109,
          111,
          220,
          278,
          170,
          302,
          8,
          84,
          174,
          228,
          227,
          121,
          132,
          236,
          152,
          55,
          7,
          278,
          38,
          92,
          39,
          282,
          401,
          18,
          379,
          104
         ],
         "xaxis": "x",
         "y": [
          18242.85546875,
          18114.013671875,
          6335.06298828125,
          18114.013671875,
          18242.85546875,
          18114.013671875,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          18242.85546875,
          18114.013671875,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          18114.013671875,
          6335.06298828125,
          18114.013671875,
          18114.013671875,
          18114.013671875,
          18242.85546875,
          18242.85546875,
          18114.013671875,
          18114.013671875,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          18114.013671875,
          18242.85546875,
          18114.013671875,
          18242.85546875,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          18114.013671875,
          18242.85546875,
          6335.06298828125,
          18114.013671875,
          18114.013671875,
          18242.85546875,
          6335.06298828125,
          6335.06298828125,
          18114.013671875,
          18242.85546875,
          18114.013671875,
          6335.06298828125,
          18114.013671875,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          18114.013671875,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          18114.013671875,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          18114.013671875,
          18242.85546875,
          18114.013671875,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          18114.013671875,
          18114.013671875,
          18114.013671875,
          6335.06298828125,
          18114.013671875,
          18242.85546875,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          18114.013671875,
          18242.85546875,
          18114.013671875,
          6335.06298828125,
          18114.013671875,
          18242.85546875,
          18114.013671875,
          18242.85546875,
          18114.013671875
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=predictions<br>vehicle_count=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "predictions",
         "marker": {
          "color": "#EF553B",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "predictions",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          102,
          200,
          33,
          59,
          222,
          29,
          9,
          17,
          346,
          55,
          8,
          7,
          5,
          55,
          16,
          55,
          26,
          63,
          209,
          52,
          18,
          12,
          102,
          13,
          140,
          25,
          132,
          14,
          379,
          210,
          302,
          46,
          271,
          222,
          13,
          140,
          30,
          252,
          60,
          30,
          54,
          153,
          103,
          15,
          39,
          337,
          30,
          302,
          9,
          305,
          108,
          111,
          290,
          27,
          97,
          17,
          5,
          316,
          364,
          133,
          91,
          331,
          8,
          15,
          238,
          48,
          304,
          174,
          32,
          165,
          25,
          379,
          91,
          111,
          109,
          111,
          220,
          278,
          170,
          302,
          8,
          84,
          174,
          228,
          227,
          121,
          132,
          236,
          152,
          55,
          7,
          278,
          38,
          92,
          39,
          282,
          401,
          18,
          379,
          104
         ],
         "xaxis": "x",
         "y": [
          13551.04296875,
          16890.658203125,
          8873.6884765625,
          13478.548828125,
          17273.0625,
          15471.5673828125,
          14416.705078125,
          15979.5556640625,
          17136.9453125,
          11267.3193359375,
          8544.8203125,
          11671.6748046875,
          9391.568359375,
          11267.3193359375,
          9604.8466796875,
          11267.3193359375,
          7415.91162109375,
          10357.76171875,
          15726.25,
          10752.0830078125,
          15641.044921875,
          11523.1376953125,
          9798.2705078125,
          9938.3779296875,
          14738.0546875,
          10554.6513671875,
          13919.171875,
          15814.8818359375,
          16504.955078125,
          15390.80859375,
          15339.7890625,
          15092.412109375,
          14952.8740234375,
          7166.064453125,
          10371.5439453125,
          15166.1748046875,
          8675.1552734375,
          13863.3486328125,
          11852.0166015625,
          14257.2548828125,
          15225.0107421875,
          13050.8154296875,
          16439.33203125,
          7187.98291015625,
          14566.2958984375,
          15557.96875,
          14257.2548828125,
          15339.7890625,
          14416.705078125,
          15171.8154296875,
          14027.7314453125,
          10639.0625,
          14634.0517578125,
          15459.5341796875,
          14513.7001953125,
          7230.68798828125,
          9391.568359375,
          16014.7998046875,
          16033.5771484375,
          13947.2412109375,
          9972.3662109375,
          16516.1171875,
          8544.8203125,
          7187.98291015625,
          15261.162109375,
          10028.2421875,
          14973.8447265625,
          14817.1220703125,
          10410.78125,
          12712.3876953125,
          10554.6513671875,
          16504.955078125,
          9972.3662109375,
          10639.0625,
          12125.4365234375,
          10639.0625,
          15422.8642578125,
          16316.7802734375,
          14307.626953125,
          15339.7890625,
          12306.83203125,
          14707.1650390625,
          14530.3486328125,
          16379.78125,
          16198.33984375,
          10170.3154296875,
          13919.171875,
          15164.9443359375,
          9440.3330078125,
          11267.3193359375,
          12923.3232421875,
          15485.9853515625,
          14541.0380859375,
          12568.1318359375,
          10080.3955078125,
          14907.97265625,
          16946.09765625,
          15641.044921875,
          16263.2626953125,
          14873.03125
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "vehicle_count"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"4639f18c-7248-40c7-a6cd-9452562ac755\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4639f18c-7248-40c7-a6cd-9452562ac755\")) {                    Plotly.newPlot(                        \"4639f18c-7248-40c7-a6cd-9452562ac755\",                        [{\"hovertemplate\":\"variable=ground_truth<br>vehicle_count=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"ground_truth\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"ground_truth\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[102.0,200.0,33.0,59.0,222.0,29.0,9.0,17.0,346.0,55.0,8.0,7.0,5.0,55.0,16.0,55.0,26.0,63.0,209.0,52.0,18.0,12.0,102.0,13.0,140.0,25.0,132.0,14.0,379.0,210.0,302.0,46.0,271.0,222.0,13.0,140.0,30.0,252.0,60.0,30.0,54.0,153.0,103.0,15.0,39.0,337.0,30.0,302.0,9.0,305.0,108.0,111.0,290.0,27.0,97.0,17.0,5.0,316.0,364.0,133.0,91.0,331.0,8.0,15.0,238.0,48.0,304.0,174.0,32.0,165.0,25.0,379.0,91.0,111.0,109.0,111.0,220.0,278.0,170.0,302.0,8.0,84.0,174.0,228.0,227.0,121.0,132.0,236.0,152.0,55.0,7.0,278.0,38.0,92.0,39.0,282.0,401.0,18.0,379.0,104.0],\"xaxis\":\"x\",\"y\":[18242.85546875,18114.013671875,6335.06298828125,18114.013671875,18242.85546875,18114.013671875,18242.85546875,18242.85546875,18242.85546875,6335.06298828125,6335.06298828125,6335.06298828125,6335.06298828125,6335.06298828125,6335.06298828125,6335.06298828125,6335.06298828125,6335.06298828125,18242.85546875,18242.85546875,18114.013671875,6335.06298828125,6335.06298828125,6335.06298828125,18114.013671875,6335.06298828125,18114.013671875,18114.013671875,18114.013671875,18242.85546875,18242.85546875,18114.013671875,18114.013671875,6335.06298828125,6335.06298828125,18242.85546875,6335.06298828125,18242.85546875,18114.013671875,18242.85546875,18114.013671875,18242.85546875,18242.85546875,6335.06298828125,18242.85546875,18242.85546875,18242.85546875,18242.85546875,18242.85546875,18114.013671875,18242.85546875,6335.06298828125,18114.013671875,18114.013671875,18242.85546875,6335.06298828125,6335.06298828125,18114.013671875,18242.85546875,18114.013671875,6335.06298828125,18114.013671875,6335.06298828125,6335.06298828125,18242.85546875,6335.06298828125,18242.85546875,18114.013671875,6335.06298828125,18242.85546875,6335.06298828125,18114.013671875,6335.06298828125,6335.06298828125,18242.85546875,6335.06298828125,18114.013671875,18242.85546875,18114.013671875,18242.85546875,6335.06298828125,18242.85546875,18114.013671875,18114.013671875,18114.013671875,6335.06298828125,18114.013671875,18242.85546875,6335.06298828125,6335.06298828125,18242.85546875,18114.013671875,18242.85546875,18114.013671875,6335.06298828125,18114.013671875,18242.85546875,18114.013671875,18242.85546875,18114.013671875],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=predictions<br>vehicle_count=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"predictions\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"predictions\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[102.0,200.0,33.0,59.0,222.0,29.0,9.0,17.0,346.0,55.0,8.0,7.0,5.0,55.0,16.0,55.0,26.0,63.0,209.0,52.0,18.0,12.0,102.0,13.0,140.0,25.0,132.0,14.0,379.0,210.0,302.0,46.0,271.0,222.0,13.0,140.0,30.0,252.0,60.0,30.0,54.0,153.0,103.0,15.0,39.0,337.0,30.0,302.0,9.0,305.0,108.0,111.0,290.0,27.0,97.0,17.0,5.0,316.0,364.0,133.0,91.0,331.0,8.0,15.0,238.0,48.0,304.0,174.0,32.0,165.0,25.0,379.0,91.0,111.0,109.0,111.0,220.0,278.0,170.0,302.0,8.0,84.0,174.0,228.0,227.0,121.0,132.0,236.0,152.0,55.0,7.0,278.0,38.0,92.0,39.0,282.0,401.0,18.0,379.0,104.0],\"xaxis\":\"x\",\"y\":[13551.04296875,16890.658203125,8873.6884765625,13478.548828125,17273.0625,15471.5673828125,14416.705078125,15979.5556640625,17136.9453125,11267.3193359375,8544.8203125,11671.6748046875,9391.568359375,11267.3193359375,9604.8466796875,11267.3193359375,7415.91162109375,10357.76171875,15726.25,10752.0830078125,15641.044921875,11523.1376953125,9798.2705078125,9938.3779296875,14738.0546875,10554.6513671875,13919.171875,15814.8818359375,16504.955078125,15390.80859375,15339.7890625,15092.412109375,14952.8740234375,7166.064453125,10371.5439453125,15166.1748046875,8675.1552734375,13863.3486328125,11852.0166015625,14257.2548828125,15225.0107421875,13050.8154296875,16439.33203125,7187.98291015625,14566.2958984375,15557.96875,14257.2548828125,15339.7890625,14416.705078125,15171.8154296875,14027.7314453125,10639.0625,14634.0517578125,15459.5341796875,14513.7001953125,7230.68798828125,9391.568359375,16014.7998046875,16033.5771484375,13947.2412109375,9972.3662109375,16516.1171875,8544.8203125,7187.98291015625,15261.162109375,10028.2421875,14973.8447265625,14817.1220703125,10410.78125,12712.3876953125,10554.6513671875,16504.955078125,9972.3662109375,10639.0625,12125.4365234375,10639.0625,15422.8642578125,16316.7802734375,14307.626953125,15339.7890625,12306.83203125,14707.1650390625,14530.3486328125,16379.78125,16198.33984375,10170.3154296875,13919.171875,15164.9443359375,9440.3330078125,11267.3193359375,12923.3232421875,15485.9853515625,14541.0380859375,12568.1318359375,10080.3955078125,14907.97265625,16946.09765625,15641.044921875,16263.2626953125,14873.03125],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"vehicle_count\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('4639f18c-7248-40c7-a6cd-9452562ac755');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "ground_truth = []\n",
    "vehicle_counts = []\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    random_idx = np.random.randint(0,len(val_data))\n",
    "    x, y = val_data[random_idx]\n",
    "    vehicle_counts.append(x[0])\n",
    "    ground_truth.append(float(y))\n",
    "    pred_y = float(nn_model(x)[0])\n",
    "    preds.append(pred_y)\n",
    "\n",
    "df = pd.DataFrame({'vehicle_count': vehicle_counts, 'ground_truth': ground_truth, 'predictions': preds})\n",
    "\n",
    "px.scatter(df, x='vehicle_count', y=['ground_truth', 'predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nn_model.state_dict(), NN_MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
