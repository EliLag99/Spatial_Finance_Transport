{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AADT Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as album\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import plotly.express as px\n",
    "import torchmetrics\n",
    "from torchmetrics import MeanAbsolutePercentageError\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR_PATH = os.path.abspath('..')\n",
    "\n",
    "IMG_SIZE = 1024\n",
    "VEHICLE_DETECTION_COUNT_PATH = os.path.join(ROOT_DIR_PATH, 'data/vehicle_counts_detection.csv')\n",
    "AADT_PROCESSED_PATH = os.path.join(ROOT_DIR_PATH, 'data/ground_truth_data/aadt_processed.csv')\n",
    "\n",
    "NN_MODEL_PATH = os.path.join(ROOT_DIR_PATH, \"models/nn_aadt_model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Count Data\n",
    "- Traffic monitoring stations for long-term traffic count data\n",
    "    - Extract at same time as Satellite Image!\n",
    "- How to use permanent and temporary traffic count stations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle detection number\n",
    "From vehicle detection model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road characteristics\n",
    "From road characterstics pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Includes:\n",
    "- Road width\n",
    "- Live speed data\n",
    "- Directionality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.labels = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['aadt'].values.astype('float32'))\n",
    "        self.vehicle_count = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['total_volume'].values.astype('float32')).unsqueeze(1) # Training\n",
    "        #self.speed_data = pd.read_csv(SPEED_DATA_PATH) \n",
    "        #self.road_width = pd.read_csv(ROAD_WIDTH_PATH)\n",
    "        self.hour = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['hour'].values.astype('float32')).unsqueeze(1)\n",
    "        self.avg_mph = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['avg_mph'].values.astype('float32')).unsqueeze(1)\n",
    "        self.day = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['day'].values.astype('float32')).unsqueeze(1)\n",
    "        self.month = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['month'].values.astype('float32')).unsqueeze(1)\n",
    "        self.small_vehicle = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['0-520cm'].values.astype('float32')).unsqueeze(1)\n",
    "        self.mid_vehicle = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['521-660cm'].values.astype('float32')).unsqueeze(1)\n",
    "        self.large_vehicle = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['661-1160cm'].values.astype('float32')).unsqueeze(1)\n",
    "        self.very_large_vehicle = torch.tensor(pd.read_csv(AADT_PROCESSED_PATH)['1160+cm'].values.astype('float32')).unsqueeze(1)\n",
    "\n",
    "        self.x = torch.concat((self.vehicle_count, self.small_vehicle, self.mid_vehicle, self.large_vehicle, self.very_large_vehicle, self.avg_mph, self.day, self.month, self.hour), dim=-1)\n",
    "        self.y = self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "custom_data = CustomDataset()\n",
    "train_split = 0.8\n",
    "train_data, val_data = random_split(custom_data, [train_split, 1-train_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(9, 9),\n",
    "            nn.Linear(9,20),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(20,20),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(20,1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (1): Linear(in_features=9, out_features=20, bias=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Dropout(p=0.2, inplace=False)\n",
       "    (7): Linear(in_features=20, out_features=1, bias=True)\n",
       "    (8): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = NeuralNetwork()\n",
    "nn_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopper(patience=3, min_delta=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (1): Linear(in_features=9, out_features=20, bias=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Dropout(p=0.2, inplace=False)\n",
       "    (7): Linear(in_features=20, out_features=1, bias=True)\n",
       "    (8): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "nn_model.apply(init_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "batch_size = 1\n",
    "epochs = 1\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "MAPE = MeanAbsolutePercentageError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False, sampler=None,\n",
    "                    batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "                    pin_memory=False, drop_last=False, timeout=0,\n",
    "                    worker_init_fn=None, prefetch_factor=2,\n",
    "                    persistent_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False, sampler=None,\n",
    "                    batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "                    pin_memory=False, drop_last=False, timeout=0,\n",
    "                    worker_init_fn=None, prefetch_factor=2,\n",
    "                    persistent_workers=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(ep_id, action, loader, model, optimizer, criterion):\n",
    "    losses = [] # Keep list of accuracies to track progress\n",
    "    is_training = action == \"train\" # True when action == \"train\", else False \n",
    "\n",
    "    # Looping over all batches\n",
    "    for batch_idx, batch in enumerate(loader): \n",
    "        x, y = batch\n",
    "\n",
    "        # Resetting the optimizer gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Setting model to train or test\n",
    "        with torch.set_grad_enabled(is_training):\n",
    "            \n",
    "            # Feed batch to model\n",
    "            logits = nn_model(x).squeeze(1)\n",
    "            print(\"logits: {}\".format(logits))\n",
    "\n",
    "            # Calculate the loss based on predictions and real labels\n",
    "            loss = criterion(logits, y)\n",
    "            mape_loss = MAPE(logits, y)\n",
    "            print(\"MAPE loss: {}\".format(mape_loss))\n",
    "\n",
    "            # If training, perform backprop and update weights\n",
    "            if is_training:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Append current batch accuracy\n",
    "            losses.append(mape_loss.detach().numpy())\n",
    "\n",
    "            # Print some stats every 50th batch \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"{action.capitalize()}ing, Epoch: {ep_id+1}, Batch {batch_idx}: Loss = {loss.item()}\")\n",
    "\n",
    "        if not is_training:\n",
    "            if early_stopper.early_stop(mape_loss.detach().numpy()):             \n",
    "                break\n",
    "                    \n",
    "    # Return accuracies to main loop                 \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(epochs, train_dl, val_dl, model, optimizer, criterion):\n",
    "\n",
    "    # Keep lists of accuracies to track performance on train and test sets\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Looping over epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Looping over train set and training\n",
    "        train_loss = run_epoch(epoch, \"train\", train_dl, model, optimizer, criterion)\n",
    "\n",
    "        # Looping over test set\n",
    "        val_loss = run_epoch(epoch, \"val\", val_dl, model, optimizer, criterion) \n",
    "\n",
    "        # Collecting stats\n",
    "        train_losses += train_loss\n",
    "        val_losses += val_loss         \n",
    "            \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([-0.1393], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0000219345092773\n",
      "Training, Epoch: 1, Batch 0: Loss = 40134788.0\n",
      "logits: tensor([152.4614], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9916427135467529\n",
      "logits: tensor([812.4283], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9554659724235535\n",
      "logits: tensor([107.7175], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9940953850746155\n",
      "logits: tensor([3304.3904], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8188666105270386\n",
      "logits: tensor([709.1791], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8880549073219299\n",
      "logits: tensor([5955.5957], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.059899527579545975\n",
      "logits: tensor([3301.3276], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47888004779815674\n",
      "logits: tensor([39050.9844], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1406179666519165\n",
      "logits: tensor([9956.1035], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4542464315891266\n",
      "logits: tensor([7159.0039], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13006041944026947\n",
      "logits: tensor([17857.5176], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.021122673526406288\n",
      "logits: tensor([9793.5186], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5459228157997131\n",
      "logits: tensor([8043.9819], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.26975563168525696\n",
      "logits: tensor([10899.7910], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7205497622489929\n",
      "logits: tensor([21113.3652], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15734981000423431\n",
      "logits: tensor([1852.1332], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7076377272605896\n",
      "logits: tensor([2279.0354], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8750724792480469\n",
      "logits: tensor([4974.1040], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7273396253585815\n",
      "logits: tensor([4981.2339], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7269487380981445\n",
      "logits: tensor([6506.3325], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.027035174891352654\n",
      "logits: tensor([9406.1035], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48439520597457886\n",
      "logits: tensor([1914.8529], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6977373361587524\n",
      "logits: tensor([3102.8516], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.510209858417511\n",
      "logits: tensor([2038.6317], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6781986355781555\n",
      "logits: tensor([5759.7227], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6842751502990723\n",
      "logits: tensor([2555.8784], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5965504050254822\n",
      "logits: tensor([10815.3389], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7072188258171082\n",
      "logits: tensor([4033.6807], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.36327695846557617\n",
      "logits: tensor([9407.7207], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48502400517463684\n",
      "logits: tensor([11305.3389], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3802867829799652\n",
      "logits: tensor([11097.9707], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3916538655757904\n",
      "logits: tensor([54794.3164], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 2.0036041736602783\n",
      "logits: tensor([4345.1538], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3141103982925415\n",
      "logits: tensor([26512.3516], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4533005356788635\n",
      "logits: tensor([8536.3359], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5320723652839661\n",
      "logits: tensor([3118.4956], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5077403783798218\n",
      "logits: tensor([9121.2959], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5000072121620178\n",
      "logits: tensor([10003.0928], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.45167067646980286\n",
      "logits: tensor([3136.4744], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5049024224281311\n",
      "logits: tensor([3606.4919], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4307093918323517\n",
      "logits: tensor([7620.7852], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5822591781616211\n",
      "logits: tensor([541.6412], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9145010709762573\n",
      "logits: tensor([1304.3147], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7941117882728577\n",
      "logits: tensor([4365.6401], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7606931328773499\n",
      "logits: tensor([3169.4766], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8262620568275452\n",
      "logits: tensor([2585.5425], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5918679237365723\n",
      "logits: tensor([1356.5624], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7858644127845764\n",
      "logits: tensor([2164.0649], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6583988070487976\n",
      "logits: tensor([6309.6289], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.004014811012893915\n",
      "logits: tensor([11381.5645], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3761083781719208\n",
      "Training, Epoch: 1, Batch 50: Loss = 47077316.0\n",
      "logits: tensor([6342.8062], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6523128151893616\n",
      "logits: tensor([5942.2666], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6742688417434692\n",
      "logits: tensor([3549.9534], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4396340847015381\n",
      "logits: tensor([13023.5166], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0557832717895508\n",
      "logits: tensor([5419.0005], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1446019560098648\n",
      "logits: tensor([7352.3589], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5969732403755188\n",
      "logits: tensor([23695.8770], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2989127039909363\n",
      "logits: tensor([56345.9492], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 2.088658332824707\n",
      "logits: tensor([3391.2500], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.46468567848205566\n",
      "logits: tensor([12654.4873], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9975314140319824\n",
      "logits: tensor([9017.6357], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.42344847321510315\n",
      "logits: tensor([12543.9580], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.31239065527915955\n",
      "logits: tensor([2743.1355], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5669916272163391\n",
      "logits: tensor([1311.7312], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9280962347984314\n",
      "logits: tensor([2820.3088], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8454020023345947\n",
      "logits: tensor([3119.4961], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.829001784324646\n",
      "logits: tensor([5019.1021], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7248730063438416\n",
      "logits: tensor([2559.1003], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8597204089164734\n",
      "logits: tensor([5821.6699], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6808794736862183\n",
      "logits: tensor([1986.8373], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6863744854927063\n",
      "logits: tensor([593.5990], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9062994122505188\n",
      "logits: tensor([3322.5725], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47552651166915894\n",
      "logits: tensor([1401.5278], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9231739044189453\n",
      "logits: tensor([11356.1709], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.37750035524368286\n",
      "logits: tensor([9263.7109], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4922006130218506\n",
      "logits: tensor([4320.2197], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.31804630160331726\n",
      "logits: tensor([10906.4883], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.40215015411376953\n",
      "logits: tensor([13183.7500], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2773198187351227\n",
      "logits: tensor([35900.3359], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9679120779037476\n",
      "logits: tensor([15402.8906], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15567545592784882\n",
      "logits: tensor([10806.6367], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7058451771736145\n",
      "logits: tensor([2678.2869], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5772280693054199\n",
      "logits: tensor([21184.4844], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16124826669692993\n",
      "logits: tensor([8179.3638], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5516402125358582\n",
      "logits: tensor([13692.6738], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24942266941070557\n",
      "logits: tensor([2878.6785], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5455959439277649\n",
      "logits: tensor([24116.4785], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3219684064388275\n",
      "logits: tensor([7321.4321], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5986685752868652\n",
      "logits: tensor([3376.9761], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4669388234615326\n",
      "logits: tensor([7241.4438], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1430736929178238\n",
      "logits: tensor([33487.5156], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8356509804725647\n",
      "logits: tensor([7104.7861], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6105442047119141\n",
      "logits: tensor([2153.0298], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8819795846939087\n",
      "logits: tensor([5169.4546], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1839931756258011\n",
      "logits: tensor([8583.1455], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5295064449310303\n",
      "logits: tensor([3968.9397], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.373496413230896\n",
      "logits: tensor([2382.3652], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6239397525787354\n",
      "logits: tensor([1659.3313], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7380717992782593\n",
      "logits: tensor([3379.6262], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4665204882621765\n",
      "logits: tensor([2798.2778], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8466097116470337\n",
      "Training, Epoch: 1, Batch 100: Loss = 238534992.0\n",
      "logits: tensor([4539.6426], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28341004252433777\n",
      "logits: tensor([2330.6812], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6320981979370117\n",
      "logits: tensor([3137.2402], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5047815442085266\n",
      "logits: tensor([7420.3931], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5932438969612122\n",
      "logits: tensor([2689.2295], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5755007266998291\n",
      "logits: tensor([14513.2090], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20444422960281372\n",
      "logits: tensor([6783.3169], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07075760513544083\n",
      "logits: tensor([6377.2441], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.650425136089325\n",
      "logits: tensor([6370.9717], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6507689356803894\n",
      "logits: tensor([14456.6123], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.2819998264312744\n",
      "logits: tensor([12014.1611], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8964548707008362\n",
      "logits: tensor([3681.1887], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7982120513916016\n",
      "logits: tensor([4759.2534], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24874410033226013\n",
      "logits: tensor([28205.2891], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5461005568504333\n",
      "logits: tensor([3466.7903], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4527615010738373\n",
      "logits: tensor([12125.2383], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9139885902404785\n",
      "logits: tensor([18115.6133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.006974905263632536\n",
      "logits: tensor([10619.6797], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6763337254524231\n",
      "logits: tensor([5480.1421], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.134950652718544\n",
      "logits: tensor([4184.9229], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.770599365234375\n",
      "logits: tensor([11759.1191], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3554123640060425\n",
      "logits: tensor([8454.7666], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5365436673164368\n",
      "logits: tensor([6630.6177], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04665378853678703\n",
      "logits: tensor([13403.6797], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2652641534805298\n",
      "logits: tensor([5979.5820], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05611324682831764\n",
      "logits: tensor([9820.6133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.46167346835136414\n",
      "logits: tensor([15416.0029], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.4334410429000854\n",
      "logits: tensor([19583.8594], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07350844144821167\n",
      "logits: tensor([15012.6768], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1770654171705246\n",
      "logits: tensor([7549.9585], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5861415863037109\n",
      "logits: tensor([20554.0527], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12669053673744202\n",
      "logits: tensor([7948.6655], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5642860531806946\n",
      "logits: tensor([551.7935], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9128984808921814\n",
      "logits: tensor([8199.5000], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29430443048477173\n",
      "logits: tensor([14668.7373], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19591878354549408\n",
      "logits: tensor([11565.0371], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8255599141120911\n",
      "logits: tensor([30566.3496], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6755244135856628\n",
      "logits: tensor([36618.5469], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0072815418243408\n",
      "logits: tensor([14756.2334], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1911226063966751\n",
      "logits: tensor([8244.0400], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3013351261615753\n",
      "logits: tensor([5247.6235], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7123463749885559\n",
      "logits: tensor([4927.4531], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22219350934028625\n",
      "logits: tensor([2512.7844], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6033529043197632\n",
      "logits: tensor([1840.8727], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8990907669067383\n",
      "logits: tensor([-55.3918], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.003036379814148\n",
      "logits: tensor([4872.6782], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2308398187160492\n",
      "logits: tensor([-195.0771], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0106933116912842\n",
      "logits: tensor([1392.0139], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7802683115005493\n",
      "logits: tensor([-43.5508], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.002387285232544\n",
      "logits: tensor([4711.7695], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7417197227478027\n",
      "Training, Epoch: 1, Batch 150: Loss = 183090288.0\n",
      "logits: tensor([1806.9948], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9009478688240051\n",
      "logits: tensor([1934.5857], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6946225166320801\n",
      "logits: tensor([1945.3168], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6929285526275635\n",
      "logits: tensor([4025.3477], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7793466448783875\n",
      "logits: tensor([2212.9673], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6506795287132263\n",
      "logits: tensor([1686.9780], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7337077856063843\n",
      "logits: tensor([1307.0385], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9283533692359924\n",
      "logits: tensor([7359.5679], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.596578061580658\n",
      "logits: tensor([3483.8232], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8090308308601379\n",
      "logits: tensor([4819.7935], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23918776214122772\n",
      "logits: tensor([8125.9258], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2826906144618988\n",
      "logits: tensor([5819.2046], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08142908662557602\n",
      "logits: tensor([9071.3008], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5027477741241455\n",
      "logits: tensor([13960.5254], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23474012315273285\n",
      "logits: tensor([26845.0879], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47153979539871216\n",
      "logits: tensor([16148.4121], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.5490531921386719\n",
      "logits: tensor([10925.3535], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4011160433292389\n",
      "logits: tensor([27798.8770], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5238226652145386\n",
      "logits: tensor([9054.7656], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5036541223526001\n",
      "logits: tensor([5457.1050], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1385871022939682\n",
      "logits: tensor([4759.9985], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7390760183334351\n",
      "logits: tensor([9569.2373], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5105196833610535\n",
      "logits: tensor([7284.3267], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14984281361103058\n",
      "logits: tensor([6700.6206], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6326988935470581\n",
      "logits: tensor([8897.8311], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5122566819190979\n",
      "logits: tensor([12127.9590], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.33519405126571655\n",
      "logits: tensor([12256.6143], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9347264766693115\n",
      "logits: tensor([8859.8213], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.39853721857070923\n",
      "logits: tensor([8588.7363], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.52920001745224\n",
      "logits: tensor([16431.4824], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09929218888282776\n",
      "logits: tensor([15918.4248], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.512749195098877\n",
      "logits: tensor([15385.8799], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15660791099071503\n",
      "logits: tensor([15874.5146], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12982292473316193\n",
      "logits: tensor([22617.6523], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23980878293514252\n",
      "logits: tensor([17949.3047], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.833327054977417\n",
      "logits: tensor([10287.4219], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6238862872123718\n",
      "logits: tensor([19676.4727], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07858513295650482\n",
      "logits: tensor([24310.3496], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3325956463813782\n",
      "logits: tensor([6671.3403], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.053081925958395004\n",
      "logits: tensor([9706.8848], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4679076075553894\n",
      "logits: tensor([11535.2803], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3676823079586029\n",
      "logits: tensor([8303.2432], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5448495745658875\n",
      "logits: tensor([12625.8799], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30790001153945923\n",
      "logits: tensor([7869.5640], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24222347140312195\n",
      "logits: tensor([3397.3462], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.463723361492157\n",
      "logits: tensor([5707.7505], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09902229905128479\n",
      "logits: tensor([7897.9272], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24670065939426422\n",
      "logits: tensor([8829.2012], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5160186886787415\n",
      "logits: tensor([4556.4248], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7502350807189941\n",
      "logits: tensor([11643.0283], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3617759943008423\n",
      "Training, Epoch: 1, Batch 200: Loss = 43557720.0\n",
      "logits: tensor([2870.5623], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5468770861625671\n",
      "logits: tensor([15562.0488], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.4564948081970215\n",
      "logits: tensor([11156.2070], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7610254287719727\n",
      "logits: tensor([16552.0977], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.6127755641937256\n",
      "logits: tensor([18919.8145], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0371081717312336\n",
      "logits: tensor([17409.6172], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.045674774795770645\n",
      "logits: tensor([24831.2246], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3611479103565216\n",
      "logits: tensor([5498.1128], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.698615550994873\n",
      "logits: tensor([5388.8706], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7046037316322327\n",
      "logits: tensor([8079.0269], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5571401715278625\n",
      "logits: tensor([13831.6562], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24180421233177185\n",
      "logits: tensor([5959.8892], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05922180041670799\n",
      "logits: tensor([8381.9443], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.32310354709625244\n",
      "logits: tensor([8600.9570], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5285301208496094\n",
      "logits: tensor([14882.4385], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18420454859733582\n",
      "logits: tensor([13959.6689], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.2035564184188843\n",
      "logits: tensor([13181.8721], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2774227559566498\n",
      "logits: tensor([27230.6152], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.492672860622406\n",
      "logits: tensor([13329.9795], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1041589975357056\n",
      "logits: tensor([7870.2949], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5685820579528809\n",
      "logits: tensor([16192.3623], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.5559906959533691\n",
      "logits: tensor([16187.9102], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11264383792877197\n",
      "logits: tensor([7256.2056], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14540386199951172\n",
      "logits: tensor([3480.6016], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8092074394226074\n",
      "logits: tensor([12143.4033], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3343474566936493\n",
      "logits: tensor([10781.1162], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.701816737651825\n",
      "logits: tensor([6275.4155], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.009415448643267155\n",
      "logits: tensor([5567.9409], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.694787859916687\n",
      "logits: tensor([14029.0225], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2309853881597519\n",
      "logits: tensor([5230.8774], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7132643461227417\n",
      "logits: tensor([14260.0928], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21831904351711273\n",
      "logits: tensor([6853.6992], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6243077516555786\n",
      "logits: tensor([13467.9473], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2617412805557251\n",
      "logits: tensor([12684.6787], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0022971630096436\n",
      "logits: tensor([9088.7900], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4346802830696106\n",
      "logits: tensor([29364.1641], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 3.635181188583374\n",
      "logits: tensor([11507.1807], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.36922261118888855\n",
      "logits: tensor([15871.0312], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1300138682126999\n",
      "logits: tensor([9503.4648], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5001373887062073\n",
      "logits: tensor([13635.0791], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1523194313049316\n",
      "logits: tensor([7750.4268], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2234174758195877\n",
      "logits: tensor([19471.0859], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06732665747404099\n",
      "logits: tensor([6502.3647], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.026408854871988297\n",
      "logits: tensor([4939.8003], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7292199730873108\n",
      "logits: tensor([3949.4724], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.37656936049461365\n",
      "logits: tensor([6640.7876], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6359787583351135\n",
      "logits: tensor([3870.1411], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7878544330596924\n",
      "logits: tensor([6196.6221], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6603260636329651\n",
      "logits: tensor([3042.5227], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5197328329086304\n",
      "logits: tensor([9740.0078], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.46609193086624146\n",
      "Training, Epoch: 1, Batch 250: Loss = 72298416.0\n",
      "logits: tensor([5438.1748], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14157526195049286\n",
      "logits: tensor([5977.2329], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05648405849933624\n",
      "logits: tensor([13633.3936], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2526721656322479\n",
      "logits: tensor([10395.8369], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.43014201521873474\n",
      "logits: tensor([5888.1743], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6772339940071106\n",
      "logits: tensor([18457.3867], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.011759740300476551\n",
      "logits: tensor([16148.3467], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.5490427017211914\n",
      "logits: tensor([17326.2617], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.050243984907865524\n",
      "logits: tensor([27343.5605], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4988640546798706\n",
      "logits: tensor([16745.8242], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08206123858690262\n",
      "logits: tensor([14663.6318], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.3146780729293823\n",
      "logits: tensor([22439.0273], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 2.542037010192871\n",
      "logits: tensor([9044.1221], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.42762938141822815\n",
      "logits: tensor([16390.9238], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10151544958353043\n",
      "logits: tensor([8944.5986], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4119194447994232\n",
      "logits: tensor([9939.6357], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5689876675605774\n",
      "logits: tensor([11386.4863], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.37583860754966736\n",
      "logits: tensor([13888.5869], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23868349194526672\n",
      "logits: tensor([5699.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10040357708930969\n",
      "logits: tensor([4449.5728], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7560923099517822\n",
      "logits: tensor([7876.9263], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5682185888290405\n",
      "logits: tensor([5124.5195], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19108624756336212\n",
      "logits: tensor([8047.3120], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.27028128504753113\n",
      "logits: tensor([6736.9219], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6307090520858765\n",
      "logits: tensor([5792.0532], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6825029253959656\n",
      "logits: tensor([10544.2568], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.422006219625473\n",
      "logits: tensor([22269.0938], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22070220112800598\n",
      "logits: tensor([21633.8633], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18588142096996307\n",
      "logits: tensor([10710.2695], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6906334757804871\n",
      "logits: tensor([13458.0068], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1243683099746704\n",
      "logits: tensor([16245.2588], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1095002219080925\n",
      "logits: tensor([21123.8906], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1579267680644989\n",
      "logits: tensor([15188.7695], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.3975720405578613\n",
      "logits: tensor([28246.0527], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.548335075378418\n",
      "logits: tensor([12761.6650], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0144495964050293\n",
      "logits: tensor([4859.3813], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.232938751578331\n",
      "logits: tensor([7731.0444], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5762152075767517\n",
      "logits: tensor([8612.4033], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5279026627540588\n",
      "logits: tensor([3710.2854], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.41432541608810425\n",
      "logits: tensor([3671.0630], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4205167293548584\n",
      "logits: tensor([1907.9463], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6988275647163391\n",
      "logits: tensor([2656.8809], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5806070566177368\n",
      "logits: tensor([7391.3901], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5948336720466614\n",
      "logits: tensor([2103.7366], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6679217219352722\n",
      "logits: tensor([6433.4336], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.015527959913015366\n",
      "logits: tensor([6470.1240], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6453338265419006\n",
      "logits: tensor([10063.4863], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4483601450920105\n",
      "logits: tensor([11659.7627], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8405125141143799\n",
      "logits: tensor([8125.2764], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28258809447288513\n",
      "logits: tensor([17780.4355], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.8066707849502563\n",
      "Training, Epoch: 1, Batch 300: Loss = 130996568.0\n",
      "logits: tensor([17607.9727], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.034801725298166275\n",
      "logits: tensor([2862.2373], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8431036472320557\n",
      "logits: tensor([5881.3984], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.071611687541008\n",
      "logits: tensor([2327.6636], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6325744986534119\n",
      "logits: tensor([11138.6660], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7582565546035767\n",
      "logits: tensor([12711.4473], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0065226554870605\n",
      "logits: tensor([3135.7288], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5050201416015625\n",
      "logits: tensor([11225.5010], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7719635963439941\n",
      "logits: tensor([9656.7510], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47065573930740356\n",
      "logits: tensor([9308.8828], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48972445726394653\n",
      "logits: tensor([5488.2256], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6991575360298157\n",
      "logits: tensor([7063.4785], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11498157680034637\n",
      "logits: tensor([9614.9365], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4729478359222412\n",
      "logits: tensor([8430.5488], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3307758569717407\n",
      "logits: tensor([10416.5879], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6442753672599792\n",
      "logits: tensor([5661.8794], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6896385550498962\n",
      "logits: tensor([22426.7793], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2293458878993988\n",
      "logits: tensor([18669.4062], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.023381799459457397\n",
      "logits: tensor([6189.3442], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.023001940920948982\n",
      "logits: tensor([5020.8896], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20744439959526062\n",
      "logits: tensor([13442.6797], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.26312634348869324\n",
      "logits: tensor([39322.8633], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.155521273612976\n",
      "logits: tensor([10245.3398], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.43839165568351746\n",
      "logits: tensor([5143.9941], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18801215291023254\n",
      "logits: tensor([3664.0232], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4216279685497284\n",
      "logits: tensor([4489.2861], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2913588881492615\n",
      "logits: tensor([14081.8789], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22808800637722015\n",
      "logits: tensor([8438.1113], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5374566316604614\n",
      "logits: tensor([8327.6016], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5435143709182739\n",
      "logits: tensor([2743.4978], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5669344067573547\n",
      "logits: tensor([3590.4985], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.43323397636413574\n",
      "logits: tensor([8020.4868], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.56034916639328\n",
      "logits: tensor([7936.1377], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2527322471141815\n",
      "logits: tensor([12715.4873], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30298811197280884\n",
      "logits: tensor([7622.8999], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.582143247127533\n",
      "logits: tensor([13192.8311], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0825098752975464\n",
      "logits: tensor([17028.5488], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06656340509653091\n",
      "logits: tensor([30878.6465], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6926432847976685\n",
      "logits: tensor([22080.2109], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21034839749336243\n",
      "logits: tensor([19234.5898], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05436289310455322\n",
      "logits: tensor([7637.3286], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20556475222110748\n",
      "logits: tensor([9975.2998], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.574617326259613\n",
      "logits: tensor([7563.0234], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1938355565071106\n",
      "logits: tensor([6083.7974], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.039662688970565796\n",
      "logits: tensor([8630.5459], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5269081592559814\n",
      "logits: tensor([7974.5737], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2587994337081909\n",
      "logits: tensor([5519.3472], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6974515318870544\n",
      "logits: tensor([4826.8594], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7354109883308411\n",
      "logits: tensor([7007.1045], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10608284175395966\n",
      "logits: tensor([4502.2891], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2893063426017761\n",
      "Training, Epoch: 1, Batch 350: Loss = 3359060.25\n",
      "logits: tensor([7082.7329], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1180209144949913\n",
      "logits: tensor([6109.3042], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03563639149069786\n",
      "logits: tensor([6646.3457], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6356740593910217\n",
      "logits: tensor([7352.6470], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5969575047492981\n",
      "logits: tensor([5758.5532], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09100300818681717\n",
      "logits: tensor([12489.4502], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3153785467147827\n",
      "logits: tensor([9436.2588], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4895288050174713\n",
      "logits: tensor([25493.0449], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3974262475967407\n",
      "logits: tensor([24643.8906], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3508790135383606\n",
      "logits: tensor([20490.7773], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12322203814983368\n",
      "logits: tensor([12547.3623], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3122040331363678\n",
      "logits: tensor([8204.7930], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2951399087905884\n",
      "logits: tensor([5916.6274], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06605073064565659\n",
      "logits: tensor([5156.7158], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18600401282310486\n",
      "logits: tensor([11825.7900], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3517577350139618\n",
      "logits: tensor([6993.3345], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10390922427177429\n",
      "logits: tensor([15130.4580], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17060911655426025\n",
      "logits: tensor([8205.6045], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2952680289745331\n",
      "logits: tensor([7859.4570], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24062807857990265\n",
      "logits: tensor([13880.8799], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2391059696674347\n",
      "logits: tensor([8455.8789], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3347742557525635\n",
      "logits: tensor([13857.2617], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24040061235427856\n",
      "logits: tensor([9782.4229], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5441713929176331\n",
      "logits: tensor([8712.2559], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5224291682243347\n",
      "logits: tensor([8552.9268], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.35009339451789856\n",
      "logits: tensor([13745.5889], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1697635650634766\n",
      "logits: tensor([12837.4854], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0264179706573486\n",
      "logits: tensor([7372.9102], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1638258695602417\n",
      "logits: tensor([21782.8438], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1940479278564453\n",
      "logits: tensor([17932.7383], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.016999376937747\n",
      "logits: tensor([9943.6709], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5696246027946472\n",
      "logits: tensor([6004.2827], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.052214205265045166\n",
      "logits: tensor([10517.9082], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4234505593776703\n",
      "logits: tensor([10925.8867], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.40108680725097656\n",
      "logits: tensor([11119.9521], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7553025484085083\n",
      "logits: tensor([7638.1816], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2056993991136551\n",
      "logits: tensor([15805.5898], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1336010992527008\n",
      "logits: tensor([2734.7131], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5683211088180542\n",
      "logits: tensor([6040.8179], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04644707217812538\n",
      "logits: tensor([5970.3472], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.057570986449718475\n",
      "logits: tensor([10847.6680], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4053744375705719\n",
      "logits: tensor([21940.8691], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20271024107933044\n",
      "logits: tensor([20460.4785], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12156117707490921\n",
      "logits: tensor([34686.5703], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.901378333568573\n",
      "logits: tensor([9033.1348], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.42589500546455383\n",
      "logits: tensor([16348.6738], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10383142530918121\n",
      "logits: tensor([4964.3892], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7278721928596497\n",
      "logits: tensor([3587.0994], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4337705373764038\n",
      "logits: tensor([5572.2827], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12040610611438751\n",
      "logits: tensor([4901.9268], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7312960624694824\n",
      "Training, Epoch: 1, Batch 400: Loss = 177980384.0\n",
      "logits: tensor([2544.3984], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5983625650405884\n",
      "logits: tensor([4689.7471], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7429268956184387\n",
      "logits: tensor([4210.1040], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.33542823791503906\n",
      "logits: tensor([4104.2163], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.775023341178894\n",
      "logits: tensor([2720.6025], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.850867509841919\n",
      "logits: tensor([4960.5923], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21696244180202484\n",
      "logits: tensor([3604.8728], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.43096497654914856\n",
      "logits: tensor([5835.1294], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07891532778739929\n",
      "logits: tensor([9568.3115], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4755036234855652\n",
      "logits: tensor([13844.8662], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24108009040355682\n",
      "logits: tensor([8003.3892], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2633480131626129\n",
      "logits: tensor([9161.8477], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4462125599384308\n",
      "logits: tensor([4351.2954], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3131409287452698\n",
      "logits: tensor([10278.4707], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6224732995033264\n",
      "logits: tensor([9027.7080], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4250383973121643\n",
      "logits: tensor([8414.2646], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5387638211250305\n",
      "logits: tensor([9741.3291], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5376846194267273\n",
      "logits: tensor([27029.4453], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48164552450180054\n",
      "logits: tensor([16630.2949], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08839408308267593\n",
      "logits: tensor([8734.7295], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5211972594261169\n",
      "logits: tensor([9254.0967], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4927276074886322\n",
      "logits: tensor([9761.9658], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5409421920776367\n",
      "logits: tensor([13481.9512], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1281479597091675\n",
      "logits: tensor([9971.5518], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5740256905555725\n",
      "logits: tensor([12146.4639], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.33417966961860657\n",
      "logits: tensor([6103.2744], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03658820316195488\n",
      "logits: tensor([9941.4785], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5692785382270813\n",
      "logits: tensor([9823.2158], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5506106019020081\n",
      "logits: tensor([9194.5195], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.49599340558052063\n",
      "logits: tensor([7963.8115], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2571006119251251\n",
      "logits: tensor([12817.9258], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29737284779548645\n",
      "logits: tensor([5707.3867], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09907972067594528\n",
      "logits: tensor([13944.9727], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23559266328811646\n",
      "logits: tensor([10179.4287], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6068393588066101\n",
      "logits: tensor([9192.3447], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4961126148700714\n",
      "logits: tensor([11235.2090], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7734960317611694\n",
      "logits: tensor([8008.6133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.26417264342308044\n",
      "logits: tensor([11507.3750], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8164578676223755\n",
      "logits: tensor([7594.0083], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1987265646457672\n",
      "logits: tensor([9539.2354], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5057837963104248\n",
      "logits: tensor([9806.6270], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5479919910430908\n",
      "logits: tensor([22583.0371], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23791131377220154\n",
      "logits: tensor([11352.0625], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3777255713939667\n",
      "logits: tensor([5248.2295], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1715584397315979\n",
      "logits: tensor([8629.3711], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.36216026544570923\n",
      "logits: tensor([9084.8916], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.43406492471694946\n",
      "logits: tensor([11466.1943], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8099574446678162\n",
      "logits: tensor([9280.3779], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4649227559566498\n",
      "logits: tensor([10962.2695], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3990924656391144\n",
      "logits: tensor([8051.5869], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5586443543434143\n",
      "Training, Epoch: 1, Batch 450: Loss = 103861952.0\n",
      "logits: tensor([6929.5103], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09383446723222733\n",
      "logits: tensor([9257.8369], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.49252259731292725\n",
      "logits: tensor([6335.3364], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 4.316255581215955e-05\n",
      "logits: tensor([11718.3770], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.849764883518219\n",
      "logits: tensor([6758.4683], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06683520972728729\n",
      "logits: tensor([6958.8315], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6185448169708252\n",
      "logits: tensor([8961.1689], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4145351052284241\n",
      "logits: tensor([7574.1865], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19559766352176666\n",
      "logits: tensor([9855.0527], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5556361079216003\n",
      "logits: tensor([5553.9155], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12330539524555206\n",
      "logits: tensor([6021.5664], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6699219346046448\n",
      "logits: tensor([19505.3145], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06920292973518372\n",
      "logits: tensor([16250.1621], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10923144221305847\n",
      "logits: tensor([28762.4062], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5766394734382629\n",
      "logits: tensor([8310.7266], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5444393754005432\n",
      "logits: tensor([6505.9165], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.026969505473971367\n",
      "logits: tensor([25568.2383], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.40154802799224854\n",
      "logits: tensor([3843.5847], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3932839035987854\n",
      "logits: tensor([11323.5723], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3792873024940491\n",
      "logits: tensor([10071.9736], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4478949010372162\n",
      "logits: tensor([5776.1455], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08822603523731232\n",
      "logits: tensor([4099.2812], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3529217839241028\n",
      "logits: tensor([8354.8555], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5420203804969788\n",
      "logits: tensor([9374.9111], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4861050546169281\n",
      "logits: tensor([8329.6523], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3148491680622101\n",
      "logits: tensor([9189.8818], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.49624761939048767\n",
      "logits: tensor([21824.7598], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19634559750556946\n",
      "logits: tensor([5982.4937], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0556536428630352\n",
      "logits: tensor([10802.1406], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4078700840473175\n",
      "logits: tensor([12146.1729], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.917293131351471\n",
      "logits: tensor([12850.2666], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.028435468673706\n",
      "logits: tensor([25968.5234], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.423490047454834\n",
      "logits: tensor([11711.8711], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.35800230503082275\n",
      "logits: tensor([10927.1152], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7248629331588745\n",
      "logits: tensor([10957.5068], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7296602725982666\n",
      "logits: tensor([7935.3145], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25260230898857117\n",
      "logits: tensor([6913.3013], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6210406422615051\n",
      "logits: tensor([5619.1143], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11301366984844208\n",
      "logits: tensor([9366.7393], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48655301332473755\n",
      "logits: tensor([5456.0093], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1387600600719452\n",
      "logits: tensor([8761.7373], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5197167992591858\n",
      "logits: tensor([14507.6709], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20474779605865479\n",
      "logits: tensor([4570.0083], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2786167562007904\n",
      "logits: tensor([13990.6416], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.2084455490112305\n",
      "logits: tensor([7493.6733], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18288852274417877\n",
      "logits: tensor([10234.0518], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6154617071151733\n",
      "logits: tensor([2823.0801], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5543721914291382\n",
      "logits: tensor([10966.6533], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.39885213971138\n",
      "logits: tensor([10832.6035], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7099440693855286\n",
      "logits: tensor([16745.7539], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08206509053707123\n",
      "Training, Epoch: 1, Batch 500: Loss = 2241313.0\n",
      "logits: tensor([11673.5254], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3601042628288269\n",
      "logits: tensor([17949.9355], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.016056692227721214\n",
      "logits: tensor([14233.5566], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.2467900514602661\n",
      "logits: tensor([15054.8477], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17475377023220062\n",
      "logits: tensor([25314.1875], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.38762199878692627\n",
      "logits: tensor([16370.3105], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1026453822851181\n",
      "logits: tensor([5996.5215], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05343932658433914\n",
      "logits: tensor([6521.3218], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.029401252046227455\n",
      "logits: tensor([7586.9912], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5841116309165955\n",
      "logits: tensor([8342.7598], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.31691819429397583\n",
      "logits: tensor([7435.8281], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17375756800174713\n",
      "logits: tensor([6476.5098], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0223276037722826\n",
      "logits: tensor([15993.7998], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12328419089317322\n",
      "logits: tensor([3617.5361], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.42896604537963867\n",
      "logits: tensor([10933.9131], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.400646835565567\n",
      "logits: tensor([3801.4905], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7916175723075867\n",
      "logits: tensor([12203.0859], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.33107587695121765\n",
      "logits: tensor([15633.5107], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14303378760814667\n",
      "logits: tensor([10495.9355], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.424655020236969\n",
      "logits: tensor([13071.5195], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28347185254096985\n",
      "logits: tensor([25406.9707], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3927080035209656\n",
      "logits: tensor([10040.1543], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5848546624183655\n",
      "logits: tensor([13588.1865], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.144917368888855\n",
      "logits: tensor([14405.2607], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.210361510515213\n",
      "logits: tensor([8658.2393], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3667171597480774\n",
      "logits: tensor([17241.1113], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05491158738732338\n",
      "logits: tensor([36926.2812], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0241502523422241\n",
      "logits: tensor([12944.9990], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0433892011642456\n",
      "logits: tensor([20244.4551], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1097196415066719\n",
      "logits: tensor([8783.7969], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3865366280078888\n",
      "logits: tensor([6899.3213], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08906909078359604\n",
      "logits: tensor([4614.4614], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.27159976959228516\n",
      "logits: tensor([4747.1611], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7397797107696533\n",
      "logits: tensor([7129.8008], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6091729998588562\n",
      "logits: tensor([3657.0872], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7995331883430481\n",
      "logits: tensor([5666.1914], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.689402163028717\n",
      "logits: tensor([4046.9531], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3611818552017212\n",
      "logits: tensor([3206.7920], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8242165446281433\n",
      "logits: tensor([7475.5098], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5902225971221924\n",
      "logits: tensor([3477.2681], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8093901872634888\n",
      "logits: tensor([4215.9653], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.33450302481651306\n",
      "logits: tensor([9625.9697], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.519474983215332\n",
      "logits: tensor([4463.7432], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29539087414741516\n",
      "logits: tensor([11802.9834], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8631201386451721\n",
      "logits: tensor([7196.9204], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13604559004306793\n",
      "logits: tensor([16155.0684], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11444409936666489\n",
      "logits: tensor([6849.1318], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.62455815076828\n",
      "logits: tensor([13810.7803], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24294854700565338\n",
      "logits: tensor([7808.8940], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2326466292142868\n",
      "logits: tensor([20832.2422], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14193977415561676\n",
      "Training, Epoch: 1, Batch 550: Loss = 6704923.5\n",
      "logits: tensor([11394.7754], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7986838221549988\n",
      "logits: tensor([9877.4170], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.45855972170829773\n",
      "logits: tensor([33748.8750], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8499776721000671\n",
      "logits: tensor([34622.3945], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8978604674339294\n",
      "logits: tensor([4799.7915], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24234509468078613\n",
      "logits: tensor([9516.6289], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4783366620540619\n",
      "logits: tensor([6812.9731], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6265402436256409\n",
      "logits: tensor([8208.4131], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5500478148460388\n",
      "logits: tensor([3689.7451], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.41756772994995117\n",
      "logits: tensor([5300.0483], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.70947265625\n",
      "logits: tensor([4958.0977], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21735621988773346\n",
      "logits: tensor([5870.0610], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.678226888179779\n",
      "logits: tensor([2644.7385], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5825237035751343\n",
      "logits: tensor([4839.1821], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23612722754478455\n",
      "logits: tensor([7080.4946], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6118757724761963\n",
      "logits: tensor([9467.2148], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48104533553123474\n",
      "logits: tensor([6756.2588], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6296490430831909\n",
      "logits: tensor([7112.7734], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.610106348991394\n",
      "logits: tensor([11321.8047], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.37938418984413147\n",
      "logits: tensor([7855.9175], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24006935954093933\n",
      "logits: tensor([13010.9199], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28679367899894714\n",
      "logits: tensor([12400.3047], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9574082493782043\n",
      "logits: tensor([16279.2031], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.5696988105773926\n",
      "logits: tensor([6983.9761], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10243198275566101\n",
      "logits: tensor([14692.5459], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19461369514465332\n",
      "logits: tensor([16548.5762], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.6122196912765503\n",
      "logits: tensor([9163.9355], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4465421438217163\n",
      "logits: tensor([11764.3516], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8570220470428467\n",
      "logits: tensor([22257.9414], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2200908660888672\n",
      "logits: tensor([15456.7197], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1527247577905655\n",
      "logits: tensor([11245.4131], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7751067280769348\n",
      "logits: tensor([9732.6318], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4664962589740753\n",
      "logits: tensor([10938.7549], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7267002463340759\n",
      "logits: tensor([3539.9121], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.44121912121772766\n",
      "logits: tensor([16819.9824], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07799617946147919\n",
      "logits: tensor([8310.6484], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3118493854999542\n",
      "logits: tensor([24251.4082], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.32936471700668335\n",
      "logits: tensor([26989.8242], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47947365045547485\n",
      "logits: tensor([5693.8647], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10121418535709381\n",
      "logits: tensor([11663.6191], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.841121256351471\n",
      "logits: tensor([10624.5039], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4176073968410492\n",
      "logits: tensor([14232.9609], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21980629861354828\n",
      "logits: tensor([9631.1523], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5202930569648743\n",
      "logits: tensor([3266.0229], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4844529628753662\n",
      "logits: tensor([8443.5273], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.53715980052948\n",
      "logits: tensor([15756.2822], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13630394637584686\n",
      "logits: tensor([3887.6831], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.38632288575172424\n",
      "logits: tensor([4446.8872], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7562395334243774\n",
      "logits: tensor([4727.0894], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2538212537765503\n",
      "logits: tensor([4188.9365], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3387695550918579\n",
      "Training, Epoch: 1, Batch 600: Loss = 4605859.0\n",
      "logits: tensor([6008.2100], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6706540584564209\n",
      "logits: tensor([18133.1660], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.006012734957039356\n",
      "logits: tensor([11012.2725], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7383051514625549\n",
      "logits: tensor([9012.4736], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4226336181163788\n",
      "logits: tensor([13013.6426], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28664442896842957\n",
      "logits: tensor([18770.2891], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.028911789879202843\n",
      "logits: tensor([28680.4199], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5721453428268433\n",
      "logits: tensor([13425.7539], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.26405414938926697\n",
      "logits: tensor([12365.9189], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.32215002179145813\n",
      "logits: tensor([12332.6875], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3239716589450836\n",
      "logits: tensor([8459.9932], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.33542367815971375\n",
      "logits: tensor([11094.5166], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7512875199317932\n",
      "logits: tensor([4510.3062], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7527630925178528\n",
      "logits: tensor([14456.3936], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20755861699581146\n",
      "logits: tensor([8785.3145], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3867761790752411\n",
      "logits: tensor([10927.2422], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.724882960319519\n",
      "logits: tensor([10837.5371], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4059297740459442\n",
      "logits: tensor([14133.1377], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.2309387922286987\n",
      "logits: tensor([7498.2700], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.183614119887352\n",
      "logits: tensor([17168.4609], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05889398977160454\n",
      "logits: tensor([14476.5410], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.2851455211639404\n",
      "logits: tensor([4500.1650], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2896416187286377\n",
      "logits: tensor([5998.5049], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.053126245737075806\n",
      "logits: tensor([5354.4082], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15479795634746552\n",
      "logits: tensor([11075.4844], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7482832074165344\n",
      "logits: tensor([10294.9287], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.43567338585853577\n",
      "logits: tensor([9642.6270], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5221043825149536\n",
      "logits: tensor([6575.1123], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0378921739757061\n",
      "logits: tensor([8229.9736], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29911473393440247\n",
      "logits: tensor([9991.0215], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.45233237743377686\n",
      "logits: tensor([9439.6191], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4900592267513275\n",
      "logits: tensor([9028.9814], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5050675272941589\n",
      "logits: tensor([17968.0176], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.015065507963299751\n",
      "logits: tensor([7386.1304], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5951219797134399\n",
      "logits: tensor([6607.0244], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.042929552495479584\n",
      "logits: tensor([8467.1543], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.33655408024787903\n",
      "logits: tensor([5498.7939], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1320064216852188\n",
      "logits: tensor([8190.5347], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.551027774810791\n",
      "logits: tensor([5709.3237], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09877395629882812\n",
      "logits: tensor([7604.2861], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20034894347190857\n",
      "logits: tensor([16274.6826], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.568985104560852\n",
      "logits: tensor([11753.0273], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8552345037460327\n",
      "logits: tensor([11309.4688], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7852180600166321\n",
      "logits: tensor([28886.3906], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5834358334541321\n",
      "logits: tensor([11014.6729], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3962199091911316\n",
      "logits: tensor([13870.7314], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2396622598171234\n",
      "logits: tensor([5021.5327], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20734289288520813\n",
      "logits: tensor([1650.0393], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7395385503768921\n",
      "logits: tensor([7444.8896], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17518794536590576\n",
      "logits: tensor([3342.8743], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47232186794281006\n",
      "Training, Epoch: 1, Batch 650: Loss = 8953193.0\n",
      "logits: tensor([3978.7861], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3719421327114105\n",
      "logits: tensor([3264.0786], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4847598671913147\n",
      "logits: tensor([1965.5499], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6897347569465637\n",
      "logits: tensor([3729.2341], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4113343358039856\n",
      "logits: tensor([7077.9922], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6120129227638245\n",
      "logits: tensor([9949.5059], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.45460808277130127\n",
      "logits: tensor([5934.4375], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.674697995185852\n",
      "logits: tensor([8731.0508], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5213989019393921\n",
      "logits: tensor([6493.3901], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6440584063529968\n",
      "logits: tensor([5276.8813], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16703569889068604\n",
      "logits: tensor([7364.8169], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5962903499603271\n",
      "logits: tensor([23657.8633], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.296828955411911\n",
      "logits: tensor([17778.4941], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02545442059636116\n",
      "logits: tensor([19157.1055], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05011551082134247\n",
      "logits: tensor([20021.4980], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09749803692102432\n",
      "logits: tensor([13003.4199], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.052610993385315\n",
      "logits: tensor([26664.7773], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4616559147834778\n",
      "logits: tensor([11013.5430], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7385056614875793\n",
      "logits: tensor([10257.9619], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4376997649669647\n",
      "logits: tensor([8484.9170], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5348909497261047\n",
      "logits: tensor([9068.7607], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.43151864409446716\n",
      "logits: tensor([8052.6187], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.27111896872520447\n",
      "logits: tensor([7564.6118], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19408628344535828\n",
      "logits: tensor([9223.8252], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4943869709968567\n",
      "logits: tensor([9370.5674], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47915929555892944\n",
      "logits: tensor([6716.3281], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6318379044532776\n",
      "logits: tensor([6258.2930], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.012118272483348846\n",
      "logits: tensor([11057.3984], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.39387786388397217\n",
      "logits: tensor([9878.6650], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5593633651733398\n",
      "logits: tensor([10503.5117], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.657996416091919\n",
      "logits: tensor([7051.8657], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6134451031684875\n",
      "logits: tensor([11540.2002], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.821639358997345\n",
      "logits: tensor([11895.3379], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8776984214782715\n",
      "logits: tensor([11802.4297], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3530382513999939\n",
      "logits: tensor([9210.2725], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.49512988328933716\n",
      "logits: tensor([6046.3296], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0455770380795002\n",
      "logits: tensor([11208.3887], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7692623734474182\n",
      "logits: tensor([21326.2285], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16901811957359314\n",
      "logits: tensor([27823.0723], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5251489877700806\n",
      "logits: tensor([21445.3008], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17554517090320587\n",
      "logits: tensor([8890.7734], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4034230411052704\n",
      "logits: tensor([14060.7402], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22924675047397614\n",
      "logits: tensor([12688.1582], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.002846360206604\n",
      "logits: tensor([8644.6436], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.36457106471061707\n",
      "logits: tensor([5805.1558], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08364671468734741\n",
      "logits: tensor([6980.1958], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10183526575565338\n",
      "logits: tensor([12399.6826], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3202992379665375\n",
      "logits: tensor([10996.9736], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.397190123796463\n",
      "logits: tensor([6819.7554], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07650948315858841\n",
      "logits: tensor([7077.8970], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6120181679725647\n",
      "Training, Epoch: 1, Batch 700: Loss = 124656312.0\n",
      "logits: tensor([5430.1611], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14284023642539978\n",
      "logits: tensor([19349.6094], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06066780164837837\n",
      "logits: tensor([3633.2439], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8008401393890381\n",
      "logits: tensor([3073.0515], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.51491379737854\n",
      "logits: tensor([3560.8428], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.437915176153183\n",
      "logits: tensor([7930.0811], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25177618861198425\n",
      "logits: tensor([5418.2783], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14471594989299774\n",
      "logits: tensor([8837.0742], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.39494654536247253\n",
      "logits: tensor([18458.6602], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.011829545721411705\n",
      "logits: tensor([8721.4229], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5219266414642334\n",
      "logits: tensor([17386.8555], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0469224788248539\n",
      "logits: tensor([9226.4766], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4564143419265747\n",
      "logits: tensor([16934.7559], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07170476019382477\n",
      "logits: tensor([11260.5283], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.38274309039115906\n",
      "logits: tensor([18722.5488], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.026294860988855362\n",
      "logits: tensor([11550.3887], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8232476711273193\n",
      "logits: tensor([10459.6602], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4266434907913208\n",
      "logits: tensor([8136.4648], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28435420989990234\n",
      "logits: tensor([12225.8389], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3298286497592926\n",
      "logits: tensor([16615.3301], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08921439945697784\n",
      "logits: tensor([18013.3633], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.012579838745296001\n",
      "logits: tensor([11680.9326], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.35969823598861694\n",
      "logits: tensor([17166.9062], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05897920951247215\n",
      "logits: tensor([9396.4951], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48325204849243164\n",
      "logits: tensor([8935.2373], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4104417562484741\n",
      "logits: tensor([5322.8496], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15977953374385834\n",
      "logits: tensor([19978.4258], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09513698518276215\n",
      "logits: tensor([7664.7832], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20989850163459778\n",
      "logits: tensor([48059.6133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.6344348192214966\n",
      "logits: tensor([31138.8574], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7069069743156433\n",
      "logits: tensor([7595.3262], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19893459975719452\n",
      "logits: tensor([5878.1450], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07212524116039276\n",
      "logits: tensor([6746.5835], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6301794052124023\n",
      "logits: tensor([2513.3796], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8622266054153442\n",
      "logits: tensor([2922.4226], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5386908650398254\n",
      "logits: tensor([5258.1797], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7117677330970764\n",
      "logits: tensor([6223.4302], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.017621420323848724\n",
      "logits: tensor([4111.5000], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7746241092681885\n",
      "logits: tensor([-303.8395], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0166553258895874\n",
      "logits: tensor([1863.0615], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.705912709236145\n",
      "logits: tensor([4217.6040], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7688079476356506\n",
      "logits: tensor([3374.5513], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.46732160449028015\n",
      "logits: tensor([4588.3364], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7484858632087708\n",
      "logits: tensor([1781.2980], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.718819260597229\n",
      "logits: tensor([4209.8223], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7692344784736633\n",
      "logits: tensor([4932.8706], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22133834660053253\n",
      "logits: tensor([3899.5437], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.786242663860321\n",
      "logits: tensor([4654.2319], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.26532191038131714\n",
      "logits: tensor([7655.7495], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5803425908088684\n",
      "logits: tensor([7266.0430], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6017047166824341\n",
      "Training, Epoch: 1, Batch 750: Loss = 120490416.0\n",
      "logits: tensor([10693.5186], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.41382429003715515\n",
      "logits: tensor([6231.2441], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.016387974843382835\n",
      "logits: tensor([8919.4014], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5110742449760437\n",
      "logits: tensor([8599.6631], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3574708104133606\n",
      "logits: tensor([9250.5762], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.46021848917007446\n",
      "logits: tensor([22137.7578], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2135028839111328\n",
      "logits: tensor([27583.6855], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5120267271995544\n",
      "logits: tensor([17332.2500], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04991573095321655\n",
      "logits: tensor([6619.9990], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.044977616518735886\n",
      "logits: tensor([18144.9980], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.005364150274544954\n",
      "logits: tensor([7485.2378], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.589689314365387\n",
      "logits: tensor([9917.9307], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5655614733695984\n",
      "logits: tensor([13394.0918], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1142791509628296\n",
      "logits: tensor([10614.3770], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6754966974258423\n",
      "logits: tensor([12979.4531], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0488277673721313\n",
      "logits: tensor([22063.7695], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20944714546203613\n",
      "logits: tensor([10050.7266], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5865235328674316\n",
      "logits: tensor([5457.3081], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13855503499507904\n",
      "logits: tensor([11353.2480], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3776605725288391\n",
      "logits: tensor([8405.1240], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.32676249742507935\n",
      "logits: tensor([5487.7783], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13374526798725128\n",
      "logits: tensor([8759.5684], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.38271212577819824\n",
      "logits: tensor([14474.8799], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20654527842998505\n",
      "logits: tensor([7021.7373], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10839265584945679\n",
      "logits: tensor([3879.3660], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.787348747253418\n",
      "logits: tensor([2652.1084], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.581360399723053\n",
      "logits: tensor([4617.4438], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.27112898230552673\n",
      "logits: tensor([2752.4561], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5655203461647034\n",
      "logits: tensor([5576.5562], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11973153799772263\n",
      "logits: tensor([10520.6758], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.42329883575439453\n",
      "logits: tensor([5897.5459], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06906278431415558\n",
      "logits: tensor([2446.8398], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.613762378692627\n",
      "logits: tensor([9404.6670], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48454198241233826\n",
      "logits: tensor([5807.8711], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.681635856628418\n",
      "logits: tensor([8159.1182], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5527499318122864\n",
      "logits: tensor([5026.0205], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2066344916820526\n",
      "logits: tensor([6194.5142], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.022185862064361572\n",
      "logits: tensor([5728.0483], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09581825137138367\n",
      "logits: tensor([17465.1816], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.042628951370716095\n",
      "logits: tensor([7898.6509], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24681489169597626\n",
      "logits: tensor([5792.0781], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0857110470533371\n",
      "logits: tensor([6489.6123], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.024395860731601715\n",
      "logits: tensor([12657.0527], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30619126558303833\n",
      "logits: tensor([43888.6914], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.4058016538619995\n",
      "logits: tensor([12478.4697], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9697467684745789\n",
      "logits: tensor([4452.9932], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29708778858184814\n",
      "logits: tensor([3605.7292], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4308297634124756\n",
      "logits: tensor([5441.2988], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14108212292194366\n",
      "logits: tensor([4036.1233], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.36289137601852417\n",
      "logits: tensor([5721.5000], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09685191512107849\n",
      "Training, Epoch: 1, Batch 800: Loss = 376459.53125\n",
      "logits: tensor([507.5754], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.972176730632782\n",
      "logits: tensor([2117.0671], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8839508891105652\n",
      "logits: tensor([-6.7502], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0003700256347656\n",
      "logits: tensor([2954.1533], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8380652070045471\n",
      "logits: tensor([2495.0107], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8632335662841797\n",
      "logits: tensor([882.3010], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8607273697853088\n",
      "logits: tensor([7618.2446], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20255230367183685\n",
      "logits: tensor([6603.6646], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6380136609077454\n",
      "logits: tensor([1734.4397], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7262158393859863\n",
      "logits: tensor([1256.9374], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9310997128486633\n",
      "logits: tensor([1699.1212], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7317909598350525\n",
      "logits: tensor([5652.2114], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10778923332691193\n",
      "logits: tensor([7117.8066], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6098304390907288\n",
      "logits: tensor([2795.5852], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5587123036384583\n",
      "logits: tensor([1449.0490], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7712652683258057\n",
      "logits: tensor([7400.9570], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1682531088590622\n",
      "logits: tensor([5515.1099], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6976838707923889\n",
      "logits: tensor([9660.1045], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5248631834983826\n",
      "logits: tensor([8656.5459], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.36644986271858215\n",
      "logits: tensor([10659.7734], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6826626062393188\n",
      "logits: tensor([834.7198], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9542439579963684\n",
      "logits: tensor([8861.6387], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3988240957260132\n",
      "logits: tensor([8889.7373], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5127003192901611\n",
      "logits: tensor([9745.1562], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.46580970287323\n",
      "logits: tensor([12705.1611], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30355414748191833\n",
      "logits: tensor([16439.7070], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0988413468003273\n",
      "logits: tensor([11858.4824], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3499656617641449\n",
      "logits: tensor([18393.3848], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.008251410908997059\n",
      "logits: tensor([9687.3613], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5291657447814941\n",
      "logits: tensor([13690.9287], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1611353158950806\n",
      "logits: tensor([11203.4375], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7684808373451233\n",
      "logits: tensor([2809.7317], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8459818363189697\n",
      "logits: tensor([20605.2910], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 2.2525787353515625\n",
      "logits: tensor([19553.9180], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 2.086617946624756\n",
      "logits: tensor([14723.8340], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.3241811990737915\n",
      "logits: tensor([13508.9414], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1324083805084229\n",
      "logits: tensor([7833.7295], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5705864429473877\n",
      "logits: tensor([9788.0332], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4634593725204468\n",
      "logits: tensor([9902.0664], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5630573034286499\n",
      "logits: tensor([6174.7529], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02530520409345627\n",
      "logits: tensor([2221.5720], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8782223463058472\n",
      "logits: tensor([3715.4182], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7963356971740723\n",
      "logits: tensor([7557.3989], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19294771552085876\n",
      "logits: tensor([5345.4131], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7069859504699707\n",
      "logits: tensor([7520.4360], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5877599716186523\n",
      "logits: tensor([3177.4885], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.49842825531959534\n",
      "logits: tensor([2661.7046], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5798456072807312\n",
      "logits: tensor([570.9054], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9098816514015198\n",
      "logits: tensor([9067.6289], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.502949059009552\n",
      "logits: tensor([15237.3320], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16475071012973785\n",
      "Training, Epoch: 1, Batch 850: Loss = 9033171.0\n",
      "logits: tensor([10569.9619], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4205971658229828\n",
      "logits: tensor([3372.5200], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.467642217874527\n",
      "logits: tensor([16559.9551], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09224983304738998\n",
      "logits: tensor([11454.5254], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.37210896611213684\n",
      "logits: tensor([11002.9180], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7368285059928894\n",
      "logits: tensor([9954.8662], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5713918209075928\n",
      "logits: tensor([25029.2383], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3720022141933441\n",
      "logits: tensor([14773.2666], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1901889145374298\n",
      "logits: tensor([10957.4316], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7296484112739563\n",
      "logits: tensor([12321.7334], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9450056552886963\n",
      "logits: tensor([6660.6611], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0513961985707283\n",
      "logits: tensor([27366.9414], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5001457333564758\n",
      "logits: tensor([16170.3701], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1136053130030632\n",
      "logits: tensor([14904.9600], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18297001719474792\n",
      "logits: tensor([9553.2568], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5079971551895142\n",
      "logits: tensor([9200.0781], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.45224729180336\n",
      "logits: tensor([12595.6484], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3095571994781494\n",
      "logits: tensor([16826.6152], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0776325985789299\n",
      "logits: tensor([5675.1934], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10416149348020554\n",
      "logits: tensor([6419.4067], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.013313798233866692\n",
      "logits: tensor([9893.6553], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5617296099662781\n",
      "logits: tensor([6241.8340], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.01471634954214096\n",
      "logits: tensor([13247.6270], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.27381834387779236\n",
      "logits: tensor([5673.3101], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1044587790966034\n",
      "logits: tensor([12659.3857], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30606335401535034\n",
      "logits: tensor([10210.1982], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6116964221000671\n",
      "logits: tensor([10219.4443], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4398111402988434\n",
      "logits: tensor([3222.8772], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.823334813117981\n",
      "logits: tensor([7353.8618], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5968908667564392\n",
      "logits: tensor([4894.6577], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22737032175064087\n",
      "logits: tensor([5427.9146], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7024635672569275\n",
      "logits: tensor([15022.2588], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17654016613960266\n",
      "logits: tensor([12539.5713], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9793917536735535\n",
      "logits: tensor([6707.0552], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05871957167983055\n",
      "logits: tensor([16553.8027], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.09258708357810974\n",
      "logits: tensor([12254.2920], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9343599081039429\n",
      "logits: tensor([7892.0391], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24577119946479797\n",
      "logits: tensor([16018.6826], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12192021310329437\n",
      "logits: tensor([16727.9492], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08304107189178467\n",
      "logits: tensor([7624.6431], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20356231927871704\n",
      "logits: tensor([30479.2812], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.670751690864563\n",
      "logits: tensor([13058.3291], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2841948866844177\n",
      "logits: tensor([27605.0312], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5131968259811401\n",
      "logits: tensor([7646.7812], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20705686509609222\n",
      "logits: tensor([10064.6396], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5887197256088257\n",
      "logits: tensor([7336.4663], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5978443622589111\n",
      "logits: tensor([9794.7861], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4630891978740692\n",
      "logits: tensor([11296.6924], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.38076072931289673\n",
      "logits: tensor([6581.1748], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03884914889931679\n",
      "logits: tensor([8922.5664], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4084416329860687\n",
      "Training, Epoch: 1, Batch 900: Loss = 6695174.0\n",
      "logits: tensor([7652.5288], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20796412229537964\n",
      "logits: tensor([11156.3486], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.38845381140708923\n",
      "logits: tensor([11873.9922], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3491154909133911\n",
      "logits: tensor([9988.9766], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.452444463968277\n",
      "logits: tensor([6289.8911], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.655213475227356\n",
      "logits: tensor([5284.9980], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16575445234775543\n",
      "logits: tensor([13889.2617], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2386465072631836\n",
      "logits: tensor([9275.5166], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.464155375957489\n",
      "logits: tensor([10452.6602], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6499693989753723\n",
      "logits: tensor([15757.4355], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13624073565006256\n",
      "logits: tensor([6137.3750], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.031205371022224426\n",
      "logits: tensor([15606.7334], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14450161159038544\n",
      "logits: tensor([12631.4629], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30759397149086\n",
      "logits: tensor([13159.0117], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2786758840084076\n",
      "logits: tensor([9333.2012], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4732609987258911\n",
      "logits: tensor([24836.0918], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.36141470074653625\n",
      "logits: tensor([25097.2168], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3757285475730896\n",
      "logits: tensor([7083.2402], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11810099333524704\n",
      "logits: tensor([13611.6084], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2538663446903229\n",
      "logits: tensor([16646.2188], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08752120286226273\n",
      "logits: tensor([9170.1738], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.44752687215805054\n",
      "logits: tensor([10522.1592], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6609399318695068\n",
      "logits: tensor([12230.2246], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.930560827255249\n",
      "logits: tensor([13473.9043], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2614147365093231\n",
      "logits: tensor([10210.3184], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6117153763771057\n",
      "logits: tensor([7986.5312], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2606869637966156\n",
      "logits: tensor([13146.6240], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.27935492992401123\n",
      "logits: tensor([11264.3330], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.38253453373908997\n",
      "logits: tensor([12496.9424], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3149678707122803\n",
      "logits: tensor([8674.3330], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5245079398155212\n",
      "logits: tensor([15455.4004], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15279708802700043\n",
      "logits: tensor([10869.0273], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7156936526298523\n",
      "logits: tensor([10441.9707], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.42761313915252686\n",
      "logits: tensor([15972.8145], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12443452328443527\n",
      "logits: tensor([15030.9805], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17606207728385925\n",
      "logits: tensor([6097.9204], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03743334114551544\n",
      "logits: tensor([13330.8428], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.104295253753662\n",
      "logits: tensor([16603.2070], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.6208432912826538\n",
      "logits: tensor([17699.6191], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02977803163230419\n",
      "logits: tensor([18354.3516], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0061117676086723804\n",
      "logits: tensor([12960.3828], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.045817494392395\n",
      "logits: tensor([10274.0078], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6217688322067261\n",
      "logits: tensor([7818.3208], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23413465917110443\n",
      "logits: tensor([16975.8262], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.6796618700027466\n",
      "logits: tensor([17706.3340], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.029409950599074364\n",
      "logits: tensor([5865.4370], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07413122802972794\n",
      "logits: tensor([8390.2354], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3244123160839081\n",
      "logits: tensor([12061.3076], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9038970470428467\n",
      "logits: tensor([5063.4663], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20072360336780548\n",
      "logits: tensor([8411.6064], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.32778576016426086\n",
      "Training, Epoch: 1, Batch 950: Loss = 4312032.5\n",
      "logits: tensor([11872.2334], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3492119014263153\n",
      "logits: tensor([8913.6943], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.40704116225242615\n",
      "logits: tensor([15531.4600], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14862780272960663\n",
      "logits: tensor([5452.4033], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13932926952838898\n",
      "logits: tensor([10410.8936], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4293166697025299\n",
      "logits: tensor([13602.8428], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2543468475341797\n",
      "logits: tensor([3145.8477], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8275572657585144\n",
      "logits: tensor([4382.3252], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7597785592079163\n",
      "logits: tensor([17077.4922], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06388053297996521\n",
      "logits: tensor([7252.3076], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14478856325149536\n",
      "logits: tensor([6065.1187], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.042611151933670044\n",
      "logits: tensor([10090.1523], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5927469730377197\n",
      "logits: tensor([12178.1299], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.33244386315345764\n",
      "logits: tensor([22428.2402], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22942596673965454\n",
      "logits: tensor([8677.4326], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3697468638420105\n",
      "logits: tensor([25976.1289], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.42390695214271545\n",
      "logits: tensor([11356.4746], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7926380038261414\n",
      "logits: tensor([10688.9590], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6872695684432983\n",
      "logits: tensor([5440.4966], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14120876789093018\n",
      "logits: tensor([8066.6006], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5578213930130005\n",
      "logits: tensor([4970.4780], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21540196239948273\n",
      "logits: tensor([6500.1196], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02605445869266987\n",
      "logits: tensor([5958.6064], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.059424277395009995\n",
      "logits: tensor([5144.8984], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18786941468715668\n",
      "logits: tensor([7897.6665], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5670816898345947\n",
      "logits: tensor([8278.0361], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3067014813423157\n",
      "logits: tensor([6081.5117], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.040023479610681534\n",
      "logits: tensor([9793.3945], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5459032654762268\n",
      "logits: tensor([8399.3086], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.539583683013916\n",
      "logits: tensor([10731.1963], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4117589592933655\n",
      "logits: tensor([8233.4961], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2996707558631897\n",
      "logits: tensor([8083.4653], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2759881615638733\n",
      "logits: tensor([10494.3906], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6565566062927246\n",
      "logits: tensor([9657.1582], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.524398148059845\n",
      "logits: tensor([9364.8008], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47824904322624207\n",
      "logits: tensor([7480.8574], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.589929461479187\n",
      "logits: tensor([13767.7754], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2453058958053589\n",
      "logits: tensor([10845.6523], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7120038866996765\n",
      "logits: tensor([10909.4346], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7220720052719116\n",
      "logits: tensor([13618.8721], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25346818566322327\n",
      "logits: tensor([15294.1514], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1616360992193222\n",
      "logits: tensor([7446.1284], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17538347840309143\n",
      "logits: tensor([21144.5410], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.15905873477458954\n",
      "logits: tensor([9394.4629], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4829312562942505\n",
      "logits: tensor([6722.3369], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06113182008266449\n",
      "logits: tensor([12005.3320], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.34191596508026123\n",
      "logits: tensor([9737.1943], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4662461578845978\n",
      "logits: tensor([9284.5742], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4655851423740387\n",
      "logits: tensor([9498.5186], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4793294072151184\n",
      "logits: tensor([10973.2598], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7321469187736511\n",
      "Training, Epoch: 1, Batch 1000: Loss = 21512870.0\n",
      "logits: tensor([16136.3027], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11547274887561798\n",
      "logits: tensor([12255.1348], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.32822278141975403\n",
      "logits: tensor([25515.7930], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3986732065677643\n",
      "logits: tensor([8075.9736], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.27480557560920715\n",
      "logits: tensor([11259.4824], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7773275971412659\n",
      "logits: tensor([11683.5449], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.35955503582954407\n",
      "logits: tensor([15269.6973], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.4103466272354126\n",
      "logits: tensor([21971.0586], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2043651044368744\n",
      "logits: tensor([11388.4844], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3757290542125702\n",
      "logits: tensor([29234.8457], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6025367379188538\n",
      "logits: tensor([11737.7969], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3565811514854431\n",
      "logits: tensor([8904.0098], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.40551242232322693\n",
      "logits: tensor([10901.7256], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7208551168441772\n",
      "logits: tensor([9879.9268], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.45842212438583374\n",
      "logits: tensor([15731.4658], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1376642882823944\n",
      "logits: tensor([6560.1621], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6403982639312744\n",
      "logits: tensor([3185.0862], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4972289502620697\n",
      "logits: tensor([11767.2959], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.35496413707733154\n",
      "logits: tensor([7379.5913], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16488049924373627\n",
      "logits: tensor([11768.0771], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3549213111400604\n",
      "logits: tensor([10198.5225], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4409579932689667\n",
      "logits: tensor([5585.8691], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11826147139072418\n",
      "logits: tensor([14966.0107], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17962345480918884\n",
      "logits: tensor([15865.0967], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13033917546272278\n",
      "logits: tensor([8365.8564], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.32056406140327454\n",
      "logits: tensor([10083.2656], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5916599035263062\n",
      "logits: tensor([9046.5537], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.42801323533058167\n",
      "logits: tensor([10909.9443], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7221524715423584\n",
      "logits: tensor([11397.9375], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.37521088123321533\n",
      "logits: tensor([7858.9775], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2405523955821991\n",
      "logits: tensor([15456.5654], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1527332216501236\n",
      "logits: tensor([15082.3584], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.380774736404419\n",
      "logits: tensor([5582.4541], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11880053579807281\n",
      "logits: tensor([10711.5996], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.690843403339386\n",
      "logits: tensor([11747.9004], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8544251918792725\n",
      "logits: tensor([12589.0635], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.9872041344642639\n",
      "logits: tensor([13794.3770], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24384771287441254\n",
      "logits: tensor([5015.8032], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.20824730396270752\n",
      "logits: tensor([5510.4268], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13017016649246216\n",
      "logits: tensor([13087.0332], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28262144327163696\n",
      "logits: tensor([10086.7715], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5922132730484009\n",
      "logits: tensor([10883.0674], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7179098725318909\n",
      "logits: tensor([9577.0762], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5117570757865906\n",
      "logits: tensor([14780.8184], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18977496027946472\n",
      "logits: tensor([6381.7046], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0073624528013169765\n",
      "logits: tensor([9629.7197], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5200669169425964\n",
      "logits: tensor([7844.7886], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23831264674663544\n",
      "logits: tensor([13797.0615], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2437005490064621\n",
      "logits: tensor([10096.7354], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4465375542640686\n",
      "logits: tensor([11912.6494], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8804311156272888\n",
      "Training, Epoch: 1, Batch 1050: Loss = 31109470.0\n",
      "logits: tensor([11747.9785], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.35602304339408875\n",
      "logits: tensor([11201.6318], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3859715759754181\n",
      "logits: tensor([18110.8164], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.007237850688397884\n",
      "logits: tensor([10967.4219], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7312253713607788\n",
      "logits: tensor([19081.3086], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04596063122153282\n",
      "logits: tensor([8346.2461], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.317468523979187\n",
      "logits: tensor([14090.2959], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22762662172317505\n",
      "logits: tensor([16983.9570], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0690077543258667\n",
      "logits: tensor([19185.8652], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05169200524687767\n",
      "logits: tensor([4422.6899], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3018712103366852\n",
      "logits: tensor([16673.0293], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.6318649053573608\n",
      "logits: tensor([7544.8979], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1909744143486023\n",
      "logits: tensor([7880.2524], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.24391067028045654\n",
      "logits: tensor([9895.3936], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5620039701461792\n",
      "logits: tensor([15503.5918], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.4472672939300537\n",
      "logits: tensor([12125.7471], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.914068877696991\n",
      "logits: tensor([19773.1270], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08388333022594452\n",
      "logits: tensor([8503.0215], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.34221577644348145\n",
      "logits: tensor([22339.2266], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22454659640789032\n",
      "logits: tensor([9236.8838], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.45805713534355164\n",
      "logits: tensor([9318.7100], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47097352147102356\n",
      "logits: tensor([14058.5391], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22936740517616272\n",
      "logits: tensor([17071.4746], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06421039253473282\n",
      "logits: tensor([4104.6489], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3520744740962982\n",
      "logits: tensor([6593.7578], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04083539918065071\n",
      "logits: tensor([7145.9375], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.12799786031246185\n",
      "logits: tensor([4040.9695], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3621263802051544\n",
      "logits: tensor([3752.7505], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.40762221813201904\n",
      "logits: tensor([7052.2134], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6134260296821594\n",
      "logits: tensor([4262.0762], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.32722434401512146\n",
      "logits: tensor([7102.0557], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6106938719749451\n",
      "logits: tensor([12671.6562], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30539074540138245\n",
      "logits: tensor([6007.9346], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05163775384426117\n",
      "logits: tensor([6462.5854], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6457470059394836\n",
      "logits: tensor([5130.2397], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19018331170082092\n",
      "logits: tensor([4749.9512], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25021249055862427\n",
      "logits: tensor([5415.4351], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.14516475796699524\n",
      "logits: tensor([11185.6172], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.38684943318367004\n",
      "logits: tensor([17638.0449], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03315328434109688\n",
      "logits: tensor([26843.8574], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4714723527431488\n",
      "logits: tensor([14406.8613], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21027377247810364\n",
      "logits: tensor([22937.2578], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2573282718658447\n",
      "logits: tensor([16172.3496], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1134968101978302\n",
      "logits: tensor([8709.9014], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.374872088432312\n",
      "logits: tensor([13711.0557], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2484150528907776\n",
      "logits: tensor([10356.3857], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4323045611381531\n",
      "logits: tensor([14282.8447], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21707187592983246\n",
      "logits: tensor([4908.3237], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22521311044692993\n",
      "logits: tensor([15298.4346], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.16140131652355194\n",
      "logits: tensor([13409.3496], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1166876554489136\n",
      "Training, Epoch: 1, Batch 1100: Loss = 50045532.0\n",
      "logits: tensor([6436.8589], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6471572518348694\n",
      "logits: tensor([16636.3398], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.6260733604431152\n",
      "logits: tensor([14343.7871], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.2641900777816772\n",
      "logits: tensor([33072.1094], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8128800988197327\n",
      "logits: tensor([9396.0303], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48317867517471313\n",
      "logits: tensor([9943.6377], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.45492976903915405\n",
      "logits: tensor([6883.8882], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08663295209407806\n",
      "logits: tensor([17746.6035], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.027202537283301353\n",
      "logits: tensor([5479.5918], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13503752648830414\n",
      "logits: tensor([11417.2305], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3741533160209656\n",
      "logits: tensor([8189.7007], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5510734915733337\n",
      "logits: tensor([4571.8950], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7493871450424194\n",
      "logits: tensor([9268.6367], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4630693793296814\n",
      "logits: tensor([2413.4609], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6190312504768372\n",
      "logits: tensor([13205.6553], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.0845341682434082\n",
      "logits: tensor([34836.], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.909569501876831\n",
      "logits: tensor([8755.4072], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.38205528259277344\n",
      "logits: tensor([7551.4849], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5860579609870911\n",
      "logits: tensor([6151.2866], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6628111600875854\n",
      "logits: tensor([2271.6753], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6414123773574829\n",
      "logits: tensor([3431.8386], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8118804097175598\n",
      "logits: tensor([5323.4688], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7081888318061829\n",
      "logits: tensor([2807.5203], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5568283796310425\n",
      "logits: tensor([1793.6367], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7168715000152588\n",
      "logits: tensor([2592.4675], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.857891321182251\n",
      "logits: tensor([4650.7061], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7450670003890991\n",
      "logits: tensor([3350.2495], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47115767002105713\n",
      "logits: tensor([6580.3320], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6392926573753357\n",
      "logits: tensor([6280.9526], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6557034254074097\n",
      "logits: tensor([8605.7588], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5282669067382812\n",
      "logits: tensor([7055.6729], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1137494370341301\n",
      "logits: tensor([7404.3525], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1687890887260437\n",
      "logits: tensor([13943.0322], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23569902777671814\n",
      "logits: tensor([6613.8560], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6374550461769104\n",
      "logits: tensor([7492.1245], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1826440393924713\n",
      "logits: tensor([9318.6963], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48918652534484863\n",
      "logits: tensor([12715.6885], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3029770851135254\n",
      "logits: tensor([12735.1680], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.30190929770469666\n",
      "logits: tensor([14753.2139], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.328818678855896\n",
      "logits: tensor([16127.8916], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.545813798904419\n",
      "logits: tensor([17326.2324], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.7349740266799927\n",
      "logits: tensor([14994.7773], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.3669500350952148\n",
      "logits: tensor([19688.3984], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07923885434865952\n",
      "logits: tensor([4121.7446], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7740625739097595\n",
      "logits: tensor([12937.5918], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.29081323742866516\n",
      "logits: tensor([17424.6211], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04485232010483742\n",
      "logits: tensor([9998.8037], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5783274173736572\n",
      "logits: tensor([17187.1133], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.05787154287099838\n",
      "logits: tensor([20263.4121], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11075878888368607\n",
      "logits: tensor([9980.4697], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.45291078090667725\n",
      "Training, Epoch: 1, Batch 1150: Loss = 68267016.0\n",
      "logits: tensor([17872.7812], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.02028598077595234\n",
      "logits: tensor([11954.7822], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8870818018913269\n",
      "logits: tensor([13352.9512], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.1077851057052612\n",
      "logits: tensor([15918.1768], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1274295449256897\n",
      "logits: tensor([8099.4243], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2785072922706604\n",
      "logits: tensor([10872.3086], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.7162116169929504\n",
      "logits: tensor([9398.1982], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48482856154441833\n",
      "logits: tensor([10230.8447], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4391862154006958\n",
      "logits: tensor([8165.9995], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2890163064002991\n",
      "logits: tensor([3083.0291], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8310007452964783\n",
      "logits: tensor([11042.6240], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3946877419948578\n",
      "logits: tensor([5112.1416], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.193040132522583\n",
      "logits: tensor([3480.1692], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.450649619102478\n",
      "logits: tensor([9708.0459], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5324308276176453\n",
      "logits: tensor([20382.8008], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.11730320006608963\n",
      "logits: tensor([6941.2349], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6195094585418701\n",
      "logits: tensor([19061.2852], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0448630265891552\n",
      "logits: tensor([10002.7354], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5789480209350586\n",
      "logits: tensor([11896.7168], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8779160976409912\n",
      "logits: tensor([16296.5791], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.5724414587020874\n",
      "logits: tensor([11826.1904], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.35173577070236206\n",
      "logits: tensor([23641.5410], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2959342300891876\n",
      "logits: tensor([13654.8604], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25149545073509216\n",
      "logits: tensor([13567.4365], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.25628766417503357\n",
      "logits: tensor([10306.0293], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6268234848976135\n",
      "logits: tensor([15965.0830], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.1248583272099495\n",
      "logits: tensor([14825.9492], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18730106949806213\n",
      "logits: tensor([27540.9297], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5096830725669861\n",
      "logits: tensor([6934.7397], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0946599543094635\n",
      "logits: tensor([9417.0596], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4864981770515442\n",
      "logits: tensor([3939.9792], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.37806785106658936\n",
      "logits: tensor([6756.9458], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06659489125013351\n",
      "logits: tensor([4908.4917], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22518660128116608\n",
      "logits: tensor([13770.8340], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2451382428407669\n",
      "logits: tensor([19474.8535], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.06753318011760712\n",
      "logits: tensor([5661.4790], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10632632672786713\n",
      "logits: tensor([8526.5674], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5326078534126282\n",
      "logits: tensor([6596.8379], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.04132159426808357\n",
      "logits: tensor([9081.3555], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.43350672721862793\n",
      "logits: tensor([7727.8066], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21984684467315674\n",
      "logits: tensor([8880.2402], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5132209062576294\n",
      "logits: tensor([14967.3594], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.17954953014850616\n",
      "logits: tensor([4736.1606], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2523893415927887\n",
      "logits: tensor([7593.6514], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.19867022335529327\n",
      "logits: tensor([7886.3765], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5677005052566528\n",
      "logits: tensor([9430.6904], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.48864981532096863\n",
      "logits: tensor([16755.0625], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.0815548300743103\n",
      "logits: tensor([5839.6812], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.07819683104753494\n",
      "logits: tensor([9960.5684], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.572291910648346\n",
      "logits: tensor([21667.1387], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.18770544230937958\n",
      "Training, Epoch: 1, Batch 1200: Loss = 11725715.0\n",
      "logits: tensor([16608.5801], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.08958441019058228\n",
      "logits: tensor([13390.5215], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2659854292869568\n",
      "logits: tensor([9559.1240], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.47600725293159485\n",
      "logits: tensor([10361.2969], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6355475783348083\n",
      "logits: tensor([9147.4424], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.49857398867607117\n",
      "logits: tensor([8833.6055], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3943989872932434\n",
      "logits: tensor([6259.1909], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.011976529844105244\n",
      "logits: tensor([7832.5396], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.23637911677360535\n",
      "logits: tensor([13379.5059], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.26658928394317627\n",
      "logits: tensor([9474.5430], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.495572030544281\n",
      "logits: tensor([22500.2852], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.2333751767873764\n",
      "logits: tensor([10138.1338], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6003209352493286\n",
      "logits: tensor([10544.2041], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.42200911045074463\n",
      "logits: tensor([13004.4463], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.28714853525161743\n",
      "logits: tensor([22323.8359], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.22370293736457825\n",
      "logits: tensor([7193.3574], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.13548317551612854\n",
      "logits: tensor([9969.5898], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.4535071551799774\n",
      "logits: tensor([14279.0693], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 1.2539743185043335\n",
      "logits: tensor([10316.0498], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.6284052729606628\n",
      "logits: tensor([24482.0059], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3420051336288452\n",
      "logits: tensor([11409.7881], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.8010535836219788\n",
      "logits: tensor([12446.9707], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.3177070915699005\n",
      "logits: tensor([7710.6294], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.21713539958000183\n",
      "logits: tensor([9910.5273], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5643928647041321\n",
      "logits: tensor([7927.6606], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.251394122838974\n",
      "logits: tensor([9038.1562], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.5045645833015442\n",
      "logits: tensor([16369.1602], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.10270844399929047\n",
      "logits: tensor([6112.0342], grad_fn=<SqueezeBackward1>)\n",
      "MAPE loss: 0.03520546108484268\n",
      "logits: tensor([22019.1152])\n",
      "MAPE loss: 0.20699937641620636\n",
      "Valing, Epoch: 1, Batch 0: Loss = 14260138.0\n",
      "logits: tensor([8405.7266])\n",
      "MAPE loss: 0.32685762643814087\n",
      "logits: tensor([7282.2905])\n",
      "MAPE loss: 0.14952141046524048\n",
      "logits: tensor([7687.7959])\n",
      "MAPE loss: 0.21353109180927277\n",
      "logits: tensor([9344.7959])\n",
      "MAPE loss: 0.47509124875068665\n",
      "logits: tensor([19837.6602])\n",
      "MAPE loss: 0.08742078393697739\n",
      "logits: tensor([14233.1387])\n",
      "MAPE loss: 0.2197965532541275\n",
      "logits: tensor([12658.1738])\n",
      "MAPE loss: 0.3061297833919525\n",
      "logits: tensor([8526.8027])\n",
      "MAPE loss: 0.34596967697143555\n",
      "logits: tensor([8600.4082])\n",
      "MAPE loss: 0.35758844017982483\n",
      "logits: tensor([10665.3223])\n",
      "MAPE loss: 0.6835384964942932\n",
      "logits: tensor([13815.5117])\n",
      "MAPE loss: 0.24268919229507446\n",
      "logits: tensor([13342.8906])\n",
      "MAPE loss: 0.2685963809490204\n",
      "logits: tensor([7399.0029])\n",
      "MAPE loss: 0.16794465482234955\n",
      "logits: tensor([11144.0127])\n",
      "MAPE loss: 0.7591005563735962\n",
      "logits: tensor([9642.7070])\n",
      "MAPE loss: 0.522117018699646\n",
      "logits: tensor([9836.4834])\n",
      "MAPE loss: 0.552704930305481\n",
      "logits: tensor([8489.4531])\n",
      "MAPE loss: 0.34007400274276733\n",
      "logits: tensor([15306.5371])\n",
      "MAPE loss: 0.16095717251300812\n",
      "logits: tensor([9535.3896])\n",
      "MAPE loss: 0.4773082733154297\n",
      "logits: tensor([15822.4658])\n",
      "MAPE loss: 0.13267603516578674\n",
      "logits: tensor([4130.0879])\n",
      "MAPE loss: 0.34805890917778015\n",
      "logits: tensor([11145.2998])\n",
      "MAPE loss: 0.38905948400497437\n",
      "logits: tensor([11893.5537])\n",
      "MAPE loss: 0.34804320335388184\n",
      "logits: tensor([10701.6992])\n",
      "MAPE loss: 0.4133758842945099\n",
      "logits: tensor([4439.])\n",
      "MAPE loss: 0.29929661750793457\n",
      "logits: tensor([6247.4033])\n",
      "MAPE loss: 0.6575424671173096\n",
      "logits: tensor([15081.1416])\n",
      "MAPE loss: 0.17331244051456451\n",
      "logits: tensor([5580.1982])\n",
      "MAPE loss: 0.11915662884712219\n",
      "logits: tensor([6138.5220])\n",
      "MAPE loss: 0.03102431818842888\n",
      "logits: tensor([5779.8770])\n",
      "MAPE loss: 0.0876370221376419\n",
      "logits: tensor([9705.2627])\n",
      "MAPE loss: 0.5319914817810059\n",
      "logits: tensor([14280.2441])\n",
      "MAPE loss: 0.2172144204378128\n",
      "logits: tensor([12329.7617])\n",
      "MAPE loss: 0.3241320252418518\n",
      "logits: tensor([15631.7578])\n",
      "MAPE loss: 0.1431298702955246\n",
      "logits: tensor([6454.8989])\n",
      "MAPE loss: 0.0189162977039814\n",
      "logits: tensor([8426.7363])\n",
      "MAPE loss: 0.3301740288734436\n",
      "logits: tensor([12704.9463])\n",
      "MAPE loss: 0.3035659193992615\n",
      "logits: tensor([9382.1699])\n",
      "MAPE loss: 0.48570716381073\n",
      "logits: tensor([6807.3584])\n",
      "MAPE loss: 0.07455259561538696\n",
      "logits: tensor([6961.3994])\n",
      "MAPE loss: 0.0988682210445404\n",
      "logits: tensor([9405.9521])\n",
      "MAPE loss: 0.4847448468208313\n",
      "logits: tensor([11998.7070])\n",
      "MAPE loss: 0.3422791063785553\n",
      "logits: tensor([8456.6172])\n",
      "MAPE loss: 0.33489078283309937\n",
      "logits: tensor([2642.7996])\n",
      "MAPE loss: 0.5828297734260559\n",
      "logits: tensor([8194.8389])\n",
      "MAPE loss: 0.55079185962677\n",
      "logits: tensor([5245.7178])\n",
      "MAPE loss: 0.1719549149274826\n",
      "logits: tensor([6190.8008])\n",
      "MAPE loss: 0.02277202345430851\n",
      "logits: tensor([7106.8486])\n",
      "MAPE loss: 0.1218276172876358\n",
      "logits: tensor([13621.2373])\n",
      "MAPE loss: 0.2533385157585144\n",
      "logits: tensor([12065.5098])\n",
      "MAPE loss: 0.3386172652244568\n",
      "Valing, Epoch: 1, Batch 50: Loss = 38159600.0\n",
      "logits: tensor([18375.6680])\n",
      "MAPE loss: 0.007280247285962105\n",
      "logits: tensor([12538.2080])\n",
      "MAPE loss: 0.3127058446407318\n",
      "logits: tensor([11319.6387])\n",
      "MAPE loss: 0.7868233919143677\n",
      "logits: tensor([7106.8027])\n",
      "MAPE loss: 0.1218203753232956\n",
      "logits: tensor([7247.3091])\n",
      "MAPE loss: 0.14399953186511993\n",
      "logits: tensor([8405.1455])\n",
      "MAPE loss: 0.5392637252807617\n",
      "logits: tensor([7663.7085])\n",
      "MAPE loss: 0.5799062848091125\n",
      "logits: tensor([17082.8574])\n",
      "MAPE loss: 0.06358642876148224\n",
      "logits: tensor([7131.9307])\n",
      "MAPE loss: 0.12578685581684113\n",
      "logits: tensor([16162.1748])\n",
      "MAPE loss: 0.11405455321073532\n",
      "logits: tensor([8911.9473])\n",
      "MAPE loss: 0.5114828944206238\n",
      "logits: tensor([12355.2051])\n",
      "MAPE loss: 0.32273733615875244\n",
      "logits: tensor([11008.6074])\n",
      "MAPE loss: 0.7377265691757202\n",
      "logits: tensor([19876.9199])\n",
      "MAPE loss: 0.08957284688949585\n",
      "logits: tensor([6139.9033])\n",
      "MAPE loss: 0.030806271359324455\n",
      "logits: tensor([27287.3848])\n",
      "MAPE loss: 0.495784729719162\n",
      "logits: tensor([10457.1777])\n",
      "MAPE loss: 0.426779568195343\n",
      "logits: tensor([7531.7949])\n",
      "MAPE loss: 0.5871372818946838\n",
      "logits: tensor([8028.1494])\n",
      "MAPE loss: 0.2672564387321472\n",
      "logits: tensor([8037.7959])\n",
      "MAPE loss: 0.2687791585922241\n",
      "logits: tensor([11018.7168])\n",
      "MAPE loss: 0.7393223643302917\n",
      "logits: tensor([19373.9277])\n",
      "MAPE loss: 0.062000833451747894\n",
      "logits: tensor([7132.3828])\n",
      "MAPE loss: 0.12585823237895966\n",
      "logits: tensor([17230.5957])\n",
      "MAPE loss: 0.05548801273107529\n",
      "logits: tensor([4810.8755])\n",
      "MAPE loss: 0.2405954748392105\n",
      "logits: tensor([8118.8721])\n",
      "MAPE loss: 0.5549560785293579\n",
      "logits: tensor([9964.5527])\n",
      "MAPE loss: 0.5729208588600159\n",
      "logits: tensor([11550.4160])\n",
      "MAPE loss: 0.36685264110565186\n",
      "logits: tensor([5348.8809])\n",
      "MAPE loss: 0.15567044913768768\n",
      "logits: tensor([3876.2856])\n",
      "MAPE loss: 0.38812199234962463\n",
      "logits: tensor([8142.2783])\n",
      "MAPE loss: 0.28527188301086426\n",
      "logits: tensor([6269.8540])\n",
      "MAPE loss: 0.010293344035744667\n",
      "logits: tensor([10619.9111])\n",
      "MAPE loss: 0.6763702630996704\n",
      "logits: tensor([5956.3965])\n",
      "MAPE loss: 0.059773124754428864\n",
      "logits: tensor([13542.4736])\n",
      "MAPE loss: 0.2576560378074646\n",
      "logits: tensor([13731.4844])\n",
      "MAPE loss: 0.24729523062705994\n",
      "logits: tensor([6548.7666])\n",
      "MAPE loss: 0.033733464777469635\n",
      "logits: tensor([8607.5000])\n",
      "MAPE loss: 0.35870787501335144\n",
      "logits: tensor([11705.5205])\n",
      "MAPE loss: 0.3583504259586334\n",
      "logits: tensor([13212.0195])\n",
      "MAPE loss: 0.2757701873779297\n",
      "logits: tensor([9713.8906])\n",
      "MAPE loss: 0.46752357482910156\n",
      "logits: tensor([6956.2549])\n",
      "MAPE loss: 0.09805615246295929\n",
      "logits: tensor([6753.2178])\n",
      "MAPE loss: 0.06600641459226608\n",
      "logits: tensor([7340.3379])\n",
      "MAPE loss: 0.15868428349494934\n",
      "logits: tensor([10744.4463])\n",
      "MAPE loss: 0.6960283517837524\n",
      "logits: tensor([9684.6797])\n",
      "MAPE loss: 0.46912479400634766\n",
      "logits: tensor([12547.3340])\n",
      "MAPE loss: 0.31220558285713196\n",
      "logits: tensor([6073.0312])\n",
      "MAPE loss: 0.04136213660240173\n",
      "logits: tensor([13714.0918])\n",
      "MAPE loss: 0.2482486218214035\n",
      "logits: tensor([4448.9170])\n",
      "MAPE loss: 0.2977312207221985\n",
      "Valing, Epoch: 1, Batch 100: Loss = 3557546.75\n",
      "logits: tensor([9544.1104])\n",
      "MAPE loss: 0.5065533518791199\n",
      "logits: tensor([12914.0781])\n",
      "MAPE loss: 0.2921021580696106\n",
      "logits: tensor([15688.6221])\n",
      "MAPE loss: 0.14001280069351196\n",
      "logits: tensor([17186.2598])\n",
      "MAPE loss: 0.057918328791856766\n",
      "logits: tensor([13704.8379])\n",
      "MAPE loss: 0.24875588715076447\n",
      "logits: tensor([8441.0684])\n",
      "MAPE loss: 0.33243638277053833\n",
      "logits: tensor([4279.1172])\n",
      "MAPE loss: 0.3245343863964081\n",
      "logits: tensor([14533.5215])\n",
      "MAPE loss: 0.20333077013492584\n",
      "logits: tensor([12098.0459])\n",
      "MAPE loss: 0.33683374524116516\n",
      "logits: tensor([16211.5244])\n",
      "MAPE loss: 0.11134940385818481\n",
      "logits: tensor([14106.0537])\n",
      "MAPE loss: 0.22676284611225128\n",
      "logits: tensor([15167.3457])\n",
      "MAPE loss: 0.1685870885848999\n",
      "logits: tensor([19089.5977])\n",
      "MAPE loss: 0.04641500115394592\n",
      "logits: tensor([14573.6602])\n",
      "MAPE loss: 0.20113053917884827\n",
      "logits: tensor([5593.9692])\n",
      "MAPE loss: 0.1169828549027443\n",
      "logits: tensor([9261.1582])\n",
      "MAPE loss: 0.4923405349254608\n",
      "logits: tensor([10738.0693])\n",
      "MAPE loss: 0.41138219833374023\n",
      "logits: tensor([17079.5176])\n",
      "MAPE loss: 0.06376950442790985\n",
      "logits: tensor([7395.2012])\n",
      "MAPE loss: 0.16734454035758972\n",
      "logits: tensor([17262.5684])\n",
      "MAPE loss: 0.0537353977560997\n",
      "logits: tensor([10358.9355])\n",
      "MAPE loss: 0.6351748108863831\n",
      "logits: tensor([6375.8662])\n",
      "MAPE loss: 0.006440855097025633\n",
      "logits: tensor([16384.3516])\n",
      "MAPE loss: 0.10187571495771408\n",
      "logits: tensor([15741.7744])\n",
      "MAPE loss: 0.13709920644760132\n",
      "logits: tensor([17447.9102])\n",
      "MAPE loss: 0.0435757078230381\n",
      "logits: tensor([7777.5381])\n",
      "MAPE loss: 0.22769704461097717\n",
      "logits: tensor([5054.4419])\n",
      "MAPE loss: 0.20214812457561493\n",
      "logits: tensor([12321.2949])\n",
      "MAPE loss: 0.9449364542961121\n",
      "logits: tensor([24021.7988])\n",
      "MAPE loss: 0.3167784512042999\n",
      "logits: tensor([7607.8115])\n",
      "MAPE loss: 0.2009054273366928\n",
      "logits: tensor([9371.0391])\n",
      "MAPE loss: 0.4792337715625763\n",
      "logits: tensor([7499.3560])\n",
      "MAPE loss: 0.183785542845726\n",
      "logits: tensor([5931.4189])\n",
      "MAPE loss: 0.0637158676981926\n",
      "logits: tensor([7308.5508])\n",
      "MAPE loss: 0.1536666303873062\n",
      "logits: tensor([17381.6699])\n",
      "MAPE loss: 0.047206729650497437\n",
      "logits: tensor([9797.5176])\n",
      "MAPE loss: 0.5465540885925293\n",
      "logits: tensor([14727.8613])\n",
      "MAPE loss: 0.19267785549163818\n",
      "logits: tensor([5789.4800])\n",
      "MAPE loss: 0.6826439499855042\n",
      "logits: tensor([5217.3604])\n",
      "MAPE loss: 0.17643117904663086\n",
      "logits: tensor([17912.4902])\n",
      "MAPE loss: 0.018109293654561043\n",
      "logits: tensor([11232.9434])\n",
      "MAPE loss: 0.7731384038925171\n",
      "logits: tensor([7493.9854])\n",
      "MAPE loss: 0.18293778598308563\n",
      "logits: tensor([10392.4541])\n",
      "MAPE loss: 0.6404657959938049\n",
      "logits: tensor([11911.9180])\n",
      "MAPE loss: 0.34703654050827026\n",
      "logits: tensor([9239.4258])\n",
      "MAPE loss: 0.49353182315826416\n",
      "logits: tensor([8468.4336])\n",
      "MAPE loss: 0.33675602078437805\n",
      "logits: tensor([7860.5063])\n",
      "MAPE loss: 0.24079371988773346\n",
      "logits: tensor([7576.9893])\n",
      "MAPE loss: 0.196040078997612\n",
      "logits: tensor([7048.7710])\n",
      "MAPE loss: 0.6136146783828735\n",
      "logits: tensor([9410.9678])\n",
      "MAPE loss: 0.48412856459617615\n",
      "Valing, Epoch: 1, Batch 150: Loss = 78002240.0\n",
      "logits: tensor([14070.3633])\n",
      "MAPE loss: 0.22871924936771393\n",
      "logits: tensor([8462.4609])\n",
      "MAPE loss: 0.5361219048500061\n",
      "logits: tensor([9248.8691])\n",
      "MAPE loss: 0.4930141866207123\n",
      "logits: tensor([11458.1631])\n",
      "MAPE loss: 0.37190955877304077\n",
      "logits: tensor([8225.5010])\n",
      "MAPE loss: 0.29840871691703796\n",
      "logits: tensor([8202.8232])\n",
      "MAPE loss: 0.29482898116111755\n",
      "logits: tensor([6365.8936])\n",
      "MAPE loss: 0.004866654984652996\n",
      "logits: tensor([8162.8857])\n",
      "MAPE loss: 0.5525434017181396\n",
      "logits: tensor([6987.2725])\n",
      "MAPE loss: 0.6169857978820801\n",
      "logits: tensor([16326.0781])\n",
      "MAPE loss: 0.10507003217935562\n",
      "logits: tensor([9625.9668])\n",
      "MAPE loss: 0.5194745063781738\n",
      "logits: tensor([15877.4893])\n",
      "MAPE loss: 0.12965986132621765\n",
      "logits: tensor([4809.2725])\n",
      "MAPE loss: 0.24084851145744324\n",
      "logits: tensor([23671.7129])\n",
      "MAPE loss: 0.29758813977241516\n",
      "logits: tensor([8856.8789])\n",
      "MAPE loss: 0.5145015120506287\n",
      "logits: tensor([13711.0732])\n",
      "MAPE loss: 0.2484140843153\n",
      "logits: tensor([9777.5781])\n",
      "MAPE loss: 0.54340660572052\n",
      "logits: tensor([8437.6455])\n",
      "MAPE loss: 0.5374822020530701\n",
      "logits: tensor([8612.5557])\n",
      "MAPE loss: 0.35950592160224915\n",
      "logits: tensor([6032.2407])\n",
      "MAPE loss: 0.04780098795890808\n",
      "logits: tensor([5951.2993])\n",
      "MAPE loss: 0.060577720403671265\n",
      "logits: tensor([9804.7520])\n",
      "MAPE loss: 0.46254292130470276\n",
      "logits: tensor([7248.4697])\n",
      "MAPE loss: 0.1441827416419983\n",
      "logits: tensor([2827.0537])\n",
      "MAPE loss: 0.5537449717521667\n",
      "logits: tensor([7200.2627])\n",
      "MAPE loss: 0.1365731805562973\n",
      "logits: tensor([24853.9551])\n",
      "MAPE loss: 0.36239391565322876\n",
      "logits: tensor([19383.5449])\n",
      "MAPE loss: 0.06252800673246384\n",
      "logits: tensor([12683.5576])\n",
      "MAPE loss: 0.3047383725643158\n",
      "logits: tensor([13961.9238])\n",
      "MAPE loss: 0.2346634566783905\n",
      "logits: tensor([6026.0693])\n",
      "MAPE loss: 0.04877515137195587\n",
      "logits: tensor([6506.0186])\n",
      "MAPE loss: 0.6433662176132202\n",
      "logits: tensor([11915.2285])\n",
      "MAPE loss: 0.3468550741672516\n",
      "logits: tensor([5285.0664])\n",
      "MAPE loss: 0.1657436639070511\n",
      "logits: tensor([5389.3638])\n",
      "MAPE loss: 0.14928016066551208\n",
      "logits: tensor([8349.2520])\n",
      "MAPE loss: 0.3179430067539215\n",
      "logits: tensor([15440.6719])\n",
      "MAPE loss: 0.1536044329404831\n",
      "logits: tensor([6378.8521])\n",
      "MAPE loss: 0.0069121746346354485\n",
      "logits: tensor([5362.2217])\n",
      "MAPE loss: 0.15356458723545074\n",
      "logits: tensor([6873.6382])\n",
      "MAPE loss: 0.6232147812843323\n",
      "logits: tensor([18001.3320])\n",
      "MAPE loss: 0.013239343650639057\n",
      "logits: tensor([10247.3867])\n",
      "MAPE loss: 0.6175666451454163\n",
      "logits: tensor([11642.8887])\n",
      "MAPE loss: 0.3617836534976959\n",
      "logits: tensor([6888.6719])\n",
      "MAPE loss: 0.0873880609869957\n",
      "logits: tensor([6268.8271])\n",
      "MAPE loss: 0.010455435141921043\n",
      "logits: tensor([10949.0303])\n",
      "MAPE loss: 0.39981818199157715\n",
      "logits: tensor([8717.6543])\n",
      "MAPE loss: 0.3760959208011627\n",
      "logits: tensor([6038.0615])\n",
      "MAPE loss: 0.04688216373324394\n",
      "logits: tensor([6575.7568])\n",
      "MAPE loss: 0.037993915379047394\n",
      "logits: tensor([17419.3301])\n",
      "MAPE loss: 0.04514235258102417\n",
      "logits: tensor([3270.6548])\n",
      "MAPE loss: 0.4837218225002289\n",
      "Valing, Epoch: 1, Batch 200: Loss = 9390598.0\n",
      "logits: tensor([6946.4531])\n",
      "MAPE loss: 0.6192233562469482\n",
      "logits: tensor([7722.2529])\n",
      "MAPE loss: 0.2189701944589615\n",
      "logits: tensor([8768.8584])\n",
      "MAPE loss: 0.5193264484405518\n",
      "logits: tensor([18632.7363])\n",
      "MAPE loss: 0.02137170173227787\n",
      "logits: tensor([7769.2983])\n",
      "MAPE loss: 0.226396381855011\n",
      "logits: tensor([11847.1035])\n",
      "MAPE loss: 0.35058942437171936\n",
      "logits: tensor([10022.5205])\n",
      "MAPE loss: 0.5820711851119995\n",
      "logits: tensor([7960.7163])\n",
      "MAPE loss: 0.25661203265190125\n",
      "logits: tensor([10275.0156])\n",
      "MAPE loss: 0.6219279170036316\n",
      "logits: tensor([10613.2793])\n",
      "MAPE loss: 0.6753234267234802\n",
      "logits: tensor([4788.9219])\n",
      "MAPE loss: 0.24406088888645172\n",
      "logits: tensor([3208.7122])\n",
      "MAPE loss: 0.4934995770454407\n",
      "logits: tensor([7678.7783])\n",
      "MAPE loss: 0.21210765838623047\n",
      "logits: tensor([10326.6895])\n",
      "MAPE loss: 0.4339323937892914\n",
      "logits: tensor([8673.1699])\n",
      "MAPE loss: 0.3690739870071411\n",
      "logits: tensor([17354.7637])\n",
      "MAPE loss: 0.04868162050843239\n",
      "logits: tensor([9628.8994])\n",
      "MAPE loss: 0.5199374556541443\n",
      "logits: tensor([10457.3799])\n",
      "MAPE loss: 0.4267684817314148\n",
      "logits: tensor([8736.9443])\n",
      "MAPE loss: 0.3791408836841583\n",
      "logits: tensor([10378.8232])\n",
      "MAPE loss: 0.6383141279220581\n",
      "logits: tensor([19639.5332])\n",
      "MAPE loss: 0.07656025886535645\n",
      "logits: tensor([14955.0449])\n",
      "MAPE loss: 0.18022455275058746\n",
      "logits: tensor([9638.9619])\n",
      "MAPE loss: 0.47163087129592896\n",
      "logits: tensor([10673.5078])\n",
      "MAPE loss: 0.41492122411727905\n",
      "logits: tensor([9171.2168])\n",
      "MAPE loss: 0.44769150018692017\n",
      "logits: tensor([14758.1406])\n",
      "MAPE loss: 0.19101805984973907\n",
      "logits: tensor([5610.5596])\n",
      "MAPE loss: 0.6924516558647156\n",
      "logits: tensor([6886.3779])\n",
      "MAPE loss: 0.0870259627699852\n",
      "logits: tensor([9338.9512])\n",
      "MAPE loss: 0.48807623982429504\n",
      "logits: tensor([10217.0176])\n",
      "MAPE loss: 0.6127728223800659\n",
      "logits: tensor([8917.6699])\n",
      "MAPE loss: 0.40766870975494385\n",
      "logits: tensor([8627.0957])\n",
      "MAPE loss: 0.3618010878562927\n",
      "logits: tensor([6964.4414])\n",
      "MAPE loss: 0.09934841096401215\n",
      "logits: tensor([6725.1191])\n",
      "MAPE loss: 0.06157099828124046\n",
      "logits: tensor([16199.0811])\n",
      "MAPE loss: 0.1120314970612526\n",
      "logits: tensor([8304.3320])\n",
      "MAPE loss: 0.3108523190021515\n",
      "logits: tensor([7038.5371])\n",
      "MAPE loss: 0.11104453355073929\n",
      "logits: tensor([5717.0479])\n",
      "MAPE loss: 0.09755469113588333\n",
      "logits: tensor([4447.1914])\n",
      "MAPE loss: 0.298003613948822\n",
      "logits: tensor([15424.0303])\n",
      "MAPE loss: 0.15451666712760925\n",
      "logits: tensor([7588.7568])\n",
      "MAPE loss: 0.19789761304855347\n",
      "logits: tensor([15406.5137])\n",
      "MAPE loss: 0.15547685325145721\n",
      "logits: tensor([14311.3037])\n",
      "MAPE loss: 0.21551185846328735\n",
      "logits: tensor([14720.0117])\n",
      "MAPE loss: 0.19310812652111053\n",
      "logits: tensor([13289.5049])\n",
      "MAPE loss: 0.27152276039123535\n",
      "logits: tensor([15341.1533])\n",
      "MAPE loss: 0.15905964374542236\n",
      "logits: tensor([19463.0020])\n",
      "MAPE loss: 0.06688352674245834\n",
      "logits: tensor([22847.0137])\n",
      "MAPE loss: 0.25238144397735596\n",
      "logits: tensor([9865.0254])\n",
      "MAPE loss: 0.5572103261947632\n",
      "logits: tensor([9114.5498])\n",
      "MAPE loss: 0.43874651193618774\n",
      "Valing, Epoch: 1, Batch 250: Loss = 7725547.0\n",
      "logits: tensor([18531.3887])\n",
      "MAPE loss: 0.015816230326890945\n",
      "logits: tensor([6227.1592])\n",
      "MAPE loss: 0.017032790929079056\n",
      "logits: tensor([13300.3574])\n",
      "MAPE loss: 0.27092787623405457\n",
      "logits: tensor([6640.9502])\n",
      "MAPE loss: 0.6359698176383972\n",
      "logits: tensor([5812.2485])\n",
      "MAPE loss: 0.08252710849046707\n",
      "logits: tensor([11636.2930])\n",
      "MAPE loss: 0.36214518547058105\n",
      "logits: tensor([13396.0361])\n",
      "MAPE loss: 0.2656831443309784\n",
      "logits: tensor([8925.0078])\n",
      "MAPE loss: 0.408827006816864\n",
      "logits: tensor([8607.3711])\n",
      "MAPE loss: 0.35868752002716064\n",
      "logits: tensor([6173.9233])\n",
      "MAPE loss: 0.0254361554980278\n",
      "logits: tensor([17836.2598])\n",
      "MAPE loss: 0.022287942469120026\n",
      "logits: tensor([11860.9004])\n",
      "MAPE loss: 0.3498331308364868\n",
      "logits: tensor([7063.3652])\n",
      "MAPE loss: 0.11496369540691376\n",
      "logits: tensor([9627.1719])\n",
      "MAPE loss: 0.5196647644042969\n",
      "logits: tensor([7690.4902])\n",
      "MAPE loss: 0.21395640075206757\n",
      "logits: tensor([5644.3198])\n",
      "MAPE loss: 0.1090349331498146\n",
      "logits: tensor([17429.9473])\n",
      "MAPE loss: 0.04456035792827606\n",
      "logits: tensor([5603.0723])\n",
      "MAPE loss: 0.11554592847824097\n",
      "logits: tensor([6172.0361])\n",
      "MAPE loss: 0.025734053924679756\n",
      "logits: tensor([10512.4043])\n",
      "MAPE loss: 0.4237522482872009\n",
      "logits: tensor([8838.1973])\n",
      "MAPE loss: 0.5155255794525146\n",
      "logits: tensor([9154.4961])\n",
      "MAPE loss: 0.4981873333454132\n",
      "logits: tensor([15909.3711])\n",
      "MAPE loss: 0.127912238240242\n",
      "logits: tensor([18271.0293])\n",
      "MAPE loss: 0.0015443760203197598\n",
      "logits: tensor([13839.7051])\n",
      "MAPE loss: 0.24136300384998322\n",
      "logits: tensor([8843.9316])\n",
      "MAPE loss: 0.5152112245559692\n",
      "logits: tensor([8932.1104])\n",
      "MAPE loss: 0.4099481403827667\n",
      "logits: tensor([8404.5029])\n",
      "MAPE loss: 0.539298951625824\n",
      "logits: tensor([8140.6621])\n",
      "MAPE loss: 0.5537616014480591\n",
      "logits: tensor([8340.2803])\n",
      "MAPE loss: 0.5428193807601929\n",
      "logits: tensor([7830.6382])\n",
      "MAPE loss: 0.5707558393478394\n",
      "logits: tensor([8913.4658])\n",
      "MAPE loss: 0.40700507164001465\n",
      "logits: tensor([7481.0322])\n",
      "MAPE loss: 0.18089310824871063\n",
      "logits: tensor([14949.7500])\n",
      "MAPE loss: 0.18051479756832123\n",
      "logits: tensor([10221.7109])\n",
      "MAPE loss: 0.613513708114624\n",
      "logits: tensor([9663.7422])\n",
      "MAPE loss: 0.5254374146461487\n",
      "logits: tensor([14127.3652])\n",
      "MAPE loss: 0.225594624876976\n",
      "logits: tensor([13372.5371])\n",
      "MAPE loss: 0.26697126030921936\n",
      "logits: tensor([11552.0654])\n",
      "MAPE loss: 0.8235123157501221\n",
      "logits: tensor([6799.4756])\n",
      "MAPE loss: 0.07330828160047531\n",
      "logits: tensor([15946.4082])\n",
      "MAPE loss: 0.12588201463222504\n",
      "logits: tensor([8744.9609])\n",
      "MAPE loss: 0.5206363797187805\n",
      "logits: tensor([20928.5352])\n",
      "MAPE loss: 0.14721816778182983\n",
      "logits: tensor([11618.2461])\n",
      "MAPE loss: 0.3631344437599182\n",
      "logits: tensor([7304.0518])\n",
      "MAPE loss: 0.15295645594596863\n",
      "logits: tensor([11022.3760])\n",
      "MAPE loss: 0.7398999929428101\n",
      "logits: tensor([7407.1816])\n",
      "MAPE loss: 0.5939680933952332\n",
      "logits: tensor([4361.5254])\n",
      "MAPE loss: 0.3115261197090149\n",
      "logits: tensor([4394.5693])\n",
      "MAPE loss: 0.30631008744239807\n",
      "logits: tensor([19033.6523])\n",
      "MAPE loss: 0.04334830492734909\n",
      "Valing, Epoch: 1, Batch 300: Loss = 625359.6875\n",
      "logits: tensor([11512.6816])\n",
      "MAPE loss: 0.36892107129096985\n",
      "logits: tensor([9208.7441])\n",
      "MAPE loss: 0.4536152482032776\n",
      "logits: tensor([9023.6270])\n",
      "MAPE loss: 0.4243941903114319\n",
      "logits: tensor([14099.9043])\n",
      "MAPE loss: 0.2270999252796173\n",
      "logits: tensor([14339.1064])\n",
      "MAPE loss: 0.2139878273010254\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = main(epochs=epochs, train_dl=train_dataloader, val_dl=val_dataloader, model=nn_model, optimizer=optimizer, criterion=loss_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Curve Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=0<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "0",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "0",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899,
          900,
          901,
          902,
          903,
          904,
          905,
          906,
          907,
          908,
          909,
          910,
          911,
          912,
          913,
          914,
          915,
          916,
          917,
          918,
          919,
          920,
          921,
          922,
          923,
          924,
          925,
          926,
          927,
          928,
          929,
          930,
          931,
          932,
          933,
          934,
          935,
          936,
          937,
          938,
          939,
          940,
          941,
          942,
          943,
          944,
          945,
          946,
          947,
          948,
          949,
          950,
          951,
          952,
          953,
          954,
          955,
          956,
          957,
          958,
          959,
          960,
          961,
          962,
          963,
          964,
          965,
          966,
          967,
          968,
          969,
          970,
          971,
          972,
          973,
          974,
          975,
          976,
          977,
          978,
          979,
          980,
          981,
          982,
          983,
          984,
          985,
          986,
          987,
          988,
          989,
          990,
          991,
          992,
          993,
          994,
          995,
          996,
          997,
          998,
          999,
          1000,
          1001,
          1002,
          1003,
          1004,
          1005,
          1006,
          1007,
          1008,
          1009,
          1010,
          1011,
          1012,
          1013,
          1014,
          1015,
          1016,
          1017,
          1018,
          1019,
          1020,
          1021,
          1022,
          1023,
          1024,
          1025,
          1026,
          1027,
          1028,
          1029,
          1030,
          1031,
          1032,
          1033,
          1034,
          1035,
          1036,
          1037,
          1038,
          1039,
          1040,
          1041,
          1042,
          1043,
          1044,
          1045,
          1046,
          1047,
          1048,
          1049,
          1050,
          1051,
          1052,
          1053,
          1054,
          1055,
          1056,
          1057,
          1058,
          1059,
          1060,
          1061,
          1062,
          1063,
          1064,
          1065,
          1066,
          1067,
          1068,
          1069,
          1070,
          1071,
          1072,
          1073,
          1074,
          1075,
          1076,
          1077,
          1078,
          1079,
          1080,
          1081,
          1082,
          1083,
          1084,
          1085,
          1086,
          1087,
          1088,
          1089,
          1090,
          1091,
          1092,
          1093,
          1094,
          1095,
          1096,
          1097,
          1098,
          1099,
          1100,
          1101,
          1102,
          1103,
          1104,
          1105,
          1106,
          1107,
          1108,
          1109,
          1110,
          1111,
          1112,
          1113,
          1114,
          1115,
          1116,
          1117,
          1118,
          1119,
          1120,
          1121,
          1122,
          1123,
          1124,
          1125,
          1126,
          1127,
          1128,
          1129,
          1130,
          1131,
          1132,
          1133,
          1134,
          1135,
          1136,
          1137,
          1138,
          1139,
          1140,
          1141,
          1142,
          1143,
          1144,
          1145,
          1146,
          1147,
          1148,
          1149,
          1150,
          1151,
          1152,
          1153,
          1154,
          1155,
          1156,
          1157,
          1158,
          1159,
          1160,
          1161,
          1162,
          1163,
          1164,
          1165,
          1166,
          1167,
          1168,
          1169,
          1170,
          1171,
          1172,
          1173,
          1174,
          1175,
          1176,
          1177,
          1178,
          1179,
          1180,
          1181,
          1182,
          1183,
          1184,
          1185,
          1186,
          1187,
          1188,
          1189,
          1190,
          1191,
          1192,
          1193,
          1194,
          1195,
          1196,
          1197,
          1198,
          1199,
          1200,
          1201,
          1202,
          1203,
          1204,
          1205,
          1206,
          1207,
          1208,
          1209,
          1210,
          1211,
          1212,
          1213,
          1214,
          1215,
          1216,
          1217,
          1218,
          1219,
          1220,
          1221,
          1222,
          1223,
          1224,
          1225,
          1226,
          1227,
          1228
         ],
         "xaxis": "x",
         "y": [
          1.0000219345092773,
          0.9916427135467529,
          0.9554659724235535,
          0.9940953850746155,
          0.8188666105270386,
          0.8880549073219299,
          0.059899527579545975,
          0.47888004779815674,
          1.1406179666519165,
          0.4542464315891266,
          0.13006041944026947,
          0.021122673526406288,
          0.5459228157997131,
          0.26975563168525696,
          0.7205497622489929,
          0.15734981000423431,
          0.7076377272605896,
          0.8750724792480469,
          0.7273396253585815,
          0.7269487380981445,
          0.027035174891352654,
          0.48439520597457886,
          0.6977373361587524,
          0.510209858417511,
          0.6781986355781555,
          0.6842751502990723,
          0.5965504050254822,
          0.7072188258171082,
          0.36327695846557617,
          0.48502400517463684,
          0.3802867829799652,
          0.3916538655757904,
          2.0036041736602783,
          0.3141103982925415,
          0.4533005356788635,
          0.5320723652839661,
          0.5077403783798218,
          0.5000072121620178,
          0.45167067646980286,
          0.5049024224281311,
          0.4307093918323517,
          0.5822591781616211,
          0.9145010709762573,
          0.7941117882728577,
          0.7606931328773499,
          0.8262620568275452,
          0.5918679237365723,
          0.7858644127845764,
          0.6583988070487976,
          0.004014811012893915,
          0.3761083781719208,
          0.6523128151893616,
          0.6742688417434692,
          0.4396340847015381,
          1.0557832717895508,
          0.1446019560098648,
          0.5969732403755188,
          0.2989127039909363,
          2.088658332824707,
          0.46468567848205566,
          0.9975314140319824,
          0.42344847321510315,
          0.31239065527915955,
          0.5669916272163391,
          0.9280962347984314,
          0.8454020023345947,
          0.829001784324646,
          0.7248730063438416,
          0.8597204089164734,
          0.6808794736862183,
          0.6863744854927063,
          0.9062994122505188,
          0.47552651166915894,
          0.9231739044189453,
          0.37750035524368286,
          0.4922006130218506,
          0.31804630160331726,
          0.40215015411376953,
          0.2773198187351227,
          0.9679120779037476,
          0.15567545592784882,
          0.7058451771736145,
          0.5772280693054199,
          0.16124826669692993,
          0.5516402125358582,
          0.24942266941070557,
          0.5455959439277649,
          0.3219684064388275,
          0.5986685752868652,
          0.4669388234615326,
          0.1430736929178238,
          0.8356509804725647,
          0.6105442047119141,
          0.8819795846939087,
          0.1839931756258011,
          0.5295064449310303,
          0.373496413230896,
          0.6239397525787354,
          0.7380717992782593,
          0.4665204882621765,
          0.8466097116470337,
          0.28341004252433777,
          0.6320981979370117,
          0.5047815442085266,
          0.5932438969612122,
          0.5755007266998291,
          0.20444422960281372,
          0.07075760513544083,
          0.650425136089325,
          0.6507689356803894,
          1.2819998264312744,
          0.8964548707008362,
          0.7982120513916016,
          0.24874410033226013,
          0.5461005568504333,
          0.4527615010738373,
          0.9139885902404785,
          0.006974905263632536,
          0.6763337254524231,
          0.134950652718544,
          0.770599365234375,
          0.3554123640060425,
          0.5365436673164368,
          0.04665378853678703,
          0.2652641534805298,
          0.05611324682831764,
          0.46167346835136414,
          1.4334410429000854,
          0.07350844144821167,
          0.1770654171705246,
          0.5861415863037109,
          0.12669053673744202,
          0.5642860531806946,
          0.9128984808921814,
          0.29430443048477173,
          0.19591878354549408,
          0.8255599141120911,
          0.6755244135856628,
          1.0072815418243408,
          0.1911226063966751,
          0.3013351261615753,
          0.7123463749885559,
          0.22219350934028625,
          0.6033529043197632,
          0.8990907669067383,
          1.003036379814148,
          0.2308398187160492,
          1.0106933116912842,
          0.7802683115005493,
          1.002387285232544,
          0.7417197227478027,
          0.9009478688240051,
          0.6946225166320801,
          0.6929285526275635,
          0.7793466448783875,
          0.6506795287132263,
          0.7337077856063843,
          0.9283533692359924,
          0.596578061580658,
          0.8090308308601379,
          0.23918776214122772,
          0.2826906144618988,
          0.08142908662557602,
          0.5027477741241455,
          0.23474012315273285,
          0.47153979539871216,
          1.5490531921386719,
          0.4011160433292389,
          0.5238226652145386,
          0.5036541223526001,
          0.1385871022939682,
          0.7390760183334351,
          0.5105196833610535,
          0.14984281361103058,
          0.6326988935470581,
          0.5122566819190979,
          0.33519405126571655,
          0.9347264766693115,
          0.39853721857070923,
          0.52920001745224,
          0.09929218888282776,
          1.512749195098877,
          0.15660791099071503,
          0.12982292473316193,
          0.23980878293514252,
          1.833327054977417,
          0.6238862872123718,
          0.07858513295650482,
          0.3325956463813782,
          0.053081925958395004,
          0.4679076075553894,
          0.3676823079586029,
          0.5448495745658875,
          0.30790001153945923,
          0.24222347140312195,
          0.463723361492157,
          0.09902229905128479,
          0.24670065939426422,
          0.5160186886787415,
          0.7502350807189941,
          0.3617759943008423,
          0.5468770861625671,
          1.4564948081970215,
          0.7610254287719727,
          1.6127755641937256,
          0.0371081717312336,
          0.045674774795770645,
          0.3611479103565216,
          0.698615550994873,
          0.7046037316322327,
          0.5571401715278625,
          0.24180421233177185,
          0.05922180041670799,
          0.32310354709625244,
          0.5285301208496094,
          0.18420454859733582,
          1.2035564184188843,
          0.2774227559566498,
          0.492672860622406,
          1.1041589975357056,
          0.5685820579528809,
          1.5559906959533691,
          0.11264383792877197,
          0.14540386199951172,
          0.8092074394226074,
          0.3343474566936493,
          0.701816737651825,
          0.009415448643267155,
          0.694787859916687,
          0.2309853881597519,
          0.7132643461227417,
          0.21831904351711273,
          0.6243077516555786,
          0.2617412805557251,
          1.0022971630096436,
          0.4346802830696106,
          3.635181188583374,
          0.36922261118888855,
          0.1300138682126999,
          0.5001373887062073,
          1.1523194313049316,
          0.2234174758195877,
          0.06732665747404099,
          0.026408854871988297,
          0.7292199730873108,
          0.37656936049461365,
          0.6359787583351135,
          0.7878544330596924,
          0.6603260636329651,
          0.5197328329086304,
          0.46609193086624146,
          0.14157526195049286,
          0.05648405849933624,
          0.2526721656322479,
          0.43014201521873474,
          0.6772339940071106,
          0.011759740300476551,
          1.5490427017211914,
          0.050243984907865524,
          0.4988640546798706,
          0.08206123858690262,
          1.3146780729293823,
          2.542037010192871,
          0.42762938141822815,
          0.10151544958353043,
          0.4119194447994232,
          0.5689876675605774,
          0.37583860754966736,
          0.23868349194526672,
          0.10040357708930969,
          0.7560923099517822,
          0.5682185888290405,
          0.19108624756336212,
          0.27028128504753113,
          0.6307090520858765,
          0.6825029253959656,
          0.422006219625473,
          0.22070220112800598,
          0.18588142096996307,
          0.6906334757804871,
          1.1243683099746704,
          0.1095002219080925,
          0.1579267680644989,
          1.3975720405578613,
          0.548335075378418,
          1.0144495964050293,
          0.232938751578331,
          0.5762152075767517,
          0.5279026627540588,
          0.41432541608810425,
          0.4205167293548584,
          0.6988275647163391,
          0.5806070566177368,
          0.5948336720466614,
          0.6679217219352722,
          0.015527959913015366,
          0.6453338265419006,
          0.4483601450920105,
          0.8405125141143799,
          0.28258809447288513,
          1.8066707849502563,
          0.034801725298166275,
          0.8431036472320557,
          0.071611687541008,
          0.6325744986534119,
          0.7582565546035767,
          1.0065226554870605,
          0.5050201416015625,
          0.7719635963439941,
          0.47065573930740356,
          0.48972445726394653,
          0.6991575360298157,
          0.11498157680034637,
          0.4729478359222412,
          0.3307758569717407,
          0.6442753672599792,
          0.6896385550498962,
          0.2293458878993988,
          0.023381799459457397,
          0.023001940920948982,
          0.20744439959526062,
          0.26312634348869324,
          1.155521273612976,
          0.43839165568351746,
          0.18801215291023254,
          0.4216279685497284,
          0.2913588881492615,
          0.22808800637722015,
          0.5374566316604614,
          0.5435143709182739,
          0.5669344067573547,
          0.43323397636413574,
          0.56034916639328,
          0.2527322471141815,
          0.30298811197280884,
          0.582143247127533,
          1.0825098752975464,
          0.06656340509653091,
          0.6926432847976685,
          0.21034839749336243,
          0.05436289310455322,
          0.20556475222110748,
          0.574617326259613,
          0.1938355565071106,
          0.039662688970565796,
          0.5269081592559814,
          0.2587994337081909,
          0.6974515318870544,
          0.7354109883308411,
          0.10608284175395966,
          0.2893063426017761,
          0.1180209144949913,
          0.03563639149069786,
          0.6356740593910217,
          0.5969575047492981,
          0.09100300818681717,
          0.3153785467147827,
          0.4895288050174713,
          0.3974262475967407,
          0.3508790135383606,
          0.12322203814983368,
          0.3122040331363678,
          0.2951399087905884,
          0.06605073064565659,
          0.18600401282310486,
          0.3517577350139618,
          0.10390922427177429,
          0.17060911655426025,
          0.2952680289745331,
          0.24062807857990265,
          0.2391059696674347,
          0.3347742557525635,
          0.24040061235427856,
          0.5441713929176331,
          0.5224291682243347,
          0.35009339451789856,
          1.1697635650634766,
          1.0264179706573486,
          0.1638258695602417,
          0.1940479278564453,
          0.016999376937747,
          0.5696246027946472,
          0.052214205265045166,
          0.4234505593776703,
          0.40108680725097656,
          0.7553025484085083,
          0.2056993991136551,
          0.1336010992527008,
          0.5683211088180542,
          0.04644707217812538,
          0.057570986449718475,
          0.4053744375705719,
          0.20271024107933044,
          0.12156117707490921,
          0.901378333568573,
          0.42589500546455383,
          0.10383142530918121,
          0.7278721928596497,
          0.4337705373764038,
          0.12040610611438751,
          0.7312960624694824,
          0.5983625650405884,
          0.7429268956184387,
          0.33542823791503906,
          0.775023341178894,
          0.850867509841919,
          0.21696244180202484,
          0.43096497654914856,
          0.07891532778739929,
          0.4755036234855652,
          0.24108009040355682,
          0.2633480131626129,
          0.4462125599384308,
          0.3131409287452698,
          0.6224732995033264,
          0.4250383973121643,
          0.5387638211250305,
          0.5376846194267273,
          0.48164552450180054,
          0.08839408308267593,
          0.5211972594261169,
          0.4927276074886322,
          0.5409421920776367,
          1.1281479597091675,
          0.5740256905555725,
          0.33417966961860657,
          0.03658820316195488,
          0.5692785382270813,
          0.5506106019020081,
          0.49599340558052063,
          0.2571006119251251,
          0.29737284779548645,
          0.09907972067594528,
          0.23559266328811646,
          0.6068393588066101,
          0.4961126148700714,
          0.7734960317611694,
          0.26417264342308044,
          0.8164578676223755,
          0.1987265646457672,
          0.5057837963104248,
          0.5479919910430908,
          0.23791131377220154,
          0.3777255713939667,
          0.1715584397315979,
          0.36216026544570923,
          0.43406492471694946,
          0.8099574446678162,
          0.4649227559566498,
          0.3990924656391144,
          0.5586443543434143,
          0.09383446723222733,
          0.49252259731292725,
          0.00004316255581215955,
          0.849764883518219,
          0.06683520972728729,
          0.6185448169708252,
          0.4145351052284241,
          0.19559766352176666,
          0.5556361079216003,
          0.12330539524555206,
          0.6699219346046448,
          0.06920292973518372,
          0.10923144221305847,
          0.5766394734382629,
          0.5444393754005432,
          0.026969505473971367,
          0.40154802799224854,
          0.3932839035987854,
          0.3792873024940491,
          0.4478949010372162,
          0.08822603523731232,
          0.3529217839241028,
          0.5420203804969788,
          0.4861050546169281,
          0.3148491680622101,
          0.49624761939048767,
          0.19634559750556946,
          0.0556536428630352,
          0.4078700840473175,
          0.917293131351471,
          1.028435468673706,
          0.423490047454834,
          0.35800230503082275,
          0.7248629331588745,
          0.7296602725982666,
          0.25260230898857117,
          0.6210406422615051,
          0.11301366984844208,
          0.48655301332473755,
          0.1387600600719452,
          0.5197167992591858,
          0.20474779605865479,
          0.2786167562007904,
          1.2084455490112305,
          0.18288852274417877,
          0.6154617071151733,
          0.5543721914291382,
          0.39885213971138,
          0.7099440693855286,
          0.08206509053707123,
          0.3601042628288269,
          0.016056692227721214,
          1.2467900514602661,
          0.17475377023220062,
          0.38762199878692627,
          0.1026453822851181,
          0.05343932658433914,
          0.029401252046227455,
          0.5841116309165955,
          0.31691819429397583,
          0.17375756800174713,
          0.0223276037722826,
          0.12328419089317322,
          0.42896604537963867,
          0.400646835565567,
          0.7916175723075867,
          0.33107587695121765,
          0.14303378760814667,
          0.424655020236969,
          0.28347185254096985,
          0.3927080035209656,
          0.5848546624183655,
          1.144917368888855,
          0.210361510515213,
          0.3667171597480774,
          0.05491158738732338,
          1.0241502523422241,
          1.0433892011642456,
          0.1097196415066719,
          0.3865366280078888,
          0.08906909078359604,
          0.27159976959228516,
          0.7397797107696533,
          0.6091729998588562,
          0.7995331883430481,
          0.689402163028717,
          0.3611818552017212,
          0.8242165446281433,
          0.5902225971221924,
          0.8093901872634888,
          0.33450302481651306,
          0.519474983215332,
          0.29539087414741516,
          0.8631201386451721,
          0.13604559004306793,
          0.11444409936666489,
          0.62455815076828,
          0.24294854700565338,
          0.2326466292142868,
          0.14193977415561676,
          0.7986838221549988,
          0.45855972170829773,
          0.8499776721000671,
          0.8978604674339294,
          0.24234509468078613,
          0.4783366620540619,
          0.6265402436256409,
          0.5500478148460388,
          0.41756772994995117,
          0.70947265625,
          0.21735621988773346,
          0.678226888179779,
          0.5825237035751343,
          0.23612722754478455,
          0.6118757724761963,
          0.48104533553123474,
          0.6296490430831909,
          0.610106348991394,
          0.37938418984413147,
          0.24006935954093933,
          0.28679367899894714,
          0.9574082493782043,
          1.5696988105773926,
          0.10243198275566101,
          0.19461369514465332,
          1.6122196912765503,
          0.4465421438217163,
          0.8570220470428467,
          0.2200908660888672,
          0.1527247577905655,
          0.7751067280769348,
          0.4664962589740753,
          0.7267002463340759,
          0.44121912121772766,
          0.07799617946147919,
          0.3118493854999542,
          0.32936471700668335,
          0.47947365045547485,
          0.10121418535709381,
          0.841121256351471,
          0.4176073968410492,
          0.21980629861354828,
          0.5202930569648743,
          0.4844529628753662,
          0.53715980052948,
          0.13630394637584686,
          0.38632288575172424,
          0.7562395334243774,
          0.2538212537765503,
          0.3387695550918579,
          0.6706540584564209,
          0.006012734957039356,
          0.7383051514625549,
          0.4226336181163788,
          0.28664442896842957,
          0.028911789879202843,
          0.5721453428268433,
          0.26405414938926697,
          0.32215002179145813,
          0.3239716589450836,
          0.33542367815971375,
          0.7512875199317932,
          0.7527630925178528,
          0.20755861699581146,
          0.3867761790752411,
          0.724882960319519,
          0.4059297740459442,
          1.2309387922286987,
          0.183614119887352,
          0.05889398977160454,
          1.2851455211639404,
          0.2896416187286377,
          0.053126245737075806,
          0.15479795634746552,
          0.7482832074165344,
          0.43567338585853577,
          0.5221043825149536,
          0.0378921739757061,
          0.29911473393440247,
          0.45233237743377686,
          0.4900592267513275,
          0.5050675272941589,
          0.015065507963299751,
          0.5951219797134399,
          0.042929552495479584,
          0.33655408024787903,
          0.1320064216852188,
          0.551027774810791,
          0.09877395629882812,
          0.20034894347190857,
          1.568985104560852,
          0.8552345037460327,
          0.7852180600166321,
          0.5834358334541321,
          0.3962199091911316,
          0.2396622598171234,
          0.20734289288520813,
          0.7395385503768921,
          0.17518794536590576,
          0.47232186794281006,
          0.3719421327114105,
          0.4847598671913147,
          0.6897347569465637,
          0.4113343358039856,
          0.6120129227638245,
          0.45460808277130127,
          0.674697995185852,
          0.5213989019393921,
          0.6440584063529968,
          0.16703569889068604,
          0.5962903499603271,
          0.296828955411911,
          0.02545442059636116,
          0.05011551082134247,
          0.09749803692102432,
          1.052610993385315,
          0.4616559147834778,
          0.7385056614875793,
          0.4376997649669647,
          0.5348909497261047,
          0.43151864409446716,
          0.27111896872520447,
          0.19408628344535828,
          0.4943869709968567,
          0.47915929555892944,
          0.6318379044532776,
          0.012118272483348846,
          0.39387786388397217,
          0.5593633651733398,
          0.657996416091919,
          0.6134451031684875,
          0.821639358997345,
          0.8776984214782715,
          0.3530382513999939,
          0.49512988328933716,
          0.0455770380795002,
          0.7692623734474182,
          0.16901811957359314,
          0.5251489877700806,
          0.17554517090320587,
          0.4034230411052704,
          0.22924675047397614,
          1.002846360206604,
          0.36457106471061707,
          0.08364671468734741,
          0.10183526575565338,
          0.3202992379665375,
          0.397190123796463,
          0.07650948315858841,
          0.6120181679725647,
          0.14284023642539978,
          0.06066780164837837,
          0.8008401393890381,
          0.51491379737854,
          0.437915176153183,
          0.25177618861198425,
          0.14471594989299774,
          0.39494654536247253,
          0.011829545721411705,
          0.5219266414642334,
          0.0469224788248539,
          0.4564143419265747,
          0.07170476019382477,
          0.38274309039115906,
          0.026294860988855362,
          0.8232476711273193,
          0.4266434907913208,
          0.28435420989990234,
          0.3298286497592926,
          0.08921439945697784,
          0.012579838745296001,
          0.35969823598861694,
          0.05897920951247215,
          0.48325204849243164,
          0.4104417562484741,
          0.15977953374385834,
          0.09513698518276215,
          0.20989850163459778,
          1.6344348192214966,
          0.7069069743156433,
          0.19893459975719452,
          0.07212524116039276,
          0.6301794052124023,
          0.8622266054153442,
          0.5386908650398254,
          0.7117677330970764,
          0.017621420323848724,
          0.7746241092681885,
          1.0166553258895874,
          0.705912709236145,
          0.7688079476356506,
          0.46732160449028015,
          0.7484858632087708,
          0.718819260597229,
          0.7692344784736633,
          0.22133834660053253,
          0.786242663860321,
          0.26532191038131714,
          0.5803425908088684,
          0.6017047166824341,
          0.41382429003715515,
          0.016387974843382835,
          0.5110742449760437,
          0.3574708104133606,
          0.46021848917007446,
          0.2135028839111328,
          0.5120267271995544,
          0.04991573095321655,
          0.044977616518735886,
          0.005364150274544954,
          0.589689314365387,
          0.5655614733695984,
          1.1142791509628296,
          0.6754966974258423,
          1.0488277673721313,
          0.20944714546203613,
          0.5865235328674316,
          0.13855503499507904,
          0.3776605725288391,
          0.32676249742507935,
          0.13374526798725128,
          0.38271212577819824,
          0.20654527842998505,
          0.10839265584945679,
          0.787348747253418,
          0.581360399723053,
          0.27112898230552673,
          0.5655203461647034,
          0.11973153799772263,
          0.42329883575439453,
          0.06906278431415558,
          0.613762378692627,
          0.48454198241233826,
          0.681635856628418,
          0.5527499318122864,
          0.2066344916820526,
          0.022185862064361572,
          0.09581825137138367,
          0.042628951370716095,
          0.24681489169597626,
          0.0857110470533371,
          0.024395860731601715,
          0.30619126558303833,
          1.4058016538619995,
          0.9697467684745789,
          0.29708778858184814,
          0.4308297634124756,
          0.14108212292194366,
          0.36289137601852417,
          0.09685191512107849,
          0.972176730632782,
          0.8839508891105652,
          1.0003700256347656,
          0.8380652070045471,
          0.8632335662841797,
          0.8607273697853088,
          0.20255230367183685,
          0.6380136609077454,
          0.7262158393859863,
          0.9310997128486633,
          0.7317909598350525,
          0.10778923332691193,
          0.6098304390907288,
          0.5587123036384583,
          0.7712652683258057,
          0.1682531088590622,
          0.6976838707923889,
          0.5248631834983826,
          0.36644986271858215,
          0.6826626062393188,
          0.9542439579963684,
          0.3988240957260132,
          0.5127003192901611,
          0.46580970287323,
          0.30355414748191833,
          0.0988413468003273,
          0.3499656617641449,
          0.008251410908997059,
          0.5291657447814941,
          1.1611353158950806,
          0.7684808373451233,
          0.8459818363189697,
          2.2525787353515625,
          2.086617946624756,
          1.3241811990737915,
          1.1324083805084229,
          0.5705864429473877,
          0.4634593725204468,
          0.5630573034286499,
          0.02530520409345627,
          0.8782223463058472,
          0.7963356971740723,
          0.19294771552085876,
          0.7069859504699707,
          0.5877599716186523,
          0.49842825531959534,
          0.5798456072807312,
          0.9098816514015198,
          0.502949059009552,
          0.16475071012973785,
          0.4205971658229828,
          0.467642217874527,
          0.09224983304738998,
          0.37210896611213684,
          0.7368285059928894,
          0.5713918209075928,
          0.3720022141933441,
          0.1901889145374298,
          0.7296484112739563,
          0.9450056552886963,
          0.0513961985707283,
          0.5001457333564758,
          0.1136053130030632,
          0.18297001719474792,
          0.5079971551895142,
          0.45224729180336,
          0.3095571994781494,
          0.0776325985789299,
          0.10416149348020554,
          0.013313798233866692,
          0.5617296099662781,
          0.01471634954214096,
          0.27381834387779236,
          0.1044587790966034,
          0.30606335401535034,
          0.6116964221000671,
          0.4398111402988434,
          0.823334813117981,
          0.5968908667564392,
          0.22737032175064087,
          0.7024635672569275,
          0.17654016613960266,
          0.9793917536735535,
          0.05871957167983055,
          0.09258708357810974,
          0.9343599081039429,
          0.24577119946479797,
          0.12192021310329437,
          0.08304107189178467,
          0.20356231927871704,
          0.670751690864563,
          0.2841948866844177,
          0.5131968259811401,
          0.20705686509609222,
          0.5887197256088257,
          0.5978443622589111,
          0.4630891978740692,
          0.38076072931289673,
          0.03884914889931679,
          0.4084416329860687,
          0.20796412229537964,
          0.38845381140708923,
          0.3491154909133911,
          0.452444463968277,
          0.655213475227356,
          0.16575445234775543,
          0.2386465072631836,
          0.464155375957489,
          0.6499693989753723,
          0.13624073565006256,
          0.031205371022224426,
          0.14450161159038544,
          0.30759397149086,
          0.2786758840084076,
          0.4732609987258911,
          0.36141470074653625,
          0.3757285475730896,
          0.11810099333524704,
          0.2538663446903229,
          0.08752120286226273,
          0.44752687215805054,
          0.6609399318695068,
          0.930560827255249,
          0.2614147365093231,
          0.6117153763771057,
          0.2606869637966156,
          0.27935492992401123,
          0.38253453373908997,
          0.3149678707122803,
          0.5245079398155212,
          0.15279708802700043,
          0.7156936526298523,
          0.42761313915252686,
          0.12443452328443527,
          0.17606207728385925,
          0.03743334114551544,
          1.104295253753662,
          1.6208432912826538,
          0.02977803163230419,
          0.0061117676086723804,
          1.045817494392395,
          0.6217688322067261,
          0.23413465917110443,
          1.6796618700027466,
          0.029409950599074364,
          0.07413122802972794,
          0.3244123160839081,
          0.9038970470428467,
          0.20072360336780548,
          0.32778576016426086,
          0.3492119014263153,
          0.40704116225242615,
          0.14862780272960663,
          0.13932926952838898,
          0.4293166697025299,
          0.2543468475341797,
          0.8275572657585144,
          0.7597785592079163,
          0.06388053297996521,
          0.14478856325149536,
          0.042611151933670044,
          0.5927469730377197,
          0.33244386315345764,
          0.22942596673965454,
          0.3697468638420105,
          0.42390695214271545,
          0.7926380038261414,
          0.6872695684432983,
          0.14120876789093018,
          0.5578213930130005,
          0.21540196239948273,
          0.02605445869266987,
          0.059424277395009995,
          0.18786941468715668,
          0.5670816898345947,
          0.3067014813423157,
          0.040023479610681534,
          0.5459032654762268,
          0.539583683013916,
          0.4117589592933655,
          0.2996707558631897,
          0.2759881615638733,
          0.6565566062927246,
          0.524398148059845,
          0.47824904322624207,
          0.589929461479187,
          0.2453058958053589,
          0.7120038866996765,
          0.7220720052719116,
          0.25346818566322327,
          0.1616360992193222,
          0.17538347840309143,
          0.15905873477458954,
          0.4829312562942505,
          0.06113182008266449,
          0.34191596508026123,
          0.4662461578845978,
          0.4655851423740387,
          0.4793294072151184,
          0.7321469187736511,
          0.11547274887561798,
          0.32822278141975403,
          0.3986732065677643,
          0.27480557560920715,
          0.7773275971412659,
          0.35955503582954407,
          1.4103466272354126,
          0.2043651044368744,
          0.3757290542125702,
          0.6025367379188538,
          0.3565811514854431,
          0.40551242232322693,
          0.7208551168441772,
          0.45842212438583374,
          0.1376642882823944,
          0.6403982639312744,
          0.4972289502620697,
          0.35496413707733154,
          0.16488049924373627,
          0.3549213111400604,
          0.4409579932689667,
          0.11826147139072418,
          0.17962345480918884,
          0.13033917546272278,
          0.32056406140327454,
          0.5916599035263062,
          0.42801323533058167,
          0.7221524715423584,
          0.37521088123321533,
          0.2405523955821991,
          0.1527332216501236,
          1.380774736404419,
          0.11880053579807281,
          0.690843403339386,
          0.8544251918792725,
          0.9872041344642639,
          0.24384771287441254,
          0.20824730396270752,
          0.13017016649246216,
          0.28262144327163696,
          0.5922132730484009,
          0.7179098725318909,
          0.5117570757865906,
          0.18977496027946472,
          0.0073624528013169765,
          0.5200669169425964,
          0.23831264674663544,
          0.2437005490064621,
          0.4465375542640686,
          0.8804311156272888,
          0.35602304339408875,
          0.3859715759754181,
          0.007237850688397884,
          0.7312253713607788,
          0.04596063122153282,
          0.317468523979187,
          0.22762662172317505,
          0.0690077543258667,
          0.05169200524687767,
          0.3018712103366852,
          1.6318649053573608,
          0.1909744143486023,
          0.24391067028045654,
          0.5620039701461792,
          1.4472672939300537,
          0.914068877696991,
          0.08388333022594452,
          0.34221577644348145,
          0.22454659640789032,
          0.45805713534355164,
          0.47097352147102356,
          0.22936740517616272,
          0.06421039253473282,
          0.3520744740962982,
          0.04083539918065071,
          0.12799786031246185,
          0.3621263802051544,
          0.40762221813201904,
          0.6134260296821594,
          0.32722434401512146,
          0.6106938719749451,
          0.30539074540138245,
          0.05163775384426117,
          0.6457470059394836,
          0.19018331170082092,
          0.25021249055862427,
          0.14516475796699524,
          0.38684943318367004,
          0.03315328434109688,
          0.4714723527431488,
          0.21027377247810364,
          0.2573282718658447,
          0.1134968101978302,
          0.374872088432312,
          0.2484150528907776,
          0.4323045611381531,
          0.21707187592983246,
          0.22521311044692993,
          0.16140131652355194,
          1.1166876554489136,
          0.6471572518348694,
          1.6260733604431152,
          1.2641900777816772,
          0.8128800988197327,
          0.48317867517471313,
          0.45492976903915405,
          0.08663295209407806,
          0.027202537283301353,
          0.13503752648830414,
          0.3741533160209656,
          0.5510734915733337,
          0.7493871450424194,
          0.4630693793296814,
          0.6190312504768372,
          1.0845341682434082,
          0.909569501876831,
          0.38205528259277344,
          0.5860579609870911,
          0.6628111600875854,
          0.6414123773574829,
          0.8118804097175598,
          0.7081888318061829,
          0.5568283796310425,
          0.7168715000152588,
          0.857891321182251,
          0.7450670003890991,
          0.47115767002105713,
          0.6392926573753357,
          0.6557034254074097,
          0.5282669067382812,
          0.1137494370341301,
          0.1687890887260437,
          0.23569902777671814,
          0.6374550461769104,
          0.1826440393924713,
          0.48918652534484863,
          0.3029770851135254,
          0.30190929770469666,
          1.328818678855896,
          1.545813798904419,
          1.7349740266799927,
          1.3669500350952148,
          0.07923885434865952,
          0.7740625739097595,
          0.29081323742866516,
          0.04485232010483742,
          0.5783274173736572,
          0.05787154287099838,
          0.11075878888368607,
          0.45291078090667725,
          0.02028598077595234,
          0.8870818018913269,
          1.1077851057052612,
          0.1274295449256897,
          0.2785072922706604,
          0.7162116169929504,
          0.48482856154441833,
          0.4391862154006958,
          0.2890163064002991,
          0.8310007452964783,
          0.3946877419948578,
          0.193040132522583,
          0.450649619102478,
          0.5324308276176453,
          0.11730320006608963,
          0.6195094585418701,
          0.0448630265891552,
          0.5789480209350586,
          0.8779160976409912,
          1.5724414587020874,
          0.35173577070236206,
          0.2959342300891876,
          0.25149545073509216,
          0.25628766417503357,
          0.6268234848976135,
          0.1248583272099495,
          0.18730106949806213,
          0.5096830725669861,
          0.0946599543094635,
          0.4864981770515442,
          0.37806785106658936,
          0.06659489125013351,
          0.22518660128116608,
          0.2451382428407669,
          0.06753318011760712,
          0.10632632672786713,
          0.5326078534126282,
          0.04132159426808357,
          0.43350672721862793,
          0.21984684467315674,
          0.5132209062576294,
          0.17954953014850616,
          0.2523893415927887,
          0.19867022335529327,
          0.5677005052566528,
          0.48864981532096863,
          0.0815548300743103,
          0.07819683104753494,
          0.572291910648346,
          0.18770544230937958,
          0.08958441019058228,
          0.2659854292869568,
          0.47600725293159485,
          0.6355475783348083,
          0.49857398867607117,
          0.3943989872932434,
          0.011976529844105244,
          0.23637911677360535,
          0.26658928394317627,
          0.495572030544281,
          0.2333751767873764,
          0.6003209352493286,
          0.42200911045074463,
          0.28714853525161743,
          0.22370293736457825,
          0.13548317551612854,
          0.4535071551799774,
          1.2539743185043335,
          0.6284052729606628,
          0.3420051336288452,
          0.8010535836219788,
          0.3177070915699005,
          0.21713539958000183,
          0.5643928647041321,
          0.251394122838974,
          0.5045645833015442,
          0.10270844399929047,
          0.03520546108484268
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"ae85e397-e9a0-4f8d-b4cf-f90906a91329\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ae85e397-e9a0-4f8d-b4cf-f90906a91329\")) {                    Plotly.newPlot(                        \"ae85e397-e9a0-4f8d-b4cf-f90906a91329\",                        [{\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228],\"xaxis\":\"x\",\"y\":[1.0000219345092773,0.9916427135467529,0.9554659724235535,0.9940953850746155,0.8188666105270386,0.8880549073219299,0.059899527579545975,0.47888004779815674,1.1406179666519165,0.4542464315891266,0.13006041944026947,0.021122673526406288,0.5459228157997131,0.26975563168525696,0.7205497622489929,0.15734981000423431,0.7076377272605896,0.8750724792480469,0.7273396253585815,0.7269487380981445,0.027035174891352654,0.48439520597457886,0.6977373361587524,0.510209858417511,0.6781986355781555,0.6842751502990723,0.5965504050254822,0.7072188258171082,0.36327695846557617,0.48502400517463684,0.3802867829799652,0.3916538655757904,2.0036041736602783,0.3141103982925415,0.4533005356788635,0.5320723652839661,0.5077403783798218,0.5000072121620178,0.45167067646980286,0.5049024224281311,0.4307093918323517,0.5822591781616211,0.9145010709762573,0.7941117882728577,0.7606931328773499,0.8262620568275452,0.5918679237365723,0.7858644127845764,0.6583988070487976,0.004014811012893915,0.3761083781719208,0.6523128151893616,0.6742688417434692,0.4396340847015381,1.0557832717895508,0.1446019560098648,0.5969732403755188,0.2989127039909363,2.088658332824707,0.46468567848205566,0.9975314140319824,0.42344847321510315,0.31239065527915955,0.5669916272163391,0.9280962347984314,0.8454020023345947,0.829001784324646,0.7248730063438416,0.8597204089164734,0.6808794736862183,0.6863744854927063,0.9062994122505188,0.47552651166915894,0.9231739044189453,0.37750035524368286,0.4922006130218506,0.31804630160331726,0.40215015411376953,0.2773198187351227,0.9679120779037476,0.15567545592784882,0.7058451771736145,0.5772280693054199,0.16124826669692993,0.5516402125358582,0.24942266941070557,0.5455959439277649,0.3219684064388275,0.5986685752868652,0.4669388234615326,0.1430736929178238,0.8356509804725647,0.6105442047119141,0.8819795846939087,0.1839931756258011,0.5295064449310303,0.373496413230896,0.6239397525787354,0.7380717992782593,0.4665204882621765,0.8466097116470337,0.28341004252433777,0.6320981979370117,0.5047815442085266,0.5932438969612122,0.5755007266998291,0.20444422960281372,0.07075760513544083,0.650425136089325,0.6507689356803894,1.2819998264312744,0.8964548707008362,0.7982120513916016,0.24874410033226013,0.5461005568504333,0.4527615010738373,0.9139885902404785,0.006974905263632536,0.6763337254524231,0.134950652718544,0.770599365234375,0.3554123640060425,0.5365436673164368,0.04665378853678703,0.2652641534805298,0.05611324682831764,0.46167346835136414,1.4334410429000854,0.07350844144821167,0.1770654171705246,0.5861415863037109,0.12669053673744202,0.5642860531806946,0.9128984808921814,0.29430443048477173,0.19591878354549408,0.8255599141120911,0.6755244135856628,1.0072815418243408,0.1911226063966751,0.3013351261615753,0.7123463749885559,0.22219350934028625,0.6033529043197632,0.8990907669067383,1.003036379814148,0.2308398187160492,1.0106933116912842,0.7802683115005493,1.002387285232544,0.7417197227478027,0.9009478688240051,0.6946225166320801,0.6929285526275635,0.7793466448783875,0.6506795287132263,0.7337077856063843,0.9283533692359924,0.596578061580658,0.8090308308601379,0.23918776214122772,0.2826906144618988,0.08142908662557602,0.5027477741241455,0.23474012315273285,0.47153979539871216,1.5490531921386719,0.4011160433292389,0.5238226652145386,0.5036541223526001,0.1385871022939682,0.7390760183334351,0.5105196833610535,0.14984281361103058,0.6326988935470581,0.5122566819190979,0.33519405126571655,0.9347264766693115,0.39853721857070923,0.52920001745224,0.09929218888282776,1.512749195098877,0.15660791099071503,0.12982292473316193,0.23980878293514252,1.833327054977417,0.6238862872123718,0.07858513295650482,0.3325956463813782,0.053081925958395004,0.4679076075553894,0.3676823079586029,0.5448495745658875,0.30790001153945923,0.24222347140312195,0.463723361492157,0.09902229905128479,0.24670065939426422,0.5160186886787415,0.7502350807189941,0.3617759943008423,0.5468770861625671,1.4564948081970215,0.7610254287719727,1.6127755641937256,0.0371081717312336,0.045674774795770645,0.3611479103565216,0.698615550994873,0.7046037316322327,0.5571401715278625,0.24180421233177185,0.05922180041670799,0.32310354709625244,0.5285301208496094,0.18420454859733582,1.2035564184188843,0.2774227559566498,0.492672860622406,1.1041589975357056,0.5685820579528809,1.5559906959533691,0.11264383792877197,0.14540386199951172,0.8092074394226074,0.3343474566936493,0.701816737651825,0.009415448643267155,0.694787859916687,0.2309853881597519,0.7132643461227417,0.21831904351711273,0.6243077516555786,0.2617412805557251,1.0022971630096436,0.4346802830696106,3.635181188583374,0.36922261118888855,0.1300138682126999,0.5001373887062073,1.1523194313049316,0.2234174758195877,0.06732665747404099,0.026408854871988297,0.7292199730873108,0.37656936049461365,0.6359787583351135,0.7878544330596924,0.6603260636329651,0.5197328329086304,0.46609193086624146,0.14157526195049286,0.05648405849933624,0.2526721656322479,0.43014201521873474,0.6772339940071106,0.011759740300476551,1.5490427017211914,0.050243984907865524,0.4988640546798706,0.08206123858690262,1.3146780729293823,2.542037010192871,0.42762938141822815,0.10151544958353043,0.4119194447994232,0.5689876675605774,0.37583860754966736,0.23868349194526672,0.10040357708930969,0.7560923099517822,0.5682185888290405,0.19108624756336212,0.27028128504753113,0.6307090520858765,0.6825029253959656,0.422006219625473,0.22070220112800598,0.18588142096996307,0.6906334757804871,1.1243683099746704,0.1095002219080925,0.1579267680644989,1.3975720405578613,0.548335075378418,1.0144495964050293,0.232938751578331,0.5762152075767517,0.5279026627540588,0.41432541608810425,0.4205167293548584,0.6988275647163391,0.5806070566177368,0.5948336720466614,0.6679217219352722,0.015527959913015366,0.6453338265419006,0.4483601450920105,0.8405125141143799,0.28258809447288513,1.8066707849502563,0.034801725298166275,0.8431036472320557,0.071611687541008,0.6325744986534119,0.7582565546035767,1.0065226554870605,0.5050201416015625,0.7719635963439941,0.47065573930740356,0.48972445726394653,0.6991575360298157,0.11498157680034637,0.4729478359222412,0.3307758569717407,0.6442753672599792,0.6896385550498962,0.2293458878993988,0.023381799459457397,0.023001940920948982,0.20744439959526062,0.26312634348869324,1.155521273612976,0.43839165568351746,0.18801215291023254,0.4216279685497284,0.2913588881492615,0.22808800637722015,0.5374566316604614,0.5435143709182739,0.5669344067573547,0.43323397636413574,0.56034916639328,0.2527322471141815,0.30298811197280884,0.582143247127533,1.0825098752975464,0.06656340509653091,0.6926432847976685,0.21034839749336243,0.05436289310455322,0.20556475222110748,0.574617326259613,0.1938355565071106,0.039662688970565796,0.5269081592559814,0.2587994337081909,0.6974515318870544,0.7354109883308411,0.10608284175395966,0.2893063426017761,0.1180209144949913,0.03563639149069786,0.6356740593910217,0.5969575047492981,0.09100300818681717,0.3153785467147827,0.4895288050174713,0.3974262475967407,0.3508790135383606,0.12322203814983368,0.3122040331363678,0.2951399087905884,0.06605073064565659,0.18600401282310486,0.3517577350139618,0.10390922427177429,0.17060911655426025,0.2952680289745331,0.24062807857990265,0.2391059696674347,0.3347742557525635,0.24040061235427856,0.5441713929176331,0.5224291682243347,0.35009339451789856,1.1697635650634766,1.0264179706573486,0.1638258695602417,0.1940479278564453,0.016999376937747,0.5696246027946472,0.052214205265045166,0.4234505593776703,0.40108680725097656,0.7553025484085083,0.2056993991136551,0.1336010992527008,0.5683211088180542,0.04644707217812538,0.057570986449718475,0.4053744375705719,0.20271024107933044,0.12156117707490921,0.901378333568573,0.42589500546455383,0.10383142530918121,0.7278721928596497,0.4337705373764038,0.12040610611438751,0.7312960624694824,0.5983625650405884,0.7429268956184387,0.33542823791503906,0.775023341178894,0.850867509841919,0.21696244180202484,0.43096497654914856,0.07891532778739929,0.4755036234855652,0.24108009040355682,0.2633480131626129,0.4462125599384308,0.3131409287452698,0.6224732995033264,0.4250383973121643,0.5387638211250305,0.5376846194267273,0.48164552450180054,0.08839408308267593,0.5211972594261169,0.4927276074886322,0.5409421920776367,1.1281479597091675,0.5740256905555725,0.33417966961860657,0.03658820316195488,0.5692785382270813,0.5506106019020081,0.49599340558052063,0.2571006119251251,0.29737284779548645,0.09907972067594528,0.23559266328811646,0.6068393588066101,0.4961126148700714,0.7734960317611694,0.26417264342308044,0.8164578676223755,0.1987265646457672,0.5057837963104248,0.5479919910430908,0.23791131377220154,0.3777255713939667,0.1715584397315979,0.36216026544570923,0.43406492471694946,0.8099574446678162,0.4649227559566498,0.3990924656391144,0.5586443543434143,0.09383446723222733,0.49252259731292725,4.316255581215955e-05,0.849764883518219,0.06683520972728729,0.6185448169708252,0.4145351052284241,0.19559766352176666,0.5556361079216003,0.12330539524555206,0.6699219346046448,0.06920292973518372,0.10923144221305847,0.5766394734382629,0.5444393754005432,0.026969505473971367,0.40154802799224854,0.3932839035987854,0.3792873024940491,0.4478949010372162,0.08822603523731232,0.3529217839241028,0.5420203804969788,0.4861050546169281,0.3148491680622101,0.49624761939048767,0.19634559750556946,0.0556536428630352,0.4078700840473175,0.917293131351471,1.028435468673706,0.423490047454834,0.35800230503082275,0.7248629331588745,0.7296602725982666,0.25260230898857117,0.6210406422615051,0.11301366984844208,0.48655301332473755,0.1387600600719452,0.5197167992591858,0.20474779605865479,0.2786167562007904,1.2084455490112305,0.18288852274417877,0.6154617071151733,0.5543721914291382,0.39885213971138,0.7099440693855286,0.08206509053707123,0.3601042628288269,0.016056692227721214,1.2467900514602661,0.17475377023220062,0.38762199878692627,0.1026453822851181,0.05343932658433914,0.029401252046227455,0.5841116309165955,0.31691819429397583,0.17375756800174713,0.0223276037722826,0.12328419089317322,0.42896604537963867,0.400646835565567,0.7916175723075867,0.33107587695121765,0.14303378760814667,0.424655020236969,0.28347185254096985,0.3927080035209656,0.5848546624183655,1.144917368888855,0.210361510515213,0.3667171597480774,0.05491158738732338,1.0241502523422241,1.0433892011642456,0.1097196415066719,0.3865366280078888,0.08906909078359604,0.27159976959228516,0.7397797107696533,0.6091729998588562,0.7995331883430481,0.689402163028717,0.3611818552017212,0.8242165446281433,0.5902225971221924,0.8093901872634888,0.33450302481651306,0.519474983215332,0.29539087414741516,0.8631201386451721,0.13604559004306793,0.11444409936666489,0.62455815076828,0.24294854700565338,0.2326466292142868,0.14193977415561676,0.7986838221549988,0.45855972170829773,0.8499776721000671,0.8978604674339294,0.24234509468078613,0.4783366620540619,0.6265402436256409,0.5500478148460388,0.41756772994995117,0.70947265625,0.21735621988773346,0.678226888179779,0.5825237035751343,0.23612722754478455,0.6118757724761963,0.48104533553123474,0.6296490430831909,0.610106348991394,0.37938418984413147,0.24006935954093933,0.28679367899894714,0.9574082493782043,1.5696988105773926,0.10243198275566101,0.19461369514465332,1.6122196912765503,0.4465421438217163,0.8570220470428467,0.2200908660888672,0.1527247577905655,0.7751067280769348,0.4664962589740753,0.7267002463340759,0.44121912121772766,0.07799617946147919,0.3118493854999542,0.32936471700668335,0.47947365045547485,0.10121418535709381,0.841121256351471,0.4176073968410492,0.21980629861354828,0.5202930569648743,0.4844529628753662,0.53715980052948,0.13630394637584686,0.38632288575172424,0.7562395334243774,0.2538212537765503,0.3387695550918579,0.6706540584564209,0.006012734957039356,0.7383051514625549,0.4226336181163788,0.28664442896842957,0.028911789879202843,0.5721453428268433,0.26405414938926697,0.32215002179145813,0.3239716589450836,0.33542367815971375,0.7512875199317932,0.7527630925178528,0.20755861699581146,0.3867761790752411,0.724882960319519,0.4059297740459442,1.2309387922286987,0.183614119887352,0.05889398977160454,1.2851455211639404,0.2896416187286377,0.053126245737075806,0.15479795634746552,0.7482832074165344,0.43567338585853577,0.5221043825149536,0.0378921739757061,0.29911473393440247,0.45233237743377686,0.4900592267513275,0.5050675272941589,0.015065507963299751,0.5951219797134399,0.042929552495479584,0.33655408024787903,0.1320064216852188,0.551027774810791,0.09877395629882812,0.20034894347190857,1.568985104560852,0.8552345037460327,0.7852180600166321,0.5834358334541321,0.3962199091911316,0.2396622598171234,0.20734289288520813,0.7395385503768921,0.17518794536590576,0.47232186794281006,0.3719421327114105,0.4847598671913147,0.6897347569465637,0.4113343358039856,0.6120129227638245,0.45460808277130127,0.674697995185852,0.5213989019393921,0.6440584063529968,0.16703569889068604,0.5962903499603271,0.296828955411911,0.02545442059636116,0.05011551082134247,0.09749803692102432,1.052610993385315,0.4616559147834778,0.7385056614875793,0.4376997649669647,0.5348909497261047,0.43151864409446716,0.27111896872520447,0.19408628344535828,0.4943869709968567,0.47915929555892944,0.6318379044532776,0.012118272483348846,0.39387786388397217,0.5593633651733398,0.657996416091919,0.6134451031684875,0.821639358997345,0.8776984214782715,0.3530382513999939,0.49512988328933716,0.0455770380795002,0.7692623734474182,0.16901811957359314,0.5251489877700806,0.17554517090320587,0.4034230411052704,0.22924675047397614,1.002846360206604,0.36457106471061707,0.08364671468734741,0.10183526575565338,0.3202992379665375,0.397190123796463,0.07650948315858841,0.6120181679725647,0.14284023642539978,0.06066780164837837,0.8008401393890381,0.51491379737854,0.437915176153183,0.25177618861198425,0.14471594989299774,0.39494654536247253,0.011829545721411705,0.5219266414642334,0.0469224788248539,0.4564143419265747,0.07170476019382477,0.38274309039115906,0.026294860988855362,0.8232476711273193,0.4266434907913208,0.28435420989990234,0.3298286497592926,0.08921439945697784,0.012579838745296001,0.35969823598861694,0.05897920951247215,0.48325204849243164,0.4104417562484741,0.15977953374385834,0.09513698518276215,0.20989850163459778,1.6344348192214966,0.7069069743156433,0.19893459975719452,0.07212524116039276,0.6301794052124023,0.8622266054153442,0.5386908650398254,0.7117677330970764,0.017621420323848724,0.7746241092681885,1.0166553258895874,0.705912709236145,0.7688079476356506,0.46732160449028015,0.7484858632087708,0.718819260597229,0.7692344784736633,0.22133834660053253,0.786242663860321,0.26532191038131714,0.5803425908088684,0.6017047166824341,0.41382429003715515,0.016387974843382835,0.5110742449760437,0.3574708104133606,0.46021848917007446,0.2135028839111328,0.5120267271995544,0.04991573095321655,0.044977616518735886,0.005364150274544954,0.589689314365387,0.5655614733695984,1.1142791509628296,0.6754966974258423,1.0488277673721313,0.20944714546203613,0.5865235328674316,0.13855503499507904,0.3776605725288391,0.32676249742507935,0.13374526798725128,0.38271212577819824,0.20654527842998505,0.10839265584945679,0.787348747253418,0.581360399723053,0.27112898230552673,0.5655203461647034,0.11973153799772263,0.42329883575439453,0.06906278431415558,0.613762378692627,0.48454198241233826,0.681635856628418,0.5527499318122864,0.2066344916820526,0.022185862064361572,0.09581825137138367,0.042628951370716095,0.24681489169597626,0.0857110470533371,0.024395860731601715,0.30619126558303833,1.4058016538619995,0.9697467684745789,0.29708778858184814,0.4308297634124756,0.14108212292194366,0.36289137601852417,0.09685191512107849,0.972176730632782,0.8839508891105652,1.0003700256347656,0.8380652070045471,0.8632335662841797,0.8607273697853088,0.20255230367183685,0.6380136609077454,0.7262158393859863,0.9310997128486633,0.7317909598350525,0.10778923332691193,0.6098304390907288,0.5587123036384583,0.7712652683258057,0.1682531088590622,0.6976838707923889,0.5248631834983826,0.36644986271858215,0.6826626062393188,0.9542439579963684,0.3988240957260132,0.5127003192901611,0.46580970287323,0.30355414748191833,0.0988413468003273,0.3499656617641449,0.008251410908997059,0.5291657447814941,1.1611353158950806,0.7684808373451233,0.8459818363189697,2.2525787353515625,2.086617946624756,1.3241811990737915,1.1324083805084229,0.5705864429473877,0.4634593725204468,0.5630573034286499,0.02530520409345627,0.8782223463058472,0.7963356971740723,0.19294771552085876,0.7069859504699707,0.5877599716186523,0.49842825531959534,0.5798456072807312,0.9098816514015198,0.502949059009552,0.16475071012973785,0.4205971658229828,0.467642217874527,0.09224983304738998,0.37210896611213684,0.7368285059928894,0.5713918209075928,0.3720022141933441,0.1901889145374298,0.7296484112739563,0.9450056552886963,0.0513961985707283,0.5001457333564758,0.1136053130030632,0.18297001719474792,0.5079971551895142,0.45224729180336,0.3095571994781494,0.0776325985789299,0.10416149348020554,0.013313798233866692,0.5617296099662781,0.01471634954214096,0.27381834387779236,0.1044587790966034,0.30606335401535034,0.6116964221000671,0.4398111402988434,0.823334813117981,0.5968908667564392,0.22737032175064087,0.7024635672569275,0.17654016613960266,0.9793917536735535,0.05871957167983055,0.09258708357810974,0.9343599081039429,0.24577119946479797,0.12192021310329437,0.08304107189178467,0.20356231927871704,0.670751690864563,0.2841948866844177,0.5131968259811401,0.20705686509609222,0.5887197256088257,0.5978443622589111,0.4630891978740692,0.38076072931289673,0.03884914889931679,0.4084416329860687,0.20796412229537964,0.38845381140708923,0.3491154909133911,0.452444463968277,0.655213475227356,0.16575445234775543,0.2386465072631836,0.464155375957489,0.6499693989753723,0.13624073565006256,0.031205371022224426,0.14450161159038544,0.30759397149086,0.2786758840084076,0.4732609987258911,0.36141470074653625,0.3757285475730896,0.11810099333524704,0.2538663446903229,0.08752120286226273,0.44752687215805054,0.6609399318695068,0.930560827255249,0.2614147365093231,0.6117153763771057,0.2606869637966156,0.27935492992401123,0.38253453373908997,0.3149678707122803,0.5245079398155212,0.15279708802700043,0.7156936526298523,0.42761313915252686,0.12443452328443527,0.17606207728385925,0.03743334114551544,1.104295253753662,1.6208432912826538,0.02977803163230419,0.0061117676086723804,1.045817494392395,0.6217688322067261,0.23413465917110443,1.6796618700027466,0.029409950599074364,0.07413122802972794,0.3244123160839081,0.9038970470428467,0.20072360336780548,0.32778576016426086,0.3492119014263153,0.40704116225242615,0.14862780272960663,0.13932926952838898,0.4293166697025299,0.2543468475341797,0.8275572657585144,0.7597785592079163,0.06388053297996521,0.14478856325149536,0.042611151933670044,0.5927469730377197,0.33244386315345764,0.22942596673965454,0.3697468638420105,0.42390695214271545,0.7926380038261414,0.6872695684432983,0.14120876789093018,0.5578213930130005,0.21540196239948273,0.02605445869266987,0.059424277395009995,0.18786941468715668,0.5670816898345947,0.3067014813423157,0.040023479610681534,0.5459032654762268,0.539583683013916,0.4117589592933655,0.2996707558631897,0.2759881615638733,0.6565566062927246,0.524398148059845,0.47824904322624207,0.589929461479187,0.2453058958053589,0.7120038866996765,0.7220720052719116,0.25346818566322327,0.1616360992193222,0.17538347840309143,0.15905873477458954,0.4829312562942505,0.06113182008266449,0.34191596508026123,0.4662461578845978,0.4655851423740387,0.4793294072151184,0.7321469187736511,0.11547274887561798,0.32822278141975403,0.3986732065677643,0.27480557560920715,0.7773275971412659,0.35955503582954407,1.4103466272354126,0.2043651044368744,0.3757290542125702,0.6025367379188538,0.3565811514854431,0.40551242232322693,0.7208551168441772,0.45842212438583374,0.1376642882823944,0.6403982639312744,0.4972289502620697,0.35496413707733154,0.16488049924373627,0.3549213111400604,0.4409579932689667,0.11826147139072418,0.17962345480918884,0.13033917546272278,0.32056406140327454,0.5916599035263062,0.42801323533058167,0.7221524715423584,0.37521088123321533,0.2405523955821991,0.1527332216501236,1.380774736404419,0.11880053579807281,0.690843403339386,0.8544251918792725,0.9872041344642639,0.24384771287441254,0.20824730396270752,0.13017016649246216,0.28262144327163696,0.5922132730484009,0.7179098725318909,0.5117570757865906,0.18977496027946472,0.0073624528013169765,0.5200669169425964,0.23831264674663544,0.2437005490064621,0.4465375542640686,0.8804311156272888,0.35602304339408875,0.3859715759754181,0.007237850688397884,0.7312253713607788,0.04596063122153282,0.317468523979187,0.22762662172317505,0.0690077543258667,0.05169200524687767,0.3018712103366852,1.6318649053573608,0.1909744143486023,0.24391067028045654,0.5620039701461792,1.4472672939300537,0.914068877696991,0.08388333022594452,0.34221577644348145,0.22454659640789032,0.45805713534355164,0.47097352147102356,0.22936740517616272,0.06421039253473282,0.3520744740962982,0.04083539918065071,0.12799786031246185,0.3621263802051544,0.40762221813201904,0.6134260296821594,0.32722434401512146,0.6106938719749451,0.30539074540138245,0.05163775384426117,0.6457470059394836,0.19018331170082092,0.25021249055862427,0.14516475796699524,0.38684943318367004,0.03315328434109688,0.4714723527431488,0.21027377247810364,0.2573282718658447,0.1134968101978302,0.374872088432312,0.2484150528907776,0.4323045611381531,0.21707187592983246,0.22521311044692993,0.16140131652355194,1.1166876554489136,0.6471572518348694,1.6260733604431152,1.2641900777816772,0.8128800988197327,0.48317867517471313,0.45492976903915405,0.08663295209407806,0.027202537283301353,0.13503752648830414,0.3741533160209656,0.5510734915733337,0.7493871450424194,0.4630693793296814,0.6190312504768372,1.0845341682434082,0.909569501876831,0.38205528259277344,0.5860579609870911,0.6628111600875854,0.6414123773574829,0.8118804097175598,0.7081888318061829,0.5568283796310425,0.7168715000152588,0.857891321182251,0.7450670003890991,0.47115767002105713,0.6392926573753357,0.6557034254074097,0.5282669067382812,0.1137494370341301,0.1687890887260437,0.23569902777671814,0.6374550461769104,0.1826440393924713,0.48918652534484863,0.3029770851135254,0.30190929770469666,1.328818678855896,1.545813798904419,1.7349740266799927,1.3669500350952148,0.07923885434865952,0.7740625739097595,0.29081323742866516,0.04485232010483742,0.5783274173736572,0.05787154287099838,0.11075878888368607,0.45291078090667725,0.02028598077595234,0.8870818018913269,1.1077851057052612,0.1274295449256897,0.2785072922706604,0.7162116169929504,0.48482856154441833,0.4391862154006958,0.2890163064002991,0.8310007452964783,0.3946877419948578,0.193040132522583,0.450649619102478,0.5324308276176453,0.11730320006608963,0.6195094585418701,0.0448630265891552,0.5789480209350586,0.8779160976409912,1.5724414587020874,0.35173577070236206,0.2959342300891876,0.25149545073509216,0.25628766417503357,0.6268234848976135,0.1248583272099495,0.18730106949806213,0.5096830725669861,0.0946599543094635,0.4864981770515442,0.37806785106658936,0.06659489125013351,0.22518660128116608,0.2451382428407669,0.06753318011760712,0.10632632672786713,0.5326078534126282,0.04132159426808357,0.43350672721862793,0.21984684467315674,0.5132209062576294,0.17954953014850616,0.2523893415927887,0.19867022335529327,0.5677005052566528,0.48864981532096863,0.0815548300743103,0.07819683104753494,0.572291910648346,0.18770544230937958,0.08958441019058228,0.2659854292869568,0.47600725293159485,0.6355475783348083,0.49857398867607117,0.3943989872932434,0.011976529844105244,0.23637911677360535,0.26658928394317627,0.495572030544281,0.2333751767873764,0.6003209352493286,0.42200911045074463,0.28714853525161743,0.22370293736457825,0.13548317551612854,0.4535071551799774,1.2539743185043335,0.6284052729606628,0.3420051336288452,0.8010535836219788,0.3177070915699005,0.21713539958000183,0.5643928647041321,0.251394122838974,0.5045645833015442,0.10270844399929047,0.03520546108484268],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ae85e397-e9a0-4f8d-b4cf-f90906a91329');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.line(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=0<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "0",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "0",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305
         ],
         "xaxis": "x",
         "y": [
          0.20699937641620636,
          0.32685762643814087,
          0.14952141046524048,
          0.21353109180927277,
          0.47509124875068665,
          0.08742078393697739,
          0.2197965532541275,
          0.3061297833919525,
          0.34596967697143555,
          0.35758844017982483,
          0.6835384964942932,
          0.24268919229507446,
          0.2685963809490204,
          0.16794465482234955,
          0.7591005563735962,
          0.522117018699646,
          0.552704930305481,
          0.34007400274276733,
          0.16095717251300812,
          0.4773082733154297,
          0.13267603516578674,
          0.34805890917778015,
          0.38905948400497437,
          0.34804320335388184,
          0.4133758842945099,
          0.29929661750793457,
          0.6575424671173096,
          0.17331244051456451,
          0.11915662884712219,
          0.03102431818842888,
          0.0876370221376419,
          0.5319914817810059,
          0.2172144204378128,
          0.3241320252418518,
          0.1431298702955246,
          0.0189162977039814,
          0.3301740288734436,
          0.3035659193992615,
          0.48570716381073,
          0.07455259561538696,
          0.0988682210445404,
          0.4847448468208313,
          0.3422791063785553,
          0.33489078283309937,
          0.5828297734260559,
          0.55079185962677,
          0.1719549149274826,
          0.02277202345430851,
          0.1218276172876358,
          0.2533385157585144,
          0.3386172652244568,
          0.007280247285962105,
          0.3127058446407318,
          0.7868233919143677,
          0.1218203753232956,
          0.14399953186511993,
          0.5392637252807617,
          0.5799062848091125,
          0.06358642876148224,
          0.12578685581684113,
          0.11405455321073532,
          0.5114828944206238,
          0.32273733615875244,
          0.7377265691757202,
          0.08957284688949585,
          0.030806271359324455,
          0.495784729719162,
          0.426779568195343,
          0.5871372818946838,
          0.2672564387321472,
          0.2687791585922241,
          0.7393223643302917,
          0.062000833451747894,
          0.12585823237895966,
          0.05548801273107529,
          0.2405954748392105,
          0.5549560785293579,
          0.5729208588600159,
          0.36685264110565186,
          0.15567044913768768,
          0.38812199234962463,
          0.28527188301086426,
          0.010293344035744667,
          0.6763702630996704,
          0.059773124754428864,
          0.2576560378074646,
          0.24729523062705994,
          0.033733464777469635,
          0.35870787501335144,
          0.3583504259586334,
          0.2757701873779297,
          0.46752357482910156,
          0.09805615246295929,
          0.06600641459226608,
          0.15868428349494934,
          0.6960283517837524,
          0.46912479400634766,
          0.31220558285713196,
          0.04136213660240173,
          0.2482486218214035,
          0.2977312207221985,
          0.5065533518791199,
          0.2921021580696106,
          0.14001280069351196,
          0.057918328791856766,
          0.24875588715076447,
          0.33243638277053833,
          0.3245343863964081,
          0.20333077013492584,
          0.33683374524116516,
          0.11134940385818481,
          0.22676284611225128,
          0.1685870885848999,
          0.04641500115394592,
          0.20113053917884827,
          0.1169828549027443,
          0.4923405349254608,
          0.41138219833374023,
          0.06376950442790985,
          0.16734454035758972,
          0.0537353977560997,
          0.6351748108863831,
          0.006440855097025633,
          0.10187571495771408,
          0.13709920644760132,
          0.0435757078230381,
          0.22769704461097717,
          0.20214812457561493,
          0.9449364542961121,
          0.3167784512042999,
          0.2009054273366928,
          0.4792337715625763,
          0.183785542845726,
          0.0637158676981926,
          0.1536666303873062,
          0.047206729650497437,
          0.5465540885925293,
          0.19267785549163818,
          0.6826439499855042,
          0.17643117904663086,
          0.018109293654561043,
          0.7731384038925171,
          0.18293778598308563,
          0.6404657959938049,
          0.34703654050827026,
          0.49353182315826416,
          0.33675602078437805,
          0.24079371988773346,
          0.196040078997612,
          0.6136146783828735,
          0.48412856459617615,
          0.22871924936771393,
          0.5361219048500061,
          0.4930141866207123,
          0.37190955877304077,
          0.29840871691703796,
          0.29482898116111755,
          0.004866654984652996,
          0.5525434017181396,
          0.6169857978820801,
          0.10507003217935562,
          0.5194745063781738,
          0.12965986132621765,
          0.24084851145744324,
          0.29758813977241516,
          0.5145015120506287,
          0.2484140843153,
          0.54340660572052,
          0.5374822020530701,
          0.35950592160224915,
          0.04780098795890808,
          0.060577720403671265,
          0.46254292130470276,
          0.1441827416419983,
          0.5537449717521667,
          0.1365731805562973,
          0.36239391565322876,
          0.06252800673246384,
          0.3047383725643158,
          0.2346634566783905,
          0.04877515137195587,
          0.6433662176132202,
          0.3468550741672516,
          0.1657436639070511,
          0.14928016066551208,
          0.3179430067539215,
          0.1536044329404831,
          0.0069121746346354485,
          0.15356458723545074,
          0.6232147812843323,
          0.013239343650639057,
          0.6175666451454163,
          0.3617836534976959,
          0.0873880609869957,
          0.010455435141921043,
          0.39981818199157715,
          0.3760959208011627,
          0.04688216373324394,
          0.037993915379047394,
          0.04514235258102417,
          0.4837218225002289,
          0.6192233562469482,
          0.2189701944589615,
          0.5193264484405518,
          0.02137170173227787,
          0.226396381855011,
          0.35058942437171936,
          0.5820711851119995,
          0.25661203265190125,
          0.6219279170036316,
          0.6753234267234802,
          0.24406088888645172,
          0.4934995770454407,
          0.21210765838623047,
          0.4339323937892914,
          0.3690739870071411,
          0.04868162050843239,
          0.5199374556541443,
          0.4267684817314148,
          0.3791408836841583,
          0.6383141279220581,
          0.07656025886535645,
          0.18022455275058746,
          0.47163087129592896,
          0.41492122411727905,
          0.44769150018692017,
          0.19101805984973907,
          0.6924516558647156,
          0.0870259627699852,
          0.48807623982429504,
          0.6127728223800659,
          0.40766870975494385,
          0.3618010878562927,
          0.09934841096401215,
          0.06157099828124046,
          0.1120314970612526,
          0.3108523190021515,
          0.11104453355073929,
          0.09755469113588333,
          0.298003613948822,
          0.15451666712760925,
          0.19789761304855347,
          0.15547685325145721,
          0.21551185846328735,
          0.19310812652111053,
          0.27152276039123535,
          0.15905964374542236,
          0.06688352674245834,
          0.25238144397735596,
          0.5572103261947632,
          0.43874651193618774,
          0.015816230326890945,
          0.017032790929079056,
          0.27092787623405457,
          0.6359698176383972,
          0.08252710849046707,
          0.36214518547058105,
          0.2656831443309784,
          0.408827006816864,
          0.35868752002716064,
          0.0254361554980278,
          0.022287942469120026,
          0.3498331308364868,
          0.11496369540691376,
          0.5196647644042969,
          0.21395640075206757,
          0.1090349331498146,
          0.04456035792827606,
          0.11554592847824097,
          0.025734053924679756,
          0.4237522482872009,
          0.5155255794525146,
          0.4981873333454132,
          0.127912238240242,
          0.0015443760203197598,
          0.24136300384998322,
          0.5152112245559692,
          0.4099481403827667,
          0.539298951625824,
          0.5537616014480591,
          0.5428193807601929,
          0.5707558393478394,
          0.40700507164001465,
          0.18089310824871063,
          0.18051479756832123,
          0.613513708114624,
          0.5254374146461487,
          0.225594624876976,
          0.26697126030921936,
          0.8235123157501221,
          0.07330828160047531,
          0.12588201463222504,
          0.5206363797187805,
          0.14721816778182983,
          0.3631344437599182,
          0.15295645594596863,
          0.7398999929428101,
          0.5939680933952332,
          0.3115261197090149,
          0.30631008744239807,
          0.04334830492734909,
          0.36892107129096985,
          0.4536152482032776,
          0.4243941903114319,
          0.2270999252796173,
          0.2139878273010254
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"7c1a9f19-74a0-4a2b-be0c-8cf4d3714dc0\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7c1a9f19-74a0-4a2b-be0c-8cf4d3714dc0\")) {                    Plotly.newPlot(                        \"7c1a9f19-74a0-4a2b-be0c-8cf4d3714dc0\",                        [{\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305],\"xaxis\":\"x\",\"y\":[0.20699937641620636,0.32685762643814087,0.14952141046524048,0.21353109180927277,0.47509124875068665,0.08742078393697739,0.2197965532541275,0.3061297833919525,0.34596967697143555,0.35758844017982483,0.6835384964942932,0.24268919229507446,0.2685963809490204,0.16794465482234955,0.7591005563735962,0.522117018699646,0.552704930305481,0.34007400274276733,0.16095717251300812,0.4773082733154297,0.13267603516578674,0.34805890917778015,0.38905948400497437,0.34804320335388184,0.4133758842945099,0.29929661750793457,0.6575424671173096,0.17331244051456451,0.11915662884712219,0.03102431818842888,0.0876370221376419,0.5319914817810059,0.2172144204378128,0.3241320252418518,0.1431298702955246,0.0189162977039814,0.3301740288734436,0.3035659193992615,0.48570716381073,0.07455259561538696,0.0988682210445404,0.4847448468208313,0.3422791063785553,0.33489078283309937,0.5828297734260559,0.55079185962677,0.1719549149274826,0.02277202345430851,0.1218276172876358,0.2533385157585144,0.3386172652244568,0.007280247285962105,0.3127058446407318,0.7868233919143677,0.1218203753232956,0.14399953186511993,0.5392637252807617,0.5799062848091125,0.06358642876148224,0.12578685581684113,0.11405455321073532,0.5114828944206238,0.32273733615875244,0.7377265691757202,0.08957284688949585,0.030806271359324455,0.495784729719162,0.426779568195343,0.5871372818946838,0.2672564387321472,0.2687791585922241,0.7393223643302917,0.062000833451747894,0.12585823237895966,0.05548801273107529,0.2405954748392105,0.5549560785293579,0.5729208588600159,0.36685264110565186,0.15567044913768768,0.38812199234962463,0.28527188301086426,0.010293344035744667,0.6763702630996704,0.059773124754428864,0.2576560378074646,0.24729523062705994,0.033733464777469635,0.35870787501335144,0.3583504259586334,0.2757701873779297,0.46752357482910156,0.09805615246295929,0.06600641459226608,0.15868428349494934,0.6960283517837524,0.46912479400634766,0.31220558285713196,0.04136213660240173,0.2482486218214035,0.2977312207221985,0.5065533518791199,0.2921021580696106,0.14001280069351196,0.057918328791856766,0.24875588715076447,0.33243638277053833,0.3245343863964081,0.20333077013492584,0.33683374524116516,0.11134940385818481,0.22676284611225128,0.1685870885848999,0.04641500115394592,0.20113053917884827,0.1169828549027443,0.4923405349254608,0.41138219833374023,0.06376950442790985,0.16734454035758972,0.0537353977560997,0.6351748108863831,0.006440855097025633,0.10187571495771408,0.13709920644760132,0.0435757078230381,0.22769704461097717,0.20214812457561493,0.9449364542961121,0.3167784512042999,0.2009054273366928,0.4792337715625763,0.183785542845726,0.0637158676981926,0.1536666303873062,0.047206729650497437,0.5465540885925293,0.19267785549163818,0.6826439499855042,0.17643117904663086,0.018109293654561043,0.7731384038925171,0.18293778598308563,0.6404657959938049,0.34703654050827026,0.49353182315826416,0.33675602078437805,0.24079371988773346,0.196040078997612,0.6136146783828735,0.48412856459617615,0.22871924936771393,0.5361219048500061,0.4930141866207123,0.37190955877304077,0.29840871691703796,0.29482898116111755,0.004866654984652996,0.5525434017181396,0.6169857978820801,0.10507003217935562,0.5194745063781738,0.12965986132621765,0.24084851145744324,0.29758813977241516,0.5145015120506287,0.2484140843153,0.54340660572052,0.5374822020530701,0.35950592160224915,0.04780098795890808,0.060577720403671265,0.46254292130470276,0.1441827416419983,0.5537449717521667,0.1365731805562973,0.36239391565322876,0.06252800673246384,0.3047383725643158,0.2346634566783905,0.04877515137195587,0.6433662176132202,0.3468550741672516,0.1657436639070511,0.14928016066551208,0.3179430067539215,0.1536044329404831,0.0069121746346354485,0.15356458723545074,0.6232147812843323,0.013239343650639057,0.6175666451454163,0.3617836534976959,0.0873880609869957,0.010455435141921043,0.39981818199157715,0.3760959208011627,0.04688216373324394,0.037993915379047394,0.04514235258102417,0.4837218225002289,0.6192233562469482,0.2189701944589615,0.5193264484405518,0.02137170173227787,0.226396381855011,0.35058942437171936,0.5820711851119995,0.25661203265190125,0.6219279170036316,0.6753234267234802,0.24406088888645172,0.4934995770454407,0.21210765838623047,0.4339323937892914,0.3690739870071411,0.04868162050843239,0.5199374556541443,0.4267684817314148,0.3791408836841583,0.6383141279220581,0.07656025886535645,0.18022455275058746,0.47163087129592896,0.41492122411727905,0.44769150018692017,0.19101805984973907,0.6924516558647156,0.0870259627699852,0.48807623982429504,0.6127728223800659,0.40766870975494385,0.3618010878562927,0.09934841096401215,0.06157099828124046,0.1120314970612526,0.3108523190021515,0.11104453355073929,0.09755469113588333,0.298003613948822,0.15451666712760925,0.19789761304855347,0.15547685325145721,0.21551185846328735,0.19310812652111053,0.27152276039123535,0.15905964374542236,0.06688352674245834,0.25238144397735596,0.5572103261947632,0.43874651193618774,0.015816230326890945,0.017032790929079056,0.27092787623405457,0.6359698176383972,0.08252710849046707,0.36214518547058105,0.2656831443309784,0.408827006816864,0.35868752002716064,0.0254361554980278,0.022287942469120026,0.3498331308364868,0.11496369540691376,0.5196647644042969,0.21395640075206757,0.1090349331498146,0.04456035792827606,0.11554592847824097,0.025734053924679756,0.4237522482872009,0.5155255794525146,0.4981873333454132,0.127912238240242,0.0015443760203197598,0.24136300384998322,0.5152112245559692,0.4099481403827667,0.539298951625824,0.5537616014480591,0.5428193807601929,0.5707558393478394,0.40700507164001465,0.18089310824871063,0.18051479756832123,0.613513708114624,0.5254374146461487,0.225594624876976,0.26697126030921936,0.8235123157501221,0.07330828160047531,0.12588201463222504,0.5206363797187805,0.14721816778182983,0.3631344437599182,0.15295645594596863,0.7398999929428101,0.5939680933952332,0.3115261197090149,0.30631008744239807,0.04334830492734909,0.36892107129096985,0.4536152482032776,0.4243941903114319,0.2270999252796173,0.2139878273010254],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('7c1a9f19-74a0-4a2b-be0c-8cf4d3714dc0');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.line(val_losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random val samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=ground_truth<br>vehicle_count=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "ground_truth",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "ground_truth",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          112,
          289,
          23,
          247,
          180,
          262,
          375,
          22,
          13,
          395,
          9,
          8,
          100,
          118,
          127,
          152,
          99,
          236,
          326,
          141,
          163,
          300,
          390,
          304,
          64,
          236,
          54,
          65,
          35,
          300,
          15,
          247,
          134,
          247,
          6,
          201,
          8,
          17,
          309,
          65,
          103,
          5,
          12,
          76,
          32,
          109,
          104,
          70,
          77,
          222,
          59,
          93,
          152,
          90,
          16,
          90,
          27,
          314,
          105,
          117,
          12,
          128,
          36,
          22,
          105,
          81,
          282,
          387,
          111,
          180,
          8,
          8,
          173,
          15,
          97,
          40,
          9,
          252,
          15,
          144,
          248,
          3,
          109,
          60,
          421,
          93,
          76,
          109,
          151,
          359,
          37,
          9,
          9,
          77,
          30,
          34,
          104,
          266,
          250,
          105
         ],
         "xaxis": "x",
         "y": [
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          18242.85546875,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          18242.85546875,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          18242.85546875,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          6335.06298828125,
          18242.85546875,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          6335.06298828125,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          18242.85546875,
          6335.06298828125
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=predictions<br>vehicle_count=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "predictions",
         "marker": {
          "color": "#EF553B",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "predictions",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          112,
          289,
          23,
          247,
          180,
          262,
          375,
          22,
          13,
          395,
          9,
          8,
          100,
          118,
          127,
          152,
          99,
          236,
          326,
          141,
          163,
          300,
          390,
          304,
          64,
          236,
          54,
          65,
          35,
          300,
          15,
          247,
          134,
          247,
          6,
          201,
          8,
          17,
          309,
          65,
          103,
          5,
          12,
          76,
          32,
          109,
          104,
          70,
          77,
          222,
          59,
          93,
          152,
          90,
          16,
          90,
          27,
          314,
          105,
          117,
          12,
          128,
          36,
          22,
          105,
          81,
          282,
          387,
          111,
          180,
          8,
          8,
          173,
          15,
          97,
          40,
          9,
          252,
          15,
          144,
          248,
          3,
          109,
          60,
          421,
          93,
          76,
          109,
          151,
          359,
          37,
          9,
          9,
          77,
          30,
          34,
          104,
          266,
          250,
          105
         ],
         "xaxis": "x",
         "y": [
          8508.654296875,
          22687.052734375,
          7486.537109375,
          12466.421875,
          5080.978515625,
          4847.572265625,
          19185.85546875,
          7950.50390625,
          12097.6025390625,
          16048.8544921875,
          14087.4833984375,
          5947.6318359375,
          4952.1982421875,
          7900.642578125,
          8316.796875,
          6668.5,
          5337.9814453125,
          18584.671875,
          17463.34375,
          8299.490234375,
          12814.1171875,
          18702.330078125,
          17721.291015625,
          10538.4560546875,
          6090.1572265625,
          10957.259765625,
          4287.22216796875,
          6280.3271484375,
          4178.68994140625,
          15452.85546875,
          10419.45703125,
          15482.666015625,
          8340.5947265625,
          14039.958984375,
          4406.33056640625,
          13762.0546875,
          8336.9560546875,
          8949.4541015625,
          17979.921875,
          4356.1904296875,
          13689.08203125,
          5453.34326171875,
          5663.14306640625,
          6992.75390625,
          6270.84033203125,
          7716.5556640625,
          16999.845703125,
          5781.83349609375,
          5343.927734375,
          17254.423828125,
          11361.1337890625,
          11261.861328125,
          5677.572265625,
          4829.0634765625,
          5805.89697265625,
          8401.5283203125,
          6331.31640625,
          22324.140625,
          4984.8828125,
          6545.52392578125,
          8615.3408203125,
          11154.078125,
          14364.279296875,
          8463.25,
          5505.2890625,
          9411.70703125,
          11088.59765625,
          17564.986328125,
          8474.2421875,
          2267.568359375,
          6540.2998046875,
          8004.06982421875,
          12284.671875,
          6842.7705078125,
          5874.068359375,
          11880.8671875,
          7181.771484375,
          12241.458984375,
          13414.04296875,
          15378.564453125,
          9068.2509765625,
          6778.4130859375,
          8995.60546875,
          10100.732421875,
          19910.458984375,
          12024.208984375,
          12742.29296875,
          4349.1376953125,
          5818.74560546875,
          10360.0869140625,
          5551.5576171875,
          5903.48388671875,
          7002.51171875,
          12429.1552734375,
          5974.17333984375,
          10338.08984375,
          14469.53125,
          21128.806640625,
          7340.3623046875,
          6385.93896484375
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "vehicle_count"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"ba4def05-09b2-4d39-86a5-34e8ed716503\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ba4def05-09b2-4d39-86a5-34e8ed716503\")) {                    Plotly.newPlot(                        \"ba4def05-09b2-4d39-86a5-34e8ed716503\",                        [{\"hovertemplate\":\"variable=ground_truth<br>vehicle_count=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"ground_truth\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"ground_truth\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[112.0,289.0,23.0,247.0,180.0,262.0,375.0,22.0,13.0,395.0,9.0,8.0,100.0,118.0,127.0,152.0,99.0,236.0,326.0,141.0,163.0,300.0,390.0,304.0,64.0,236.0,54.0,65.0,35.0,300.0,15.0,247.0,134.0,247.0,6.0,201.0,8.0,17.0,309.0,65.0,103.0,5.0,12.0,76.0,32.0,109.0,104.0,70.0,77.0,222.0,59.0,93.0,152.0,90.0,16.0,90.0,27.0,314.0,105.0,117.0,12.0,128.0,36.0,22.0,105.0,81.0,282.0,387.0,111.0,180.0,8.0,8.0,173.0,15.0,97.0,40.0,9.0,252.0,15.0,144.0,248.0,3.0,109.0,60.0,421.0,93.0,76.0,109.0,151.0,359.0,37.0,9.0,9.0,77.0,30.0,34.0,104.0,266.0,250.0,105.0],\"xaxis\":\"x\",\"y\":[6335.06298828125,18242.85546875,6335.06298828125,18242.85546875,6335.06298828125,18242.85546875,18242.85546875,6335.06298828125,18242.85546875,18242.85546875,18242.85546875,6335.06298828125,6335.06298828125,6335.06298828125,6335.06298828125,18242.85546875,6335.06298828125,18242.85546875,18242.85546875,6335.06298828125,18242.85546875,18242.85546875,18242.85546875,18242.85546875,6335.06298828125,18242.85546875,6335.06298828125,6335.06298828125,18242.85546875,18242.85546875,18242.85546875,18242.85546875,6335.06298828125,18242.85546875,6335.06298828125,18242.85546875,6335.06298828125,6335.06298828125,18242.85546875,6335.06298828125,18242.85546875,6335.06298828125,6335.06298828125,6335.06298828125,6335.06298828125,6335.06298828125,18242.85546875,6335.06298828125,6335.06298828125,18242.85546875,18242.85546875,18242.85546875,6335.06298828125,6335.06298828125,18242.85546875,6335.06298828125,6335.06298828125,18242.85546875,6335.06298828125,18242.85546875,6335.06298828125,18242.85546875,18242.85546875,6335.06298828125,6335.06298828125,6335.06298828125,18242.85546875,18242.85546875,6335.06298828125,6335.06298828125,6335.06298828125,18242.85546875,18242.85546875,6335.06298828125,6335.06298828125,18242.85546875,6335.06298828125,18242.85546875,18242.85546875,18242.85546875,18242.85546875,6335.06298828125,6335.06298828125,6335.06298828125,18242.85546875,18242.85546875,18242.85546875,18242.85546875,6335.06298828125,18242.85546875,6335.06298828125,6335.06298828125,6335.06298828125,6335.06298828125,18242.85546875,18242.85546875,18242.85546875,18242.85546875,18242.85546875,6335.06298828125],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=predictions<br>vehicle_count=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"predictions\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"predictions\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[112.0,289.0,23.0,247.0,180.0,262.0,375.0,22.0,13.0,395.0,9.0,8.0,100.0,118.0,127.0,152.0,99.0,236.0,326.0,141.0,163.0,300.0,390.0,304.0,64.0,236.0,54.0,65.0,35.0,300.0,15.0,247.0,134.0,247.0,6.0,201.0,8.0,17.0,309.0,65.0,103.0,5.0,12.0,76.0,32.0,109.0,104.0,70.0,77.0,222.0,59.0,93.0,152.0,90.0,16.0,90.0,27.0,314.0,105.0,117.0,12.0,128.0,36.0,22.0,105.0,81.0,282.0,387.0,111.0,180.0,8.0,8.0,173.0,15.0,97.0,40.0,9.0,252.0,15.0,144.0,248.0,3.0,109.0,60.0,421.0,93.0,76.0,109.0,151.0,359.0,37.0,9.0,9.0,77.0,30.0,34.0,104.0,266.0,250.0,105.0],\"xaxis\":\"x\",\"y\":[8508.654296875,22687.052734375,7486.537109375,12466.421875,5080.978515625,4847.572265625,19185.85546875,7950.50390625,12097.6025390625,16048.8544921875,14087.4833984375,5947.6318359375,4952.1982421875,7900.642578125,8316.796875,6668.5,5337.9814453125,18584.671875,17463.34375,8299.490234375,12814.1171875,18702.330078125,17721.291015625,10538.4560546875,6090.1572265625,10957.259765625,4287.22216796875,6280.3271484375,4178.68994140625,15452.85546875,10419.45703125,15482.666015625,8340.5947265625,14039.958984375,4406.33056640625,13762.0546875,8336.9560546875,8949.4541015625,17979.921875,4356.1904296875,13689.08203125,5453.34326171875,5663.14306640625,6992.75390625,6270.84033203125,7716.5556640625,16999.845703125,5781.83349609375,5343.927734375,17254.423828125,11361.1337890625,11261.861328125,5677.572265625,4829.0634765625,5805.89697265625,8401.5283203125,6331.31640625,22324.140625,4984.8828125,6545.52392578125,8615.3408203125,11154.078125,14364.279296875,8463.25,5505.2890625,9411.70703125,11088.59765625,17564.986328125,8474.2421875,2267.568359375,6540.2998046875,8004.06982421875,12284.671875,6842.7705078125,5874.068359375,11880.8671875,7181.771484375,12241.458984375,13414.04296875,15378.564453125,9068.2509765625,6778.4130859375,8995.60546875,10100.732421875,19910.458984375,12024.208984375,12742.29296875,4349.1376953125,5818.74560546875,10360.0869140625,5551.5576171875,5903.48388671875,7002.51171875,12429.1552734375,5974.17333984375,10338.08984375,14469.53125,21128.806640625,7340.3623046875,6385.93896484375],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"vehicle_count\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ba4def05-09b2-4d39-86a5-34e8ed716503');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "ground_truth = []\n",
    "vehicle_counts = []\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    vehicle_count = 0\n",
    "    random_idx = np.random.randint(0,len(val_data))\n",
    "    x, y = val_data[random_idx]\n",
    "    vehicle_count = x[0]\n",
    "    vehicle_counts.append(vehicle_count)\n",
    "    ground_truth.append(float(y))\n",
    "    pred_y = float(nn_model(x)[0])\n",
    "    preds.append(pred_y)\n",
    "\n",
    "df = pd.DataFrame({'vehicle_count': vehicle_counts, 'ground_truth': ground_truth, 'predictions': preds})\n",
    "\n",
    "px.scatter(df, x='vehicle_count', y=['ground_truth', 'predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nn_model.state_dict(), NN_MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
